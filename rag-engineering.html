<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RAG Engineering - 50 Advanced Interview Questions | Chandan Kumar</title>
    <link rel="icon" type="image/x-icon" href="favicon.ico">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        :root { --primary-color: #00d4ff; --secondary-color: #0099cc; --neon-blue: #00d4ff; --dark-blue: #0a1929; --darker-blue: #051422; }
        body { font-family: "Segoe UI", -apple-system, BlinkMacSystemFont, Arial, sans-serif; background: var(--darker-blue); color: #e0e0e0; line-height: 1.6; overflow-x: hidden; position: relative; }
        .animated-bg { position: fixed; top: 0; left: 0; width: 100%; height: 100%; z-index: -1; background: var(--darker-blue); background-image: url('background-image.jpg'); background-size: cover; background-position: center; background-repeat: no-repeat; background-color: var(--darker-blue); }
        .animated-bg::before { content: ''; position: absolute; top: 0; left: 0; width: 100%; height: 100%; background-image: radial-gradient(circle at 20% 30%, rgba(0, 212, 255, 0.1) 0%, transparent 50%), radial-gradient(circle at 80% 70%, rgba(0, 153, 204, 0.1) 0%, transparent 50%); opacity: 0.6; }
        .animated-bg::after { content: ''; position: absolute; top: 0; left: 0; width: 100%; height: 100%; background-image: linear-gradient(rgba(0, 212, 255, 0.03) 1px, transparent 1px), linear-gradient(90deg, rgba(0, 212, 255, 0.03) 1px, transparent 1px); background-size: 50px 50px; opacity: 0.4; }
        nav { position: fixed; top: 0; width: 100%; background: rgba(10, 25, 41, 0.85); backdrop-filter: blur(10px); box-shadow: 0 2px 20px rgba(0, 212, 255, 0.2); border-bottom: 1px solid rgba(0, 212, 255, 0.2); z-index: 1000; padding: 15px 0; }
        .nav-container { max-width: 1200px; margin: 0 auto; padding: 0 30px; display: flex; justify-content: space-between; align-items: center; }
        .logo { font-size: 24px; font-weight: 700; background: linear-gradient(135deg, #00d4ff 0%, #0099cc 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }
        .back-btn { color: rgba(255, 255, 255, 0.9); text-decoration: none; font-weight: 500; display: flex; align-items: center; gap: 8px; transition: all 0.3s ease; }
        .back-btn:hover { color: var(--neon-blue); text-shadow: 0 0 10px rgba(0, 212, 255, 0.8); }
        .container { max-width: 1000px; margin: 0 auto; padding: 120px 30px 60px; }
        .page-header { text-align: center; margin-bottom: 60px; padding: 40px 0; }
        .page-icon { font-size: 80px; margin-bottom: 30px; color: var(--neon-blue); text-shadow: 0 0 30px rgba(0, 212, 255, 0.8); }
        .page-title { font-size: 48px; font-weight: 800; margin-bottom: 20px; color: var(--neon-blue); text-shadow: 0 0 20px rgba(0, 212, 255, 0.8); }
        .page-subtitle { font-size: 20px; opacity: 0.9; color: #e0e0e0; }
        .content-section { background: rgba(10, 25, 41, 0.6); backdrop-filter: blur(5px); border-radius: 20px; padding: 50px; margin-bottom: 30px; box-shadow: 0 8px 32px rgba(0, 212, 255, 0.1); border: 1px solid rgba(0, 212, 255, 0.2); }
        .content-section h2 { font-size: 32px; margin-bottom: 20px; color: var(--neon-blue); }
        .content-section p { font-size: 18px; line-height: 1.8; margin-bottom: 20px; color: #e0e0e0; }
        .content-section ul { list-style: none; padding-left: 0; }
        .content-section li { font-size: 18px; line-height: 1.8; margin-bottom: 15px; padding-left: 30px; position: relative; color: #e0e0e0; }
        .content-section li::before { content: '▸'; position: absolute; left: 0; color: var(--neon-blue); font-weight: bold; }
        .container { max-width: 1200px; margin: 0 auto; padding: 120px 30px 60px; }
        .question-card {
            background: rgba(10, 25, 41, 0.6);
            backdrop-filter: blur(5px);
            border-radius: 15px;
            padding: 40px;
            margin-bottom: 40px;
            box-shadow: 0 8px 32px rgba(0, 212, 255, 0.1);
            border: 1px solid rgba(0, 212, 255, 0.2);
        }
        .question-number {
            color: var(--neon-blue);
            font-size: 18px;
            font-weight: 700;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 2px solid rgba(0, 212, 255, 0.3);
        }
        .question-title {
            font-size: 28px;
            font-weight: 700;
            color: var(--neon-blue);
            margin-bottom: 25px;
        }
        .question-section {
            margin-bottom: 30px;
        }
        .question-section h3 {
            font-size: 22px;
            color: var(--neon-blue);
            margin-bottom: 15px;
            border-bottom: 2px solid rgba(0, 212, 255, 0.3);
            padding-bottom: 10px;
        }
        .question-section p {
            font-size: 16px;
            line-height: 1.8;
            color: #e0e0e0;
            margin-bottom: 15px;
        }
        .question-section ul {
            list-style: none;
            padding-left: 0;
        }
        .question-section li {
            font-size: 16px;
            line-height: 1.8;
            margin-bottom: 12px;
            padding-left: 25px;
            position: relative;
            color: #e0e0e0;
        }
        .question-section li::before {
            content: '▸';
            position: absolute;
            left: 0;
            color: var(--neon-blue);
            font-weight: bold;
        }
        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }
        .pros-box, .cons-box {
            padding: 20px;
            border-radius: 10px;
        }
        .pros-box {
            background: rgba(0, 255, 0, 0.1);
            border: 2px solid rgba(0, 255, 0, 0.3);
        }
        .cons-box {
            background: rgba(255, 0, 0, 0.1);
            border: 2px solid rgba(255, 0, 0, 0.3);
        }
        .pros-box h4 {
            color: #00ff00;
            margin-bottom: 15px;
            font-size: 18px;
        }
        .cons-box h4 {
            color: #ff4444;
            margin-bottom: 15px;
            font-size: 18px;
        }
        .diagram-container {
            background: rgba(5, 20, 34, 0.8);
            border-radius: 15px;
            padding: 30px;
            margin: 25px 0;
            border: 2px solid rgba(0, 212, 255, 0.3);
        }
        .diagram-title {
            font-size: 18px;
            font-weight: 700;
            color: var(--neon-blue);
            margin-bottom: 15px;
            text-align: center;
        }
        .mermaid {
            background: rgba(255, 255, 255, 0.05);
            padding: 20px;
            border-radius: 10px;
        }
        .mermaid svg {
            max-width: 100%;
            height: auto;
        }
        .mermaid .nodeLabel,
        .mermaid .edgeLabel,
        .mermaid .cluster-label text,
        .mermaid .label text,
        .mermaid text {
            fill: #000000 !important;
            color: #000000 !important;
        }
        .mermaid .node rect,
        .mermaid .node circle,
        .mermaid .node ellipse,
        .mermaid .cluster rect {
            fill: #ffffff;
            stroke: #333333;
        }
        .mermaid .edgePath .path {
            stroke: #333333;
        }
        .mermaid .arrowheadPath {
            fill: #333333;
        }
        .code-block {
            background: rgba(0, 0, 0, 0.5);
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            border-left: 4px solid var(--neon-blue);
            overflow-x: auto;
        }
        .code-block pre {
            color: #e0e0e0;
            font-family: 'Courier New', monospace;
            font-size: 14px;
            line-height: 1.6;
            white-space: pre-wrap;
        }
        .code-block code {
            color: #e0e0e0;
        }
        .toc {
            background: rgba(10, 25, 41, 0.6);
            backdrop-filter: blur(5px);
            border-radius: 15px;
            padding: 30px;
            margin-bottom: 40px;
            border: 1px solid rgba(0, 212, 255, 0.2);
        }
        .toc h2 {
            color: var(--neon-blue);
            margin-bottom: 20px;
        }
        .toc-list {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(220px, 1fr));
            gap: 10px;
        }
        .toc-list a {
            color: #e0e0e0;
            text-decoration: none;
            padding: 8px 12px;
            border-radius: 5px;
            transition: all 0.3s ease;
            display: block;
            font-size: 14px;
        }
        .toc-list a:hover {
            background: rgba(0, 212, 255, 0.2);
            color: var(--neon-blue);
        }
        @media (max-width: 768px) { .page-title { font-size: 32px; } .question-card { padding: 25px 20px; } .pros-cons { grid-template-columns: 1fr; } }
    </style>
</head>
<body>
    <div class="animated-bg"></div>
    <nav><div class="nav-container"><div class="logo">Chandan Kumar</div><a href="interview.html" class="back-btn"><i class="fas fa-arrow-left"></i> Back to Interview Guide</a></div></nav>
    <div class="container">
        <div class="page-header">
            <div class="page-icon"><i class="fas fa-book"></i></div>
            <h1 class="page-title">RAG Engineering</h1>
            <p class="page-subtitle">50 Advanced Interview Questions &amp; Answers</p>
        </div>

        <!-- Table of Contents -->
        <div class="toc">
            <h2>Table of Contents</h2>
            <div class="toc-list" id="tocList"></div>
        </div>

        <!-- Questions Container -->
        <div id="questionsContainer"></div>
        </div>

    <script>
        const questions = [];

        // Q1 - What is RAG and why is it important?
        questions.push({
            number: 1,
            title: "What is Retrieval-Augmented Generation (RAG) and why is it important?",
            description:
                "Retrieval-Augmented Generation (RAG) is an AI architecture that combines information retrieval with generative language models. " +
                "Instead of relying solely on the LLM's training data, RAG retrieves relevant documents from an external knowledge base and uses them as context for generation. " +
                "This allows LLMs to access up-to-date, domain-specific information and cite sources, making responses more accurate and trustworthy.",
            why:
                "RAG solves critical limitations of LLMs: knowledge cutoff dates, lack of domain-specific information, and inability to cite sources. " +
                "It enables building production AI systems that can answer questions about recent events, proprietary documents, and specialized domains without expensive fine-tuning.",
            what:
                "RAG consists of two main components: a Retriever (searches and retrieves relevant documents) and a Generator (LLM that generates responses using retrieved context). " +
                "The process involves: query embedding, vector similarity search, context retrieval, and prompt augmentation before generation.",
            how:
                "Implement RAG by: 1) Chunk documents and generate embeddings, 2) Store embeddings in a vector database, 3) Embed user query, 4) Retrieve top-k similar chunks, " +
                "5) Inject retrieved context into LLM prompt, 6) Generate response with citations.",
            pros: [
                "Access to up-to-date information beyond training data",
                "Domain-specific knowledge without fine-tuning",
                "Source citations for transparency",
                "Reduced hallucination through grounding",
                "Cost-effective compared to fine-tuning",
                "Easy to update knowledge base"
            ],
            cons: [
                "Latency from retrieval step",
                "Quality depends on retrieval accuracy",
                "Context window limitations",
                "Requires vector database infrastructure",
                "Chunking strategy affects performance"
            ],
            diagram: `flowchart TD
    A["User Query"] --> B["Query Embedding"]
    B --> C["Vector Search"]
    C --> D["Retrieve Top-K Documents"]
    D --> E["Build Context"]
    E --> F["Augment Prompt"]
    F --> G["LLM Generation"]
    G --> H["Response with Citations"]`,
            implementation: `# Basic RAG Implementation
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA

# Load and chunk documents
loader = PyPDFLoader("document.pdf")
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)

# Create embeddings and vector store
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings)

# Create RAG chain
llm = OpenAI(temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True
)

# Query
result = qa_chain({"query": "What is the main topic?"})
print(result["result"])
print("Sources:", result["source_documents"])`,
            approaches: [
                "Start with simple RAG before adding complexity.",
                "Choose appropriate chunk size (typically 200-500 tokens).",
                "Use overlap between chunks to preserve context.",
                "Experiment with different retrieval strategies (dense, sparse, hybrid).",
                "Monitor retrieval quality and adjust k (number of retrieved chunks).",
                "Implement reranking for better relevance.",
                "Add metadata filtering for domain-specific retrieval."
            ]
        });

        // Q2 - Explain DSPy and how it improves RAG systems
        questions.push({
            number: 2,
            title: "Explain DSPy and how it improves RAG systems over traditional prompt tuning",
            description:
                "DSPy (Declarative Self-improving Python) is a framework that treats prompts as optimizable parameters rather than hand-tuned strings. " +
                "Instead of manually tweaking prompts, DSPy uses automated optimization techniques to find the best prompt structure, examples, and instructions. " +
                "This addresses the 'leaky boat' problem of prompt tuning where small changes can break the system.",
            why:
                "Traditional prompt tuning is brittle and time-consuming. DSPy provides systematic optimization that finds better prompts automatically, " +
                "making RAG systems more robust and maintainable. It enables reproducible, data-driven prompt engineering.",
            what:
                "DSPy introduces: Signatures (declarative input/output specs), Modules (reusable prompt components), Optimizers (automated prompt tuning), " +
                "and Metrics (evaluation functions). It uses techniques like BootstrapFewShot, MIPRO, and COPRO to optimize prompts based on training data.",
            how:
                "Define your task with a Signature, create a DSPy Module (like ChainOfThought or ReAct), provide training examples, " +
                "run an optimizer to find best prompts, and evaluate on test set. DSPy automatically generates and tests prompt variations.",
            pros: [
                "Systematic prompt optimization",
                "Reproducible results",
                "Less manual tuning required",
                "Better performance through optimization",
                "Handles prompt brittleness",
                "Data-driven approach"
            ],
            cons: [
                "Learning curve for DSPy concepts",
                "Requires training data",
                "Optimization can be time-consuming",
                "May need multiple optimization runs",
                "Less control over exact prompt wording"
            ],
            diagram: `flowchart TD
    A["Training Data"] --> B["DSPy Module"]
    B --> C["Optimizer"]
    C --> D["Generate Prompt Variations"]
    D --> E["Evaluate on Metrics"]
    E --> F{"Best Prompt?"}
    F -->|No| C
    F -->|Yes| G["Optimized RAG System"]
    G --> H["Production Deployment"]`,
            implementation: `# DSPy RAG Implementation
import dspy
from dspy.teleprompt import BootstrapFewShot

# Configure LLM
lm = dspy.LM(model="gpt-3.5-turbo")
dspy.configure(lm=lm)

# Define signature for RAG task
class RAGSignature(dspy.Signature):
    """Answer questions based on retrieved context."""
    context = dspy.InputField(desc="Retrieved document context")
    question = dspy.InputField(desc="User question")
    answer = dspy.OutputField(desc="Answer based on context")

# Create RAG module
class RAGModule(dspy.Module):
    def __init__(self):
        super().__init__()
        self.retrieve = dspy.Retrieve(k=3)
        self.generate_answer = dspy.ChainOfThought(RAGSignature)
    
    def forward(self, question):
        # Retrieve relevant context
        context = self.retrieve(question).passages
        
        # Generate answer with context
        return self.generate_answer(context=context, question=question)

# Training examples
trainset = [
    dspy.Example(question="What is RAG?", 
                 context="RAG is Retrieval-Augmented Generation...",
                 answer="RAG combines retrieval and generation...").with_inputs("question"),
    # More examples...
]

# Optimize the module
optimizer = BootstrapFewShot(metric=dspy.evaluate.answer_exact_match)
optimized_rag = optimizer.compile(RAGModule(), trainset=trainset)

# Use optimized module
result = optimized_rag("How does RAG work?")
print(result.answer)`,
            approaches: [
                "Start with simple DSPy modules before complex ones.",
                "Collect diverse training examples covering edge cases.",
                "Use appropriate metrics for your task (exact match, F1, etc.).",
                "Experiment with different optimizers (BootstrapFewShot, MIPRO).",
                "Validate optimized prompts on held-out test set.",
                "Monitor performance in production and retrain as needed.",
                "Combine DSPy with traditional RAG components for best results."
            ]
        });

        // Q3 - Building a PDF chat system with RAG
        questions.push({
            number: 3,
            title: "How do you build a 'Talk with PDF' system using RAG?",
            description:
                "A PDF chat system allows users to ask questions about PDF documents using natural language. " +
                "The system uses RAG to extract text from PDFs, chunk it appropriately, create embeddings, store in a vector database, " +
                "and retrieve relevant sections when answering questions. This enables conversational access to document content.",
            why:
                "PDF chat systems make large documents accessible through natural language queries. Users don't need to read entire documents " +
                "but can ask specific questions and get precise answers with source citations. This is valuable for legal documents, research papers, manuals, etc.",
            what:
                "PDF Chat System Components: PDF loader (extracts text), Text splitter (chunks documents), Embedding model (creates vectors), " +
                "Vector store (stores and searches embeddings), Retriever (finds relevant chunks), LLM (generates answers), and UI (chat interface).",
            how:
                "1) Load PDF and extract text, 2) Split into chunks with overlap, 3) Generate embeddings for each chunk, 4) Store in vector DB, " +
                "5) For each query: embed query, retrieve top-k chunks, build context, generate answer with citations, 6) Display in chat UI.",
            pros: [
                "Natural language access to documents",
                "Fast information retrieval",
                "Source citations for verification",
                "Handles large documents efficiently",
                "Supports multiple PDFs",
                "Conversational interface"
            ],
            cons: [
                "PDF parsing can be complex (tables, images)",
                "Chunking may break context",
                "Requires embedding and vector DB infrastructure",
                "Latency for first-time document processing",
                "May miss information if retrieval fails"
            ],
            diagram: `flowchart TD
    A["PDF Document"] --> B["PDF Parser"]
    B --> C["Text Extraction"]
    C --> D["Text Chunking"]
    D --> E["Generate Embeddings"]
    E --> F["Vector Database"]
    G["User Question"] --> H["Query Embedding"]
    H --> I["Similarity Search"]
    F --> I
    I --> J["Retrieve Chunks"]
    J --> K["Build Context"]
    K --> L["LLM Generation"]
    L --> M["Answer with Citations"]`,
            implementation: `# Talk with PDF using RAG
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

class PDFChatSystem:
    def __init__(self, pdf_path):
        # Load PDF
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()
        
        # Split into chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        chunks = text_splitter.split_documents(documents)
        
        # Create embeddings and vector store
        embeddings = OpenAIEmbeddings()
        self.vectorstore = FAISS.from_documents(chunks, embeddings)
        
        # Setup LLM and memory
        llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo")
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        
        # Create conversational chain
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": 3}
            ),
            memory=memory,
            return_source_documents=True
        )
    
    def ask(self, question):
        result = self.qa_chain({"question": question})
        return {
            "answer": result["answer"],
            "sources": [
                doc.page_content[:200] + "..." 
                for doc in result["source_documents"]
            ]
        }

# Usage
pdf_chat = PDFChatSystem("research_paper.pdf")
response = pdf_chat.ask("What is the main finding?")
print("Answer:", response["answer"])
print("Sources:", response["sources"])`,
            approaches: [
                "Use appropriate PDF loaders (PyPDFLoader, PDFMiner, etc.).",
                "Handle different PDF structures (text, tables, images).",
                "Choose chunk size based on document type and LLM context window.",
                "Add metadata (page numbers, sections) for better citations.",
                "Implement conversation memory for follow-up questions.",
                "Add document management for multiple PDFs.",
                "Implement streaming responses for better UX.",
                "Add error handling for corrupted or complex PDFs."
            ]
        });

        // Q4 - Vector stores and embedding strategies
        questions.push({
            number: 4,
            title: "What are the key considerations for choosing vector stores and embedding models in RAG?",
            description:
                "Vector stores are databases optimized for storing and searching high-dimensional vectors (embeddings). " +
                "The choice of vector store and embedding model significantly impacts RAG system performance, accuracy, and cost. " +
                "Different options offer trade-offs in scalability, latency, accuracy, and features.",
            why:
                "Poor choices in vector store or embedding model can lead to slow retrieval, inaccurate results, high costs, or scalability issues. " +
                "Understanding these trade-offs is crucial for building production-ready RAG systems that meet performance and accuracy requirements.",
            what:
                "Vector Stores: Chroma (simple, local), Pinecone (managed, scalable), Weaviate (graph + vector), Qdrant (self-hosted, fast), " +
                "FAISS (Facebook, in-memory), Milvus (distributed). Embedding Models: OpenAI text-embedding-ada-002, Sentence-BERT, " +
                "Instructor, E5, BGE. Considerations: dimension size, distance metric, indexing algorithm, filtering capabilities.",
            how:
                "Evaluate based on: 1) Scale requirements (documents, queries/sec), 2) Latency needs, 3) Budget (managed vs self-hosted), " +
                "4) Feature needs (metadata filtering, hybrid search), 5) Embedding model quality on your domain, 6) Cost per embedding.",
            pros: [
                "Chroma: Easy setup, good for prototyping",
                "Pinecone: Managed, scalable, production-ready",
                "Weaviate: Rich features, graph capabilities",
                "Qdrant: Fast, self-hosted option",
                "FAISS: Very fast for in-memory search",
                "OpenAI embeddings: High quality, consistent"
            ],
            cons: [
                "Chroma: Limited scalability",
                "Pinecone: Cost at scale",
                "Weaviate: Complex setup",
                "Qdrant: Requires infrastructure",
                "FAISS: Memory limitations",
                "OpenAI embeddings: API costs"
            ],
            diagram: `flowchart TD
    A["Document Chunks"] --> B["Embedding Model"]
    B --> C["Vector Embeddings"]
    C --> D["Vector Store"]
    D --> E["Indexing"]
    E --> F["Query Embedding"]
    F --> G["Similarity Search"]
    D --> G
    G --> H["Top-K Results"]`,
            implementation: `# Comparing Vector Stores
from langchain.vectorstores import Chroma, FAISS, Pinecone
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings

# Option 1: Chroma (local, simple)
embeddings = OpenAIEmbeddings()
chroma_store = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# Option 2: FAISS (in-memory, fast)
faiss_store = FAISS.from_documents(chunks, embeddings)
faiss_store.save_local("./faiss_index")

# Option 3: Pinecone (managed, scalable)
import pinecone
pinecone.init(api_key="your-key", environment="us-east-1")
pinecone_store = Pinecone.from_documents(
    chunks, 
    embeddings, 
    index_name="rag-index"
)

# Option 4: Using different embedding models
# OpenAI (high quality, paid)
openai_embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")

# HuggingFace (free, local)
hf_embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Compare retrieval
def compare_retrieval(query, stores):
    results = {}
    for name, store in stores.items():
        docs = store.similarity_search(query, k=3)
        results[name] = [doc.page_content[:100] for doc in docs]
    return results

stores = {
    "Chroma": chroma_store,
    "FAISS": faiss_store,
    "Pinecone": pinecone_store
}
comparison = compare_retrieval("What is RAG?", stores)`,
            approaches: [
                "Start with Chroma or FAISS for prototyping.",
                "Use Pinecone for production if budget allows.",
                "Consider self-hosted options (Qdrant, Milvus) for control.",
                "Test embedding models on your domain data.",
                "Use OpenAI embeddings for best quality, HF for cost savings.",
                "Implement hybrid search (dense + sparse) for better results.",
                "Monitor retrieval latency and accuracy metrics.",
                "Plan for scaling: consider distributed vector stores.",
                "Add metadata filtering for domain-specific retrieval."
            ]
        });

        // Q5 - Chunking strategies for RAG
        questions.push({
            number: 5,
            title: "What are the best practices for document chunking in RAG systems?",
            description:
                "Document chunking is the process of splitting large documents into smaller pieces that can be embedded and retrieved. " +
                "The chunking strategy significantly impacts retrieval quality, as chunks that are too small may lose context, " +
                "while chunks that are too large may contain irrelevant information and waste context window space.",
            why:
                "Poor chunking leads to incomplete context retrieval, information loss at boundaries, and inefficient use of LLM context windows. " +
                "The right chunking strategy ensures that retrieved chunks contain complete, relevant information for answering queries.",
            what:
                "Chunking Strategies: Fixed-size (simple, may break sentences), Sentence-based (preserves sentence boundaries), " +
                "Semantic chunking (groups related content), Recursive (hierarchical splitting), Overlapping chunks (preserves context at boundaries). " +
                "Key parameters: chunk_size (typically 200-1000 tokens), chunk_overlap (10-20% of chunk size), separators (sentence, paragraph, etc.).",
            how:
                "1) Analyze document structure (paragraphs, sections, headers), 2) Choose appropriate chunk size based on content type, " +
                "3) Use overlap to preserve context, 4) Respect natural boundaries (sentences, paragraphs), " +
                "5) Add metadata (chunk index, source, section), 6) Test retrieval quality with different strategies.",
            pros: [
                "Fixed-size: Simple, fast, predictable",
                "Sentence-based: Preserves linguistic structure",
                "Semantic: Groups related concepts together",
                "Overlapping: Reduces boundary information loss",
                "Recursive: Handles varied document structures",
                "Metadata: Enables better filtering and citation"
            ],
            cons: [
                "Fixed-size: May break sentences or concepts",
                "Sentence-based: May create very small chunks",
                "Semantic: More complex, requires additional processing",
                "Overlapping: Increases storage and embedding costs",
                "Recursive: Can be computationally expensive",
                "No one-size-fits-all solution"
            ],
            diagram: `flowchart TD
    A["Document"] --> B["Analyze Structure"]
    B --> C["Choose Chunking Strategy"]
    C --> D["Fixed-Size"]
    C --> E["Sentence-Based"]
    C --> F["Semantic"]
    D --> G["Split with Overlap"]
    E --> G
    F --> G
    G --> H["Add Metadata"]
    H --> I["Generate Embeddings"]
    I --> J["Store in Vector DB"]`,
            implementation: `# Chunking Strategies
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    CharacterTextSplitter,
    TokenTextSplitter,
    MarkdownTextSplitter
)

# Strategy 1: Recursive Character Splitter (Recommended)
recursive_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    separators=["\\n\\n", "\\n", ". ", " ", ""]
)
chunks = recursive_splitter.split_documents(documents)

# Strategy 2: Sentence-based with overlap
from langchain.text_splitter import SentenceTransformersTokenTextSplitter
sentence_splitter = SentenceTransformersTokenTextSplitter(
    chunk_overlap=50,
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
chunks = sentence_splitter.split_documents(documents)

# Strategy 3: Semantic chunking (preserves meaning)
from langchain_experimental.text_splitter import SemanticChunker
from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
semantic_splitter = SemanticChunker(
    embeddings=embeddings,
    breakpoint_threshold_type="percentile"
)
chunks = semantic_splitter.create_documents([text])

# Strategy 4: Markdown-aware chunking
markdown_splitter = MarkdownTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = markdown_splitter.split_documents(markdown_docs)

# Add metadata to chunks
for i, chunk in enumerate(chunks):
    chunk.metadata = {
        "chunk_index": i,
        "source": "document.pdf",
        "page": chunk.metadata.get("page", 0),
        "section": chunk.metadata.get("section", "unknown")
    }`,
            approaches: [
                "Start with RecursiveCharacterTextSplitter as a baseline.",
                "Use chunk_size of 500-1000 tokens for most use cases.",
                "Set chunk_overlap to 10-20% of chunk_size.",
                "Respect document structure (paragraphs, sections, headers).",
                "Use semantic chunking for documents with complex structure.",
                "Add metadata to enable better filtering and citation.",
                "Test different strategies on your specific documents.",
                "Monitor chunk quality through retrieval metrics.",
                "Consider document type: code, prose, tables need different approaches."
            ]
        });

        // Q6 - Hybrid search (dense + sparse)
        questions.push({
            number: 6,
            title: "How does hybrid search improve RAG retrieval compared to dense-only search?",
            description:
                "Hybrid search combines dense vector search (semantic similarity) with sparse keyword search (BM25, TF-IDF) to improve retrieval accuracy. " +
                "Dense search captures semantic meaning but may miss exact keyword matches, while sparse search finds exact terms but misses semantic variations. " +
                "Hybrid search leverages both approaches for better recall and precision.",
            why:
                "Pure dense search can miss relevant documents when queries contain specific terms, technical jargon, or when semantic embeddings don't capture exact matches. " +
                "Hybrid search addresses this by combining semantic understanding with keyword matching, resulting in more comprehensive retrieval.",
            what:
                "Dense Search: Uses embeddings to find semantically similar content (captures meaning, synonyms, paraphrases). " +
                "Sparse Search: Uses keyword matching (BM25, TF-IDF) to find exact term matches. " +
                "Hybrid: Combines both with weighted scoring (e.g., 70% dense + 30% sparse) or reciprocal rank fusion (RRF).",
            how:
                "1) Generate dense embeddings for query and documents, 2) Perform keyword indexing (BM25), " +
                "3) Run both dense and sparse searches, 4) Combine results using weighted scores or RRF, " +
                "5) Re-rank final results, 6) Return top-k combined results.",
            pros: [
                "Better recall: finds both semantic and keyword matches",
                "Handles technical terms and exact matches better",
                "More robust across different query types",
                "Can be tuned with weights for domain-specific needs",
                "Reduces false negatives from pure semantic search",
                "Works well with mixed content (text + code)"
            ],
            cons: [
                "More complex implementation",
                "Requires both embedding and keyword indexing",
                "Higher computational cost",
                "Need to tune weights for optimal performance",
                "More infrastructure requirements",
                "Slightly higher latency"
            ],
            diagram: `flowchart TD
    A["User Query"] --> B["Dense Embedding"]
    A --> C["Keyword Extraction"]
    B --> D["Vector Search"]
    C --> E["BM25 Search"]
    D --> F["Dense Results"]
    E --> G["Sparse Results"]
    F --> H["Combine Scores"]
    G --> H
    H --> I["Reciprocal Rank Fusion"]
    I --> J["Re-rank"]
    J --> K["Top-K Results"]`,
            implementation: `# Hybrid Search Implementation
from langchain.retrievers import BM25Retriever
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.retrievers import EnsembleRetriever

# Dense retriever (vector search)
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(documents, embeddings)
dense_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

# Sparse retriever (BM25 keyword search)
bm25_retriever = BM25Retriever.from_documents(documents)
bm25_retriever.k = 10

# Hybrid retriever (combines both)
ensemble_retriever = EnsembleRetriever(
    retrievers=[dense_retriever, bm25_retriever],
    weights=[0.7, 0.3]  # 70% dense, 30% sparse
)

# Query
docs = ensemble_retriever.get_relevant_documents("What is RAG?")

# Alternative: Using Weaviate with hybrid search
from langchain.vectorstores import Weaviate
import weaviate

client = weaviate.Client("http://localhost:8080")
vectorstore = Weaviate(client, "Document", "content")

# Hybrid search query
query = "What is RAG?"
docs = vectorstore.similarity_search(
    query,
    search_type="hybrid",
    alpha=0.7  # 0.7 = 70% dense, 30% sparse
)

# Manual hybrid search with RRF (Reciprocal Rank Fusion)
def reciprocal_rank_fusion(results_list, k=60):
    fused_scores = {}
    for results in results_list:
        for rank, doc in enumerate(results, start=1):
            doc_id = doc.page_content
            if doc_id not in fused_scores:
                fused_scores[doc_id] = 0
            fused_scores[doc_id] += 1 / (k + rank)
    
    # Sort by fused score
    sorted_docs = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
    return sorted_docs

# Get results from both retrievers
dense_results = dense_retriever.get_relevant_documents(query)
sparse_results = bm25_retriever.get_relevant_documents(query)

# Combine using RRF
combined_results = reciprocal_rank_fusion([dense_results, sparse_results])`,
            approaches: [
                "Start with dense-only search, add sparse if needed.",
                "Use 70/30 or 60/40 ratio (dense/sparse) as starting point.",
                "Tune weights based on your domain and query types.",
                "Use RRF for better combination than simple weighted average.",
                "Consider query type: semantic queries favor dense, keyword queries favor sparse.",
                "Monitor both precision and recall metrics.",
                "Use Weaviate or Pinecone for built-in hybrid search.",
                "Implement re-ranking after hybrid retrieval for best results.",
                "Cache sparse index for faster keyword search."
            ]
        });

        // Q7 - RAG evaluation metrics
        questions.push({
            number: 7,
            title: "How do you evaluate the performance of a RAG system?",
            description:
                "RAG evaluation requires measuring both retrieval quality (are we getting the right documents?) and generation quality (are the answers correct?). " +
                "Common metrics include retrieval metrics (precision, recall, MRR), generation metrics (BLEU, ROUGE, semantic similarity), " +
                "and end-to-end metrics (answer correctness, faithfulness, relevance).",
            why:
                "Without proper evaluation, you can't know if your RAG system is improving or if changes are making it worse. " +
                "Evaluation helps identify bottlenecks (retrieval vs generation), guides optimization efforts, and ensures production readiness.",
            what:
                "Retrieval Metrics: Precision@K, Recall@K, Mean Reciprocal Rank (MRR), Hit Rate. " +
                "Generation Metrics: BLEU, ROUGE, BERTScore, Semantic Similarity. " +
                "End-to-End Metrics: Answer Correctness, Faithfulness (grounded in context), Relevance, Citation Accuracy. " +
                "Tools: RAGAS, TruLens, LangSmith, custom evaluation frameworks.",
            how:
                "1) Create evaluation dataset with questions and ground truth, 2) Measure retrieval metrics on retrieved chunks, " +
                "3) Measure generation quality on LLM outputs, 4) Evaluate end-to-end with human or LLM-as-judge, " +
                "5) Track metrics over time, 6) A/B test different configurations.",
            pros: [
                "Systematic approach to improvement",
                "Identifies specific problem areas",
                "Enables data-driven optimization",
                "Builds confidence before production",
                "Helps set performance baselines",
                "Supports continuous monitoring"
            ],
            cons: [
                "Requires labeled evaluation data",
                "Can be time-consuming to set up",
                "Metrics may not capture all quality aspects",
                "Human evaluation is expensive",
                "LLM-as-judge may have biases",
                "Need to balance multiple metrics"
            ],
            diagram: `flowchart TD
    A["RAG System"] --> B["Retrieval Evaluation"]
    A --> C["Generation Evaluation"]
    A --> D["End-to-End Evaluation"]
    B --> E["Precision@K"]
    B --> F["Recall@K"]
    B --> G["MRR"]
    C --> H["BLEU/ROUGE"]
    C --> I["Semantic Similarity"]
    D --> J["Answer Correctness"]
    D --> K["Faithfulness"]
    D --> L["Relevance"]
    E --> M["Performance Report"]
    F --> M
    G --> M
    H --> M
    I --> M
    J --> M
    K --> M
    L --> M`,
            implementation: `# RAG Evaluation
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall
)
from datasets import Dataset

# Prepare evaluation dataset
eval_dataset = Dataset.from_dict({
    "question": ["What is RAG?", "How does retrieval work?"],
    "answer": ["RAG is Retrieval-Augmented Generation...", "Retrieval finds relevant documents..."],
    "contexts": [
        ["RAG combines retrieval and generation...", "It uses vector search..."],
        ["Retrieval uses embeddings...", "Similarity search finds documents..."]
    ],
    "ground_truth": ["RAG is a technique that...", "Retrieval involves..."]
})

# Evaluate with RAGAS
results = evaluate(
    dataset=eval_dataset,
    metrics=[faithfulness, answer_relevancy, context_precision, context_recall]
)
print(results)

# Custom evaluation
def evaluate_retrieval(retriever, queries, ground_truth_docs):
    precision_scores = []
    recall_scores = []
    
    for query, gt_docs in zip(queries, ground_truth_docs):
        retrieved = retriever.get_relevant_documents(query, k=5)
        retrieved_ids = {doc.page_content for doc in retrieved}
        gt_ids = {doc.page_content for doc in gt_docs}
        
        # Precision@K
        precision = len(retrieved_ids & gt_ids) / len(retrieved_ids) if retrieved_ids else 0
        precision_scores.append(precision)
        
        # Recall@K
        recall = len(retrieved_ids & gt_ids) / len(gt_ids) if gt_ids else 0
        recall_scores.append(recall)
    
    return {
        "precision@5": np.mean(precision_scores),
        "recall@5": np.mean(recall_scores)
    }

# LLM-as-judge evaluation
from langchain.evaluation import QAEvalChain

llm = ChatOpenAI(temperature=0)
eval_chain = QAEvalChain.from_llm(llm)

predictions = [
    {"question": "What is RAG?", "answer": "RAG is..."}
]
examples = [
    {"question": "What is RAG?", "answer": "RAG combines retrieval and generation..."}
]

eval_results = eval_chain.evaluate(examples, predictions)
print(eval_results)

# Semantic similarity evaluation
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('all-MiniLM-L6-v2')
predicted_emb = model.encode("RAG is Retrieval-Augmented Generation")
ground_truth_emb = model.encode("RAG combines retrieval with generation")
similarity = util.cos_sim(predicted_emb, ground_truth_emb)
print(f"Semantic similarity: {similarity.item()}")`,
            approaches: [
                "Start with retrieval metrics (easier to measure).",
                "Use RAGAS for comprehensive evaluation.",
                "Create diverse evaluation dataset covering edge cases.",
                "Measure both individual components and end-to-end.",
                "Use LLM-as-judge for scalable evaluation.",
                "Combine automated metrics with human evaluation.",
                "Track metrics over time to detect regressions.",
                "Set up continuous evaluation in CI/CD.",
                "Focus on metrics that matter for your use case.",
                "Evaluate on production-like queries, not just training data."
            ]
        });

        // Q8 - Advanced retrieval techniques
        questions.push({
            number: 8,
            title: "What are advanced retrieval techniques beyond simple similarity search?",
            description:
                "Beyond basic vector similarity search, advanced retrieval techniques include query expansion, reranking, " +
                "multi-query retrieval, parent-document retrieval, and query decomposition. These techniques improve retrieval accuracy " +
                "by addressing limitations of simple embedding-based search.",
            why:
                "Simple similarity search can miss relevant documents due to vocabulary mismatch, query ambiguity, or information spread across chunks. " +
                "Advanced techniques address these issues, significantly improving retrieval quality and ultimately answer accuracy.",
            what:
                "Query Expansion: Generate multiple query variations to improve recall. " +
                "Reranking: Use cross-encoders to reorder retrieved results for better precision. " +
                "Multi-Query: Generate multiple queries and combine results. " +
                "Parent-Document Retrieval: Retrieve small chunks but return parent documents for context. " +
                "Query Decomposition: Break complex queries into sub-queries. " +
                "Contextual Compression: Filter irrelevant parts of retrieved chunks.",
            how:
                "1) Query expansion: Use LLM to generate query variations, 2) Initial retrieval with expanded queries, " +
                "3) Rerank results using cross-encoder model, 4) For parent-doc: store small chunks, return parent on match, " +
                "5) Decompose complex queries into simpler sub-queries, 6) Compress retrieved context to remove noise.",
            pros: [
                "Query expansion: Better recall for diverse queries",
                "Reranking: Significantly improves precision",
                "Multi-query: Handles ambiguous queries better",
                "Parent-doc: More context without large chunks",
                "Query decomposition: Handles complex questions",
                "Context compression: Focuses on relevant information"
            ],
            cons: [
                "Query expansion: More LLM calls, higher latency",
                "Reranking: Additional computational cost",
                "Multi-query: More complex, higher costs",
                "Parent-doc: Requires document hierarchy",
                "Query decomposition: May over-complicate simple queries",
                "Context compression: May remove important context"
            ],
            diagram: `flowchart TD
    A["User Query"] --> B["Query Expansion"]
    B --> C["Multi-Query Generation"]
    C --> D["Vector Search"]
    D --> E["Initial Retrieval"]
    E --> F["Reranking"]
    F --> G["Context Compression"]
    G --> H["Parent Document Retrieval"]
    H --> I["Final Context"]
    I --> J["LLM Generation"]`,
            implementation: `# Advanced Retrieval Techniques
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers import ParentDocumentRetriever
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.retrievers import BM25Retriever
from sentence_transformers import CrossEncoder

# 1. Query Expansion with Multi-Query
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.llms import OpenAI

llm = OpenAI(temperature=0)
base_retriever = vectorstore.as_retriever()
multi_query_retriever = MultiQueryRetriever.from_llm(
    retriever=base_retriever,
    llm=llm
)
docs = multi_query_retriever.get_relevant_documents("What is RAG?")

# 2. Reranking with Cross-Encoder
cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def rerank_documents(query, documents, top_k=3):
    pairs = [[query, doc.page_content] for doc in documents]
    scores = cross_encoder.predict(pairs)
    
    # Sort by score
    ranked = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
    return [doc for doc, score in ranked[:top_k]]

# Initial retrieval
initial_docs = vectorstore.similarity_search(query, k=10)
# Rerank
reranked_docs = rerank_documents(query, initial_docs, top_k=3)

# 3. Parent Document Retrieval
from langchain.storage import InMemoryStore
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Split into small chunks for retrieval
child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)
# Keep parent documents larger
parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)

store = InMemoryStore()
parent_retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter,
    parent_splitter=parent_splitter
)
parent_retriever.add_documents(documents)

# Retrieve returns parent documents
docs = parent_retriever.get_relevant_documents(query)

# 4. Contextual Compression
from langchain.retrievers.document_compressors import LLMChainExtractor

llm = OpenAI(temperature=0)
compressor = LLMChainExtractor.from_llm(llm)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever()
)

# Returns only relevant parts of documents
compressed_docs = compression_retriever.get_relevant_documents(query)

# 5. Query Decomposition
def decompose_query(query, llm):
    prompt = f"""Break down this complex question into simpler sub-questions:
    Question: {query}
    
    Sub-questions:"""
    response = llm(prompt)
    sub_queries = [q.strip() for q in response.split("\\n") if q.strip()]
    return sub_queries

sub_queries = decompose_query("How does RAG work and what are its benefits?", llm)
# Retrieve for each sub-query and combine
all_docs = []
for sub_query in sub_queries:
    docs = vectorstore.similarity_search(sub_query, k=2)
    all_docs.extend(docs)
# Deduplicate
unique_docs = list({doc.page_content: doc for doc in all_docs}.values())`,
            approaches: [
                "Start with reranking - often provides biggest improvement.",
                "Use query expansion for diverse or ambiguous queries.",
                "Implement parent-document retrieval for better context.",
                "Apply contextual compression to reduce noise.",
                "Use query decomposition for complex multi-part questions.",
                "Combine multiple techniques for best results.",
                "Monitor latency and cost when adding techniques.",
                "A/B test to measure impact of each technique.",
                "Consider your specific use case - not all techniques needed.",
                "Cache expanded queries and reranking results when possible."
            ]
        });

        // Q9 - Production deployment considerations
        questions.push({
            number: 9,
            title: "What are the key considerations for deploying RAG systems in production?",
            description:
                "Production RAG deployment requires addressing scalability, reliability, monitoring, cost optimization, and security. " +
                "Unlike prototypes, production systems must handle high traffic, maintain low latency, provide observability, " +
                "and ensure data privacy while managing costs effectively.",
            why:
                "Production RAG systems face real-world challenges: high query volumes, latency requirements, cost constraints, " +
                "and reliability needs. Without proper planning, systems can fail under load, become too expensive, or provide poor user experience.",
            what:
                "Key Areas: Scalability (horizontal scaling, caching, load balancing), Latency (async processing, streaming, caching), " +
                "Monitoring (metrics, logging, tracing), Cost (embedding costs, LLM costs, infrastructure), Security (data privacy, access control), " +
                "Reliability (error handling, fallbacks, retries), Data management (versioning, updates, consistency).",
            how:
                "1) Design for horizontal scaling (stateless services, shared vector DB), 2) Implement caching (query results, embeddings), " +
                "3) Add comprehensive monitoring (retrieval quality, latency, costs), 4) Optimize costs (batch embeddings, model selection), " +
                "5) Implement security (encryption, access control, audit logs), 6) Add error handling and fallbacks, 7) Plan for data updates.",
            pros: [
                "Scalable architecture handles growth",
                "Monitoring enables proactive issue detection",
                "Cost optimization reduces operational expenses",
                "Security ensures data protection",
                "Reliability improves user experience",
                "Proper planning prevents production issues"
            ],
            cons: [
                "More complex than prototype",
                "Requires additional infrastructure",
                "Higher initial development cost",
                "Need for DevOps expertise",
                "Ongoing maintenance required",
                "More moving parts to manage"
            ],
            diagram: `flowchart TD
    A["User Requests"] --> B["Load Balancer"]
    B --> C["RAG Service 1"]
    B --> D["RAG Service 2"]
    B --> E["RAG Service N"]
    C --> F["Cache Layer"]
    D --> F
    E --> F
    F --> G["Vector Database"]
    C --> H["LLM Service"]
    D --> H
    E --> H
    G --> I["Monitoring"]
    H --> I
    I --> J["Metrics & Logs"]`,
            implementation: `# Production RAG Deployment
from fastapi import FastAPI, BackgroundTasks
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache
import redis
from prometheus_client import Counter, Histogram
import logging

# Caching setup
redis_client = redis.Redis(host='localhost', port=6379, db=0)
set_llm_cache(InMemoryCache())

# Metrics
query_counter = Counter('rag_queries_total', 'Total RAG queries')
query_latency = Histogram('rag_query_duration_seconds', 'RAG query latency')

# Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# Cached retrieval
def get_cached_embeddings(text):
    cache_key = f"embedding:{hash(text)}"
    cached = redis_client.get(cache_key)
    if cached:
        return pickle.loads(cached)
    embedding = embeddings.embed_query(text)
    redis_client.setex(cache_key, 3600, pickle.dumps(embedding))
    return embedding

# Async processing
async def process_query_async(query: str):
    try:
        with query_latency.time():
            query_counter.inc()
            
            # Check cache
            cache_key = f"result:{hash(query)}"
            cached_result = redis_client.get(cache_key)
            if cached_result:
                logger.info(f"Cache hit for query: {query}")
                return pickle.loads(cached_result)
            
            # Process query
            docs = vectorstore.similarity_search(query, k=3)
            result = llm_chain.run(context=docs, question=query)
            
            # Cache result
            redis_client.setex(cache_key, 1800, pickle.dumps(result))
            
            logger.info(f"Query processed: {query}")
            return result
            
    except Exception as e:
        logger.error(f"Error processing query: {e}", exc_info=True)
        # Fallback response
        return "I apologize, but I'm having trouble processing your query right now."

@app.post("/query")
async def query_endpoint(query: str, background_tasks: BackgroundTasks):
    result = await process_query_async(query)
    # Log asynchronously
    background_tasks.add_task(log_query, query, result)
    return {"answer": result}

def log_query(query, result):
    # Log to external system
    logger.info(f"Query: {query}, Result length: {len(result)}")

# Health check
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "vector_db": check_vector_db_health(),
        "llm_service": check_llm_health()
    }`,
            approaches: [
                "Start with horizontal scaling architecture from day one.",
                "Implement caching at multiple levels (embeddings, results).",
                "Use async processing for non-blocking operations.",
                "Add comprehensive monitoring and alerting.",
                "Implement circuit breakers for external services.",
                "Use message queues for high-volume scenarios.",
                "Plan for cost optimization (batch processing, model selection).",
                "Implement proper error handling and graceful degradation.",
                "Add rate limiting to prevent abuse.",
                "Use CDN for static content and caching."
            ]
        });

        // Q10 - Handling long context windows
        questions.push({
            number: 10,
            title: "How do you handle long documents and context window limitations in RAG?",
            description:
                "LLMs have token limits (e.g., 4K, 8K, 32K, 128K tokens), but documents can be much longer. " +
                "RAG systems must efficiently handle long documents by chunking, selecting relevant chunks, " +
                "and managing context window usage to provide complete answers without truncation.",
            why:
                "Long documents exceed context windows, requiring strategies to select and prioritize information. " +
                "Poor handling leads to incomplete answers, lost context, or wasted tokens on irrelevant information.",
            what:
                "Strategies: Hierarchical chunking (sections → paragraphs), Sliding window (overlapping chunks), " +
                "Summary-based (summarize chunks, then retrieve), Map-reduce (process chunks separately, combine), " +
                "Refine (iteratively refine answer), Context compression (extract only relevant parts), " +
                "Long-context models (use models with larger windows when available).",
            how:
                "1) Chunk long documents hierarchically, 2) Retrieve top-k most relevant chunks, " +
                "3) Use summarization for very long contexts, 4) Implement map-reduce for processing, " +
                "5) Compress context to essential information, 6) Use refine pattern for iterative answers, " +
                "7) Consider long-context models for specific use cases.",
            pros: [
                "Hierarchical chunking: Preserves document structure",
                "Sliding window: Captures boundary context",
                "Summary-based: Reduces token usage",
                "Map-reduce: Handles very long documents",
                "Context compression: Focuses on relevant info",
                "Refine pattern: Improves answer quality"
            ],
            cons: [
                "Hierarchical: More complex implementation",
                "Sliding window: Higher storage costs",
                "Summary-based: May lose details",
                "Map-reduce: Higher latency",
                "Context compression: Additional processing",
                "Refine pattern: Multiple LLM calls"
            ],
            diagram: `flowchart TD
    A["Long Document"] --> B["Hierarchical Chunking"]
    B --> C["Section Level"]
    B --> D["Paragraph Level"]
    C --> E["Retrieve Relevant Sections"]
    D --> F["Retrieve Relevant Paragraphs"]
    E --> G["Summarize if Needed"]
    F --> G
    G --> H["Compress Context"]
    H --> I["Fit in Context Window"]
    I --> J["LLM Generation"]`,
            implementation: `# Handling Long Documents
from langchain.chains import MapReduceDocumentsChain, RefineDocumentsChain
from langchain.chains.summarize import load_summarize_chain
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Strategy 1: Map-Reduce for long documents
def map_reduce_summarize(long_document):
    # Split into chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=4000,
        chunk_overlap=200
    )
    chunks = text_splitter.split_text(long_document)
    
    # Map: Summarize each chunk
    map_prompt = """Summarize the following text:
    {text}
    Summary:"""
    
    # Reduce: Combine summaries
    reduce_prompt = """Combine these summaries into a final summary:
    {text}
    Final Summary:"""
    
    chain = load_summarize_chain(
        llm,
        chain_type="map_reduce",
        map_prompt=map_prompt,
        combine_prompt=reduce_prompt
    )
    
    return chain.run(chunks)

# Strategy 2: Refine pattern
def refine_summarize(long_document):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=4000,
        chunk_overlap=200
    )
    chunks = text_splitter.split_text(long_document)
    
    initial_prompt = """Write a summary of the following:
    {text}
    Summary:"""
    
    refine_prompt = """Your job is to produce a final summary.
    We have provided an existing summary: {existing_answer}
    We have the opportunity to refine the summary with more context:
    {text}
    If the context is useful, refine the summary. Otherwise, return the original summary.
    Refined Summary:"""
    
    chain = load_summarize_chain(
        llm,
        chain_type="refine",
        question_prompt=initial_prompt,
        refine_prompt=refine_prompt
    )
    
    return chain.run(chunks)

# Strategy 3: Context compression for RAG
from langchain.retrievers.document_compressors import LLMChainExtractor

def compress_context(query, retrieved_docs):
    compressor = LLMChainExtractor.from_llm(llm)
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=vectorstore.as_retriever()
    )
    
    # Returns only relevant parts
    compressed_docs = compression_retriever.get_relevant_documents(query)
    return compressed_docs

# Strategy 4: Hierarchical retrieval
def hierarchical_retrieval(query, long_document):
    # First level: Section summaries
    sections = split_into_sections(long_document)
    section_summaries = [summarize_section(s) for s in sections]
    
    # Retrieve relevant sections
    relevant_sections = vectorstore.similarity_search(
        query, 
        filter={"type": "section"},
        k=3
    )
    
    # Second level: Detailed chunks from relevant sections
    detailed_chunks = []
    for section in relevant_sections:
        chunks = get_chunks_from_section(section.id)
        detailed_chunks.extend(chunks)
    
    # Final retrieval from detailed chunks
    final_docs = vectorstore.similarity_search(
        query,
        filter={"section_id": [s.id for s in relevant_sections]},
        k=5
    )
    
    return final_docs`,
            approaches: [
                "Use map-reduce for very long documents that need full processing.",
                "Use refine pattern when you need high-quality summaries.",
                "Implement hierarchical chunking for structured documents.",
                "Compress context to fit within token limits.",
                "Prioritize most relevant chunks when context is limited.",
                "Consider long-context models (Claude, GPT-4 Turbo) when available.",
                "Use sliding window for documents with important boundary information.",
                "Cache summaries to avoid recomputation.",
                "Monitor token usage and optimize chunk sizes.",
                "Test with your specific document types and lengths."
            ]
        });

        // Q11 - RAG optimization and cost reduction
        questions.push({
            number: 11,
            title: "How do you optimize RAG systems for cost and performance?",
            description:
                "RAG systems can be expensive due to embedding generation, vector storage, and LLM API calls. " +
                "Optimization involves reducing API calls, caching strategically, choosing efficient models, " +
                "batch processing, and implementing smart retrieval to minimize token usage.",
            why:
                "Costs can scale quickly with high query volumes. Without optimization, RAG systems can become prohibitively expensive. " +
                "Performance optimization also improves user experience through lower latency and better throughput.",
            what:
                "Cost Optimization: Caching (embeddings, results), Batch processing (embeddings), Model selection (smaller models when possible), " +
                "Token optimization (shorter prompts, compression), Smart retrieval (fewer chunks), Reuse embeddings. " +
                "Performance: Async processing, Parallel retrieval, Connection pooling, CDN for static content, Efficient indexing.",
            how:
                "1) Cache embeddings and query results, 2) Batch embedding generation, 3) Use smaller/faster models when quality allows, " +
                "4) Optimize prompts to reduce tokens, 5) Implement smart retrieval (fewer, better chunks), " +
                "6) Use async/parallel processing, 7) Monitor and optimize hot paths, 8) Implement rate limiting and quotas.",
            pros: [
                "Significant cost reduction (50-80% possible)",
                "Improved latency and throughput",
                "Better resource utilization",
                "Scalable to higher volumes",
                "Sustainable long-term operation",
                "Better user experience"
            ],
            cons: [
                "Requires careful implementation",
                "May need trade-offs in quality",
                "Cache invalidation complexity",
                "More complex monitoring",
                "Initial optimization effort",
                "Need to balance cost vs quality"
            ],
            diagram: `flowchart TD
    A["Query"] --> B{"Cache Check"}
    B -->|Hit| C["Return Cached"]
    B -->|Miss| D["Batch Embedding"]
    D --> E["Smart Retrieval"]
    E --> F["Context Compression"]
    F --> G["Optimized Prompt"]
    G --> H["Efficient LLM Call"]
    H --> I["Cache Result"]
    I --> J["Return Answer"]`,
            implementation: `# RAG Cost Optimization
import asyncio
from langchain.cache import InMemoryCache, SQLiteCache
from langchain.globals import set_llm_cache
import redis
import hashlib

# 1. Caching
redis_cache = redis.Redis(host='localhost', port=6379)
set_llm_cache(SQLiteCache(database_path=".langchain.db"))

def get_cached_embedding(text):
    key = f"emb:{hashlib.md5(text.encode()).hexdigest()}"
    cached = redis_cache.get(key)
    if cached:
        return pickle.loads(cached)
    return None

def cache_embedding(text, embedding):
    key = f"emb:{hashlib.md5(text.encode()).hexdigest()}"
    redis_cache.setex(key, 86400 * 7, pickle.dumps(embedding))  # 7 days

# 2. Batch embedding generation
async def batch_embed_documents(documents, batch_size=100):
    embeddings = []
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i+batch_size]
        # Check cache first
        batch_embeddings = []
        texts_to_embed = []
        indices = []
        
        for j, doc in enumerate(batch):
            cached = get_cached_embedding(doc.page_content)
            if cached:
                batch_embeddings.append((j, cached))
            else:
                texts_to_embed.append(doc.page_content)
                indices.append(j)
        
        # Batch embed uncached texts
        if texts_to_embed:
            new_embeddings = await embeddings_model.aembed_documents(texts_to_embed)
            for idx, emb in zip(indices, new_embeddings):
                batch_embeddings.append((idx, emb))
                cache_embedding(batch[idx].page_content, emb)
        
        # Sort by original index
        batch_embeddings.sort(key=lambda x: x[0])
        embeddings.extend([emb for _, emb in batch_embeddings])
    
    return embeddings

# 3. Smart retrieval (fewer, better chunks)
def optimized_retrieval(query, k=3):
    # Initial broad retrieval
    initial_docs = vectorstore.similarity_search(query, k=k*2)
    
    # Rerank to get best k
    reranked = rerank_documents(query, initial_docs, top_k=k)
    
    # Compress context
    compressed = compress_context(query, reranked)
    
    return compressed

# 4. Token-optimized prompts
def create_optimized_prompt(query, context):
    # Use concise prompt template
    prompt = f"""Q: {query}

Context: {compress_text(context, max_tokens=2000)}

A:"""
    return prompt

def compress_text(text, max_tokens=2000):
    # Simple compression: keep first and last parts
    tokens = text.split()
    if len(tokens) <= max_tokens:
        return text
    
    # Keep important parts
    first_part = ' '.join(tokens[:max_tokens//2])
    last_part = ' '.join(tokens[-max_tokens//2:])
    return f"{first_part}... [truncated] ...{last_part}"

# 5. Model selection based on complexity
def select_model(query_complexity):
    if query_complexity == "simple":
        return "gpt-3.5-turbo"  # Cheaper
    elif query_complexity == "medium":
        return "gpt-4"
    else:
        return "gpt-4-turbo"

# 6. Async processing
async def process_query_optimized(query):
    # Parallel operations
    embedding_task = asyncio.create_task(
        get_embedding_async(query)
    )
    cache_check_task = asyncio.create_task(
        check_cache_async(query)
    )
    
    embedding, cached_result = await asyncio.gather(
        embedding_task, cache_check_task
    )
    
    if cached_result:
        return cached_result
    
    # Continue with retrieval and generation
    docs = await retrieve_async(embedding)
    result = await generate_async(query, docs)
    
    # Cache result
    await cache_result_async(query, result)
    
    return result`,
            approaches: [
                "Implement multi-level caching (embeddings, results, intermediate).",
                "Batch embedding generation to reduce API calls.",
                "Use smaller models for simple queries, larger for complex ones.",
                "Optimize prompts to minimize token usage.",
                "Implement smart retrieval to reduce context size.",
                "Use async/parallel processing for better throughput.",
                "Monitor costs per query and optimize hot paths.",
                "Implement result caching with appropriate TTLs.",
                "Consider self-hosted models for high-volume scenarios.",
                "Regularly review and optimize based on usage patterns."
            ]
        });

        // Q12 - Multi-modal RAG
        questions.push({
            number: 12,
            title: "How do you implement multi-modal RAG for documents with images, tables, and text?",
            description:
                "Multi-modal RAG extends traditional text-based RAG to handle images, tables, charts, and other non-text content. " +
                "This requires specialized embedding models for different modalities, unified retrieval strategies, " +
                "and generation that can reference and describe visual content.",
            why:
                "Real-world documents contain images, tables, diagrams, and charts that are crucial for understanding. " +
                "Text-only RAG misses this information, leading to incomplete or incorrect answers. Multi-modal RAG enables comprehensive document understanding.",
            what:
                "Components: Vision models (CLIP, BLIP for images), Table extraction (OCR, structure recognition), " +
                "Multi-modal embeddings (unified or separate embeddings per modality), Cross-modal retrieval (text query → image, image query → text), " +
                "Multi-modal generation (LLMs that can describe images, GPT-4V, Claude 3).",
            how:
                "1) Extract and process different modalities (text, images, tables), 2) Generate embeddings for each modality, " +
                "3) Store in unified or separate vector stores, 4) Implement cross-modal retrieval, " +
                "5) Use vision-capable LLMs for generation, 6) Combine modalities in final answer.",
            pros: [
                "Handles real-world document complexity",
                "Captures information from images and tables",
                "More comprehensive answers",
                "Better user experience",
                "Supports diverse query types",
                "Future-proof for rich documents"
            ],
            cons: [
                "More complex implementation",
                "Higher computational costs",
                "Requires vision models",
                "Larger storage requirements",
                "More challenging evaluation",
                "Integration complexity"
            ],
            diagram: `flowchart TD
    A["Multi-modal Document"] --> B["Text Extraction"]
    A --> C["Image Extraction"]
    A --> D["Table Extraction"]
    B --> E["Text Embeddings"]
    C --> F["Image Embeddings"]
    D --> G["Table Embeddings"]
    E --> H["Unified Vector Store"]
    F --> H
    G --> H
    I["User Query"] --> J["Query Embedding"]
    J --> K["Cross-Modal Retrieval"]
    H --> K
    K --> L["Retrieve Text"]
    K --> M["Retrieve Images"]
    K --> N["Retrieve Tables"]
    L --> O["Multi-modal LLM"]
    M --> O
    N --> O
    O --> P["Answer with References"]`,
            implementation: `# Multi-modal RAG
from langchain.document_loaders import PyPDFLoader
from PIL import Image
import pytesseract
from langchain.embeddings import OpenAIEmbeddings
from sentence_transformers import SentenceTransformer
import clip

# 1. Extract different modalities
def extract_multimodal_content(pdf_path):
    loader = PyPDFLoader(pdf_path)
    pages = loader.load()
    
    multimodal_chunks = []
    
    for page in pages:
        # Text content
        text_chunk = {
            "type": "text",
            "content": page.page_content,
            "page": page.metadata.get("page", 0)
        }
        multimodal_chunks.append(text_chunk)
        
        # Extract images (if PDF has images)
        # This is simplified - real implementation needs PDF image extraction
        images = extract_images_from_page(page)
        for img in images:
            img_chunk = {
                "type": "image",
                "content": img,
                "page": page.metadata.get("page", 0)
            }
            multimodal_chunks.append(img_chunk)
        
        # Extract tables
        tables = extract_tables_from_page(page)
        for table in tables:
            table_chunk = {
                "type": "table",
                "content": table,
                "page": page.metadata.get("page", 0)
            }
            multimodal_chunks.append(table_chunk)
    
    return multimodal_chunks

# 2. Generate embeddings for each modality
text_embeddings = OpenAIEmbeddings()
image_model = SentenceTransformer('clip-ViT-B-32')

def embed_multimodal_chunk(chunk):
    if chunk["type"] == "text":
        return text_embeddings.embed_query(chunk["content"])
    elif chunk["type"] == "image":
        # CLIP can embed images
        return image_model.encode(chunk["content"])
    elif chunk["type"] == "table":
        # Convert table to text description
        table_text = table_to_text(chunk["content"])
        return text_embeddings.embed_query(table_text)

# 3. Store in vector database with metadata
def store_multimodal_chunks(chunks):
    embeddings = []
    documents = []
    metadatas = []
    
    for chunk in chunks:
        embedding = embed_multimodal_chunk(chunk)
        embeddings.append(embedding)
        
        # Create document representation
        if chunk["type"] == "text":
            doc_text = chunk["content"]
        elif chunk["type"] == "image":
            doc_text = f"[Image on page {chunk['page']}]"
        elif chunk["type"] == "table":
            doc_text = table_to_text(chunk["content"])
        
        documents.append(doc_text)
        metadatas.append({
            "type": chunk["type"],
            "page": chunk["page"],
            "original_content": chunk["content"] if chunk["type"] != "image" else None
        })
    
    vectorstore.add(embeddings=embeddings, documents=documents, metadatas=metadatas)

# 4. Cross-modal retrieval
def cross_modal_retrieval(query, k=5):
    # Embed query as text
    query_embedding = text_embeddings.embed_query(query)
    
    # Search across all modalities
    results = vectorstore.similarity_search_by_vector(
        query_embedding,
        k=k,
        filter=None  # Can filter by type if needed
    )
    
    return results

# 5. Multi-modal generation with GPT-4V
from openai import OpenAI

client = OpenAI()

def generate_multimodal_answer(query, retrieved_chunks):
    # Prepare context with images
    context_parts = []
    images = []
    
    for chunk in retrieved_chunks:
        if chunk.metadata["type"] == "text":
            context_parts.append(f"Text: {chunk.page_content}")
        elif chunk.metadata["type"] == "image":
            context_parts.append(f"[Image reference: page {chunk.metadata['page']}]")
            images.append(chunk.metadata["original_content"])
        elif chunk.metadata["type"] == "table":
            context_parts.append(f"Table: {chunk.page_content}")
    
    context = "\\n".join(context_parts)
    
    # Use GPT-4V for vision capabilities
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": f"Question: {query}\\n\\nContext: {context}\\n\\nAnswer the question based on the context."},
                *[{"type": "image_url", "image_url": {"url": img}} for img in images[:4]]  # Max 4 images
            ]
        }
    ]
    
    response = client.chat.completions.create(
        model="gpt-4-vision-preview",
        messages=messages,
        max_tokens=500
    )
    
    return response.choices[0].message.content`,
            approaches: [
                "Start with text RAG, add modalities incrementally.",
                "Use CLIP for image embeddings, specialized models for tables.",
                "Store modalities separately or with type metadata.",
                "Implement cross-modal retrieval for flexible queries.",
                "Use vision-capable LLMs (GPT-4V, Claude 3) for generation.",
                "Optimize image processing and storage.",
                "Handle table extraction carefully (structure matters).",
                "Consider costs of vision models.",
                "Test with diverse document types.",
                "Provide fallbacks when modalities aren't available."
            ]
        });

        // Q13 - RAG with streaming responses
        questions.push({
            number: 13,
            title: "How do you implement streaming responses in RAG systems?",
            description:
                "Streaming responses provide real-time feedback to users by sending tokens as they're generated, rather than waiting for the complete response. " +
                "This improves perceived latency and user experience, especially for longer answers. RAG systems can stream both retrieval status and generation.",
            why:
                "Users expect fast, responsive interfaces. Streaming reduces perceived latency by showing progress immediately. " +
                "For RAG systems generating long answers, streaming is essential for good UX.",
            what:
                "Streaming Components: Token streaming (send tokens as generated), Status updates (retrieval progress), " +
                "Partial results (show retrieved chunks), Error streaming (stream errors as they occur). " +
                "Technologies: Server-Sent Events (SSE), WebSockets, Streaming APIs (OpenAI streaming, LangChain streaming).",
            how:
                "1) Use streaming-capable LLM APIs, 2) Implement SSE or WebSocket endpoints, " +
                "3) Stream retrieval status updates, 4) Stream generated tokens, 5) Handle errors gracefully, " +
                "6) Implement client-side rendering of streamed content.",
            pros: [
                "Improved user experience",
                "Reduced perceived latency",
                "Better for long responses",
                "Real-time feedback",
                "More engaging interface",
                "Progressive disclosure"
            ],
            cons: [
                "More complex implementation",
                "Requires streaming infrastructure",
                "Error handling complexity",
                "Client-side complexity",
                "May need connection management",
                "Testing complexity"
            ],
            diagram: `flowchart LR
    A["User Query"] --> B["Stream: Retrieval Start"]
    B --> C["Stream: Retrieving Docs"]
    C --> D["Stream: Found 3 docs"]
    D --> E["Stream: Generating Answer"]
    E --> F["Stream: Token 1"]
    F --> G["Stream: Token 2"]
    G --> H["Stream: Token N"]
    H --> I["Stream: Complete"]`,
            implementation: `# Streaming RAG Implementation
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.base import BaseCallbackHandler
import json
import asyncio

app = FastAPI()

class StreamingCallbackHandler(BaseCallbackHandler):
    def __init__(self, queue):
        self.queue = queue
    
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.queue.put(f"data: {json.dumps({'type': 'token', 'content': token})}\\n\\n")
    
    def on_retriever_start(self, query: str, **kwargs) -> None:
        self.queue.put(f"data: {json.dumps({'type': 'status', 'content': 'Retrieving documents...'})}\\n\\n")
    
    def on_retriever_end(self, documents, **kwargs) -> None:
        self.queue.put(f"data: {json.dumps({'type': 'status', 'content': f'Found {len(documents)} documents'})}\\n\\n")

async def stream_rag_response(query: str):
    queue = asyncio.Queue()
    callback = StreamingCallbackHandler(queue)
    
    # Stream retrieval status
    yield f"data: {json.dumps({'type': 'status', 'content': 'Starting retrieval...'})}\\n\\n"
    
    # Retrieve documents
    docs = vectorstore.similarity_search(query, k=3)
    yield f"data: {json.dumps({'type': 'status', 'content': f'Retrieved {len(docs)} documents'})}\\n\\n"
    
    # Stream generation
    yield f"data: {json.dumps({'type': 'status', 'content': 'Generating answer...'})}\\n\\n"
    
    # Create chain with streaming
    chain = ConversationalRetrievalChain.from_llm(
        llm=ChatOpenAI(
            streaming=True,
            callbacks=[callback],
            temperature=0
        ),
        retriever=vectorstore.as_retriever(),
        return_source_documents=True
    )
    
    # Start generation in background
    async def generate():
        result = await chain.ainvoke({"question": query, "chat_history": []})
        queue.put(f"data: {json.dumps({'type': 'done', 'sources': [d.page_content[:100] for d in result['source_documents']]})}\\n\\n")
        queue.put(None)  # Signal completion
    
    asyncio.create_task(generate())
    
    # Stream tokens
    while True:
        item = await queue.get()
        if item is None:
            break
        yield item

@app.post("/query/stream")
async def stream_query(query: str):
    return StreamingResponse(
        stream_rag_response(query),
        media_type="text/event-stream"
    )

# Client-side (JavaScript)
# const eventSource = new EventSource('/query/stream?query=What is RAG?');
# eventSource.onmessage = (event) => {
#     const data = JSON.parse(event.data);
#     if (data.type === 'token') {
#         appendToAnswer(data.content);
#     } else if (data.type === 'status') {
#         updateStatus(data.content);
#     }
# };`,
            approaches: [
                "Use streaming-capable LLM APIs (OpenAI, Anthropic).",
                "Implement Server-Sent Events for simple streaming.",
                "Use WebSockets for bidirectional communication.",
                "Stream status updates for better UX.",
                "Handle connection errors gracefully.",
                "Implement client-side buffering for smooth display.",
                "Consider rate limiting for streaming endpoints.",
                "Monitor streaming performance and latency.",
                "Test with various network conditions.",
                "Provide fallback to non-streaming mode."
            ]
        });

        // Q14 - RAG with conversation memory
        questions.push({
            number: 14,
            title: "How do you implement conversational RAG with memory and context?",
            description:
                "Conversational RAG maintains conversation history to handle follow-up questions, references to previous answers, " +
                "and multi-turn dialogues. This requires managing chat history, context windows, and ensuring retrieved documents " +
                "remain relevant across conversation turns.",
            why:
                "Users ask follow-up questions and reference previous answers. Without conversation memory, each query is isolated, " +
                "leading to poor user experience and inability to handle natural conversation flow.",
            what:
                "Components: Conversation memory (stores chat history), Context management (tracks conversation state), " +
                "Query rewriting (reformulates follow-ups with context), History-aware retrieval (considers conversation context), " +
                "Memory compression (summarizes old messages to fit context).",
            how:
                "1) Store conversation history, 2) Rewrite queries with context, 3) Retrieve documents considering conversation, " +
                "4) Generate answers with conversation context, 5) Update memory, 6) Compress old history if needed.",
            pros: [
                "Natural conversation flow",
                "Handles follow-up questions",
                "Better user experience",
                "Context-aware responses",
                "More engaging interactions",
                "Supports complex multi-turn tasks"
            ],
            cons: [
                "Increased complexity",
                "Context window management",
                "Higher token costs",
                "Memory management needed",
                "Potential context drift",
                "More challenging debugging"
            ],
            diagram: `flowchart TD
    A["User Query"] --> B["Conversation History"]
    B --> C["Query Rewriting"]
    C --> D["Context-Aware Retrieval"]
    D --> E["Retrieve Documents"]
    E --> F["Generate with Context"]
    F --> G["Update Memory"]
    G --> H["Return Answer"]
    H --> I["Store in History"]
    I --> B`,
            implementation: `# Conversational RAG
from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI

# Option 1: Buffer Memory (simple, stores all)
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)

# Option 2: Summary Memory (compresses old messages)
memory = ConversationSummaryMemory(
    llm=ChatOpenAI(temperature=0),
    memory_key="chat_history",
    return_messages=True
)

# Create conversational chain
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=ChatOpenAI(temperature=0),
    retriever=vectorstore.as_retriever(),
    memory=memory,
    return_source_documents=True,
    verbose=True
)

# Query with conversation
result1 = qa_chain({"question": "What is RAG?"})
print(result1["answer"])

# Follow-up question (uses conversation history)
result2 = qa_chain({"question": "How does it work?"})
print(result2["answer"])

# Advanced: Query rewriting with context
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

def rewrite_query_with_context(query, chat_history):
    rewrite_prompt = PromptTemplate(
        input_variables=["question", "chat_history"],
        template="""Given the following conversation and a follow-up question, rephrase the follow-up question to be a standalone question.

Chat History:
{chat_history}

Follow-up Question: {question}
Standalone Question:"""
    )
    
    llm = ChatOpenAI(temperature=0)
    rewrite_chain = LLMChain(llm=llm, prompt=rewrite_prompt)
    
    standalone_query = rewrite_chain.run(
        question=query,
        chat_history=chat_history
    )
    
    return standalone_query

# Use rewritten query for retrieval
chat_history = memory.chat_memory.messages
rewritten_query = rewrite_query_with_context("How does it work?", chat_history)
docs = vectorstore.similarity_search(rewritten_query, k=3)

# Custom memory with compression
class CompressedConversationMemory:
    def __init__(self, max_tokens=2000):
        self.messages = []
        self.max_tokens = max_tokens
    
    def add_message(self, role, content):
        self.messages.append({"role": role, "content": content})
        self._compress_if_needed()
    
    def _compress_if_needed(self):
        total_tokens = sum(len(msg["content"].split()) for msg in self.messages)
        
        if total_tokens > self.max_tokens:
            # Summarize old messages
            old_messages = self.messages[:-2]  # Keep last 2
            summary = self._summarize_messages(old_messages)
            
            # Replace with summary
            self.messages = [
                {"role": "system", "content": f"Previous conversation summary: {summary}"},
                *self.messages[-2:]
            ]
    
    def _summarize_messages(self, messages):
        # Use LLM to summarize
        text = "\\n".join([f"{m['role']}: {m['content']}" for m in messages])
        prompt = f"Summarize this conversation:\\n{text}\\nSummary:"
        return llm(prompt)
    
    def get_history(self):
        return self.messages`,
            approaches: [
                "Start with ConversationBufferMemory for simplicity.",
                "Use ConversationSummaryMemory for longer conversations.",
                "Implement query rewriting for better follow-up handling.",
                "Monitor context window usage and compress when needed.",
                "Store conversation history in database for persistence.",
                "Implement session management for multi-user scenarios.",
                "Consider conversation context in retrieval.",
                "Test with various conversation patterns.",
                "Handle edge cases (empty history, very long history).",
                "Provide conversation reset functionality."
            ]
        });

        // Q15 - RAG evaluation with RAGAS
        questions.push({
            number: 15,
            title: "How do you use RAGAS for comprehensive RAG evaluation?",
            description:
                "RAGAS (Retrieval-Augmented Generation Assessment) is a framework for evaluating RAG systems. " +
                "It provides metrics for retrieval quality, generation quality, and end-to-end performance. " +
                "RAGAS uses LLMs to evaluate answers, making it scalable and automated.",
            why:
                "Manual evaluation is time-consuming and doesn't scale. RAGAS provides automated, comprehensive evaluation " +
                "that helps identify specific issues in retrieval or generation, enabling data-driven improvements.",
            what:
                "RAGAS Metrics: Faithfulness (answer grounded in context), Answer Relevancy (answer relevant to question), " +
                "Context Precision (retrieved context relevant), Context Recall (all relevant context retrieved), " +
                "Answer Correctness (answer matches ground truth), Answer Similarity (semantic similarity to ground truth).",
            how:
                "1) Prepare evaluation dataset (questions, ground truth, contexts), 2) Run RAG system on questions, " +
                "3) Use RAGAS to compute metrics, 4) Analyze results, 5) Identify bottlenecks, 6) Iterate and improve.",
            pros: [
                "Automated evaluation",
                "Comprehensive metrics",
                "Identifies specific issues",
                "Scalable to large datasets",
                "Reproducible results",
                "Data-driven optimization"
            ],
            cons: [
                "Requires evaluation dataset",
                "LLM-based evaluation has costs",
                "May have evaluation biases",
                "Requires understanding metrics",
                "Setup complexity",
                "Need to interpret results"
            ],
            diagram: `flowchart TD
    A["Evaluation Dataset"] --> B["RAG System"]
    B --> C["Answers"]
    B --> D["Retrieved Contexts"]
    A --> E["Ground Truth"]
    C --> F["RAGAS Evaluation"]
    D --> F
    E --> F
    F --> G["Faithfulness"]
    F --> H["Answer Relevancy"]
    F --> I["Context Precision"]
    F --> J["Context Recall"]
    G --> K["Evaluation Report"]
    H --> K
    I --> K
    J --> K`,
            implementation: `# RAGAS Evaluation
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
    answer_correctness,
    answer_similarity
)
from datasets import Dataset
import pandas as pd

# Prepare evaluation dataset
eval_data = {
    "question": [
        "What is RAG?",
        "How does retrieval work in RAG?",
        "What are the benefits of RAG?"
    ],
    "answer": [
        "RAG is Retrieval-Augmented Generation, a technique that combines retrieval and generation.",
        "Retrieval uses vector similarity search to find relevant documents.",
        "RAG provides up-to-date information and source citations."
    ],
    "contexts": [
        [
            "RAG combines retrieval mechanisms with language models.",
            "It retrieves relevant documents before generation."
        ],
        [
            "Vector search finds similar documents using embeddings.",
            "Similarity is computed using cosine distance."
        ],
        [
            "RAG enables access to current information.",
            "It provides transparency through source citations."
        ]
    ],
    "ground_truth": [
        "RAG is a technique that enhances LLMs by retrieving relevant information.",
        "Retrieval in RAG uses embedding-based similarity search.",
        "Benefits include current information access and source transparency."
    ]
}

dataset = Dataset.from_dict(eval_data)

# Run evaluation
results = evaluate(
    dataset=dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
        answer_correctness,
        answer_similarity
    ]
)

print(results)

# Custom evaluation with RAGAS
from ragas.metrics.base import Metric

class CustomMetric(Metric):
    name = "custom_metric"
    
    def score(self, row):
        # Custom scoring logic
        question = row["question"]
        answer = row["answer"]
        # Compute custom score
        return 0.85  # Example score

# Use custom metric
results = evaluate(
    dataset=dataset,
    metrics=[faithfulness, answer_relevancy, CustomMetric()]
)

# Batch evaluation
def evaluate_rag_system(rag_system, test_questions):
    results = []
    
    for question in test_questions:
        # Run RAG system
        result = rag_system.query(question)
        
        # Prepare for RAGAS
        eval_row = {
            "question": question,
            "answer": result["answer"],
            "contexts": [doc.page_content for doc in result["source_documents"]],
            "ground_truth": get_ground_truth(question)  # From your dataset
        }
        
        results.append(eval_row)
    
    # Evaluate
    eval_dataset = Dataset.from_list(results)
    scores = evaluate(
        dataset=eval_dataset,
        metrics=[faithfulness, answer_relevancy, context_precision]
    )
    
    return scores

# Continuous evaluation
def continuous_evaluation(rag_system, evaluation_interval=100):
    query_count = 0
    evaluation_batch = []
    
    def evaluate_batch():
        if len(evaluation_batch) > 0:
            eval_dataset = Dataset.from_list(evaluation_batch)
            scores = evaluate(
                dataset=eval_dataset,
                metrics=[faithfulness, answer_relevancy]
            )
            log_metrics(scores)
            evaluation_batch.clear()
    
    # In your query handler
    def handle_query(question):
        nonlocal query_count
        result = rag_system.query(question)
        query_count += 1
        
        # Add to evaluation batch
        evaluation_batch.append({
            "question": question,
            "answer": result["answer"],
            "contexts": [doc.page_content for doc in result["source_documents"]]
        })
        
        # Evaluate periodically
        if query_count % evaluation_interval == 0:
            evaluate_batch()
        
        return result`,
            approaches: [
                "Start with core metrics (faithfulness, answer_relevancy).",
                "Create diverse evaluation dataset covering edge cases.",
                "Run evaluations regularly to track performance.",
                "Use RAGAS for automated evaluation, human for validation.",
                "Monitor metrics over time to detect regressions.",
                "Focus on metrics relevant to your use case.",
                "Combine RAGAS with custom metrics if needed.",
                "Set up continuous evaluation in production.",
                "Use evaluation results to guide improvements.",
                "Document evaluation methodology for reproducibility."
            ]
        });

        // Q16 - RAG with graph databases
        questions.push({
            number: 16,
            title: "How do you combine RAG with graph databases for structured knowledge?",
            description:
                "Graph databases store relationships between entities, enabling structured knowledge retrieval. " +
                "Combining RAG with graph databases allows both semantic search (RAG) and relationship traversal (graph), " +
                "providing more precise and contextually rich answers.",
            why:
                "Some queries require understanding relationships (e.g., 'Who worked with X?', 'What are the dependencies?'). " +
                "Pure vector search may miss these relationship-based queries. Graph databases excel at relationship queries.",
            what:
                "Hybrid Approach: Vector search for semantic similarity, Graph traversal for relationships. " +
                "Components: Knowledge graph (entities and relationships), Vector embeddings (for semantic search), " +
                "Graph-RAG (combines both), Entity extraction (populates graph), Relationship extraction (creates edges).",
            how:
                "1) Extract entities and relationships from documents, 2) Build knowledge graph, 3) Generate embeddings for entities, " +
                "4) For queries: use vector search for semantic matches, graph traversal for relationships, " +
                "5) Combine results, 6) Generate answer with both semantic and relational context.",
            pros: [
                "Handles relationship queries",
                "More precise answers",
                "Structured knowledge representation",
                "Combines semantic and relational search",
                "Better for complex queries",
                "Enables reasoning over relationships"
            ],
            cons: [
                "More complex architecture",
                "Requires graph database",
                "Entity extraction complexity",
                "Higher infrastructure costs",
                "More challenging to maintain",
                "Integration complexity"
            ],
            diagram: `flowchart TD
    A["Documents"] --> B["Entity Extraction"]
    B --> C["Knowledge Graph"]
    B --> D["Vector Embeddings"]
    C --> E["Graph Database"]
    D --> F["Vector Database"]
    G["User Query"] --> H["Entity Recognition"]
    H --> I["Graph Traversal"]
    H --> J["Vector Search"]
    E --> I
    F --> J
    I --> K["Combine Results"]
    J --> K
    K --> L["Context Assembly"]
    L --> M["LLM Generation"]`,
            implementation: `# Graph-RAG Implementation
from neo4j import GraphDatabase
from langchain.graphs import Neo4jGraph
from langchain.vectorstores import Neo4jVector
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms import OpenAI

# 1. Setup Neo4j graph database
driver = GraphDatabase.driver("bolt://localhost:7687", auth=("neo4j", "password"))
graph = Neo4jGraph(driver=driver)

# 2. Extract entities and relationships
def extract_entities_relationships(text):
    prompt = f"""Extract entities and relationships from the following text:
    
    Text: {text}
    
    Return as JSON:
    {{
        "entities": [{{"name": "...", "type": "..."}}],
        "relationships": [{{"source": "...", "target": "...", "type": "..."}}]
    }}"""
    
    llm = OpenAI(temperature=0)
    result = llm(prompt)
    return json.loads(result)

# 3. Build knowledge graph
def build_knowledge_graph(documents):
    for doc in documents:
        entities_rel = extract_entities_relationships(doc.page_content)
        
        # Create entities
        for entity in entities_rel["entities"]:
            graph.query(
                """
                MERGE (e:Entity {{name: $name, type: $type}})
                """,
                {"name": entity["name"], "type": entity["type"]}
            )
        
        # Create relationships
        for rel in entities_rel["relationships"]:
            graph.query(
                """
                MATCH (a:Entity {{name: $source}})
                MATCH (b:Entity {{name: $target}})
                MERGE (a)-[r:RELATES {{type: $rel_type}}]->(b)
                """,
                {
                    "source": rel["source"],
                    "target": rel["target"],
                    "rel_type": rel["type"]
                }
            )

# 4. Create vector store with graph
vectorstore = Neo4jVector.from_documents(
    documents=documents,
    embedding=embeddings,
    url="bolt://localhost:7687",
    username="neo4j",
    password="password",
    index_name="document_embeddings"
)

# 5. Hybrid retrieval (graph + vector)
def hybrid_retrieval(query):
    # Vector search
    vector_results = vectorstore.similarity_search(query, k=5)
    
    # Extract entities from query
    query_entities = extract_entities_from_query(query)
    
    # Graph traversal
    graph_results = []
    for entity in query_entities:
        # Find related entities
        related = graph.query(
            """
            MATCH (e:Entity {{name: $name}})-[r]->(related:Entity)
            RETURN related.name as name, related.type as type, type(r) as relationship
            LIMIT 10
            """,
            {"name": entity}
        )
        graph_results.extend(related)
    
    # Combine results
    combined_context = combine_vector_and_graph_results(vector_results, graph_results)
    return combined_context

# 6. Query with graph context
def query_with_graph_rag(query):
    # Hybrid retrieval
    context = hybrid_retrieval(query)
    
    # Generate answer
    prompt = f"""Answer the question using the following context from documents and knowledge graph:
    
    Context:
    {format_context(context)}
    
    Question: {query}
    
    Answer:"""
    
    llm = OpenAI(temperature=0)
    answer = llm(prompt)
    
    return answer

# Alternative: Using LangChain's graph capabilities
from langchain.chains import GraphCypherQAChain

# Create graph QA chain
graph_qa_chain = GraphCypherQAChain.from_llm(
    llm=ChatOpenAI(temperature=0),
    graph=graph,
    verbose=True
)

# Query
result = graph_qa_chain.run("Who worked with Albert Einstein?")`,
            approaches: [
                "Start with simple entity extraction before complex relationships.",
                "Use Neo4j or similar graph database for relationships.",
                "Combine vector search with graph traversal.",
                "Extract entities and relationships during document ingestion.",
                "Use graph queries for relationship-based questions.",
                "Maintain both vector and graph representations.",
                "Consider using LangChain's graph integration.",
                "Monitor graph size and performance.",
                "Update graph as documents change.",
                "Test with relationship-heavy queries."
            ]
        });

        // Q17 - RAG with self-query and metadata filtering
        questions.push({
            number: 17,
            title: "How do you implement self-query retrieval with metadata filtering in RAG?",
            description:
                "Self-query retrieval uses LLMs to extract metadata filters and search terms from natural language queries. " +
                "This enables users to query with natural language while applying structured filters (date ranges, categories, authors, etc.), " +
                "combining semantic search with precise metadata filtering.",
            why:
                "Users want to ask natural questions like 'recent articles about AI' or 'papers by author X'. " +
                "Self-query automatically extracts both the semantic query and metadata filters, enabling powerful, intuitive search.",
            what:
                "Components: Query parser (extracts search terms and filters), Metadata extraction (identifies filter criteria), " +
                "Structured query (combines semantic search + filters), Metadata store (enables filtering). " +
                "Example: 'Recent AI papers' → query='AI papers', filter={'date': '>2023-01-01'}'.",
            how:
                "1) Define metadata schema, 2) Use LLM to parse query into search terms and filters, " +
                "3) Perform vector search with metadata filters, 4) Return filtered results, " +
                "5) Handle ambiguous queries gracefully.",
            pros: [
                "Natural language queries with filters",
                "More precise retrieval",
                "Better user experience",
                "Handles complex queries",
                "Flexible filtering",
                "Combines semantic and structured search"
            ],
            cons: [
                "Requires metadata schema",
                "LLM parsing can be imperfect",
                "More complex implementation",
                "Need to handle parsing errors",
                "Requires structured metadata",
                "Additional LLM call cost"
            ],
            diagram: `flowchart TD
    A["Natural Language Query"] --> B["LLM Query Parser"]
    B --> C["Extract Search Terms"]
    B --> D["Extract Metadata Filters"]
    C --> E["Vector Search"]
    D --> F["Apply Filters"]
    E --> G["Filtered Results"]
    F --> G
    G --> H["Return Top-K"]`,
            implementation: `# Self-Query Retrieval
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo
from langchain.llms import OpenAI

# Define metadata attributes
metadata_field_info = [
    AttributeInfo(
        name="author",
        description="The author of the document",
        type="string"
    ),
    AttributeInfo(
        name="date",
        description="The publication date of the document in YYYY-MM-DD format",
        type="date"
    ),
    AttributeInfo(
        name="category",
        description="The category of the document (e.g., AI, Science, Technology)",
        type="string"
    ),
    AttributeInfo(
        name="page_count",
        description="The number of pages in the document",
        type="integer"
    )
]

# Document content description
document_content_description = "Documents about AI, machine learning, and technology"

# Create self-query retriever
llm = OpenAI(temperature=0)
self_query_retriever = SelfQueryRetriever.from_llm(
    llm=llm,
    vectorstore=vectorstore,
    document_contents=document_content_description,
    metadata_field_info=metadata_field_info,
    verbose=True
)

# Natural language queries with filters
queries = [
    "Recent AI papers from 2023",
    "Articles by John Doe about machine learning",
    "Long documents (more than 50 pages) on deep learning",
    "Technology papers published after 2022"
]

for query in queries:
    docs = self_query_retriever.get_relevant_documents(query)
    print(f"Query: {query}")
    print(f"Retrieved {len(docs)} documents")
    print()

# Custom self-query implementation
def parse_query_with_filters(query, metadata_schema):
    prompt = f"""Parse this query into search terms and metadata filters.

Query: {query}

Metadata schema:
{json.dumps(metadata_schema, indent=2)}

Return JSON:
{{
    "search_terms": "semantic search query",
    "filters": {{
        "field": "value or condition"
    }}
}}"""
    
    llm = OpenAI(temperature=0)
    result = llm(prompt)
    parsed = json.loads(result)
    return parsed["search_terms"], parsed["filters"]

# Use parsed query
search_terms, filters = parse_query_with_filters(
    "Recent AI papers by John Doe",
    {
        "author": "string",
        "date": "date",
        "category": "string"
    }
)

# Perform filtered search
docs = vectorstore.similarity_search(
    search_terms,
    k=5,
    filter=filters
)`,
            approaches: [
                "Define clear metadata schema with descriptions.",
                "Use SelfQueryRetriever from LangChain for quick setup.",
                "Provide good examples in prompt for LLM parsing.",
                "Handle parsing errors gracefully with fallbacks.",
                "Test with various query patterns.",
                "Combine with other retrieval techniques.",
                "Monitor parsing accuracy and improve prompts.",
                "Consider fine-tuning for domain-specific queries.",
                "Cache parsed queries when possible.",
                "Validate extracted filters before applying."
            ]
        });

        // Q18 - RAG with time-based retrieval
        questions.push({
            number: 18,
            title: "How do you handle time-sensitive information and temporal queries in RAG?",
            description:
                "Many documents have temporal aspects (publication dates, event dates, version timestamps). " +
                "Time-based RAG enables queries like 'recent updates', 'papers from last year', or 'latest version'. " +
                "This requires temporal indexing, time-aware retrieval, and handling of document versions.",
            why:
                "Information changes over time. Users need current information, not outdated content. " +
                "Time-based retrieval ensures relevance by prioritizing recent or time-appropriate documents.",
            what:
                "Components: Temporal metadata (dates, timestamps), Time-aware indexing (index by time), " +
                "Temporal queries (date ranges, relative time), Version management (handle document updates), " +
                "Recency weighting (boost recent documents in retrieval).",
            how:
                "1) Extract and store temporal metadata, 2) Index documents with time information, " +
                "3) Parse temporal queries, 4) Apply time-based filters and weighting, " +
                "5) Handle document versions and updates, 6) Combine temporal and semantic search.",
            pros: [
                "Handles time-sensitive queries",
                "Prioritizes recent information",
                "Supports temporal filtering",
                "Better for dynamic content",
                "Handles document versions",
                "More relevant results"
            ],
            cons: [
                "Requires temporal metadata",
                "More complex indexing",
                "Need to handle time zones",
                "Version management complexity",
                "Storage overhead",
                "Query parsing complexity"
            ],
            diagram: `flowchart TD
    A["Document"] --> B["Extract Temporal Metadata"]
    B --> C["Publication Date"]
    B --> D["Last Updated"]
    B --> E["Event Date"]
    C --> F["Time-Indexed Vector Store"]
    D --> F
    E --> F
    G["Temporal Query"] --> H["Parse Time Expression"]
    H --> I["Extract Date Range"]
    I --> J["Time-Filtered Search"]
    F --> J
    J --> K["Recency-Weighted Results"]
    K --> L["Top-K Temporal Results"]`,
            implementation: `# Time-Based RAG
from datetime import datetime, timedelta
from langchain.vectorstores import Pinecone
import pytz

# 1. Extract temporal metadata
def extract_temporal_metadata(document):
    metadata = {
        "publication_date": extract_publication_date(document),
        "last_updated": extract_last_updated(document),
        "event_date": extract_event_date(document),
        "timestamp": datetime.now().isoformat()
    }
    return metadata

# 2. Time-aware indexing
def index_with_time(document, vectorstore):
    # Generate embedding
    embedding = embeddings.embed_query(document.page_content)
    
    # Add temporal metadata
    temporal_metadata = extract_temporal_metadata(document)
    
    # Store with temporal info
    vectorstore.add_texts(
        texts=[document.page_content],
        embeddings=[embedding],
        metadatas=[temporal_metadata],
        ids=[document.metadata.get("id")]
    )

# 3. Parse temporal queries
def parse_temporal_query(query):
    prompt = f"""Extract temporal information from this query:
    
    Query: {query}
    
    Return JSON:
    {{
        "search_terms": "non-temporal query text",
        "time_filter": {{
            "type": "relative|absolute|range",
            "value": "last week|2023-01-01|2023-01-01 to 2023-12-31"
        }}
    }}"""
    
    llm = OpenAI(temperature=0)
    result = llm(prompt)
    return json.loads(result)

# 4. Time-filtered retrieval
def time_filtered_retrieval(query, vectorstore):
    parsed = parse_temporal_query(query)
    search_terms = parsed["search_terms"]
    time_filter = parsed["time_filter"]
    
    # Build time filter
    if time_filter["type"] == "relative":
        # Parse relative time (e.g., "last week", "recent")
        date_threshold = parse_relative_time(time_filter["value"])
        filter_dict = {"publication_date": {"$gte": date_threshold.isoformat()}}
    elif time_filter["type"] == "absolute":
        filter_dict = {"publication_date": {"$eq": time_filter["value"]}}
    elif time_filter["type"] == "range":
        start, end = time_filter["value"].split(" to ")
        filter_dict = {
            "publication_date": {
                "$gte": start,
                "$lte": end
            }
        }
    else:
        filter_dict = {}
    
    # Retrieve with time filter
    docs = vectorstore.similarity_search(
        search_terms,
        k=10,
        filter=filter_dict
    )
    
    # Apply recency weighting
    weighted_docs = apply_recency_weighting(docs)
    
    return weighted_docs[:5]

def apply_recency_weighting(docs):
    now = datetime.now()
    
    for doc in docs:
        pub_date = datetime.fromisoformat(doc.metadata.get("publication_date", now.isoformat()))
        days_old = (now - pub_date).days
        
        # Recency score (more recent = higher score)
        recency_score = 1 / (1 + days_old / 365)  # Decay over 1 year
        
        # Combine with similarity score
        doc.metadata["final_score"] = (
            doc.metadata.get("similarity_score", 0) * 0.7 +
            recency_score * 0.3
        )
    
    # Sort by final score
    return sorted(docs, key=lambda x: x.metadata.get("final_score", 0), reverse=True)

# 5. Handle document updates
def update_document_version(document_id, new_content, vectorstore):
    # Mark old version as outdated
    vectorstore.update(
        ids=[document_id],
        metadatas=[{"status": "outdated", "superseded_by": f"{document_id}_v2"}]
    )
    
    # Add new version
    new_doc = {
        "id": f"{document_id}_v2",
        "content": new_content,
        "publication_date": datetime.now().isoformat(),
        "version": 2,
        "previous_version": document_id
    }
    
    vectorstore.add_texts(
        texts=[new_content],
        metadatas=[new_doc],
        ids=[new_doc["id"]]
    )

# 6. Query with version handling
def retrieve_latest_version(query, vectorstore):
    # Retrieve documents
    docs = vectorstore.similarity_search(query, k=10)
    
    # Filter to latest versions only
    latest_docs = {}
    for doc in docs:
        doc_id = doc.metadata.get("id")
        version = doc.metadata.get("version", 1)
        
        if doc_id not in latest_docs or version > latest_docs[doc_id].metadata.get("version", 1):
            latest_docs[doc_id] = doc
    
    return list(latest_docs.values())`,
            approaches: [
                "Extract and store temporal metadata during indexing.",
                "Use relative time parsing for natural queries.",
                "Apply recency weighting to boost recent documents.",
                "Handle document versions and updates properly.",
                "Filter out outdated versions in retrieval.",
                "Support both absolute and relative time queries.",
                "Consider time zones in temporal comparisons.",
                "Index temporal metadata for efficient filtering.",
                "Test with various temporal query patterns.",
                "Monitor temporal relevance in results."
            ]
        });

        // Q19 - RAG with active learning and feedback loops
        questions.push({
            number: 19,
            title: "How do you implement active learning and feedback loops to improve RAG systems?",
            description:
                "Active learning uses user feedback to continuously improve RAG systems. " +
                "Feedback can be explicit (thumbs up/down, corrections) or implicit (click-through, time spent). " +
                "This feedback is used to improve retrieval, fine-tune models, and update embeddings.",
            why:
                "RAG systems improve with usage data. User feedback identifies failures, highlights what works, " +
                "and provides training data for continuous improvement. Without feedback loops, systems stagnate.",
            what:
                "Feedback Types: Explicit (ratings, corrections), Implicit (clicks, dwell time, skip rate), " +
                "Negative feedback (wrong answers, irrelevant results). " +
                "Applications: Improve retrieval (update embeddings, adjust weights), Fine-tune models, " +
                "Update prompts, Re-rank training, Error analysis.",
            how:
                "1) Collect feedback (explicit and implicit), 2) Store feedback with queries and results, " +
                "3) Analyze feedback patterns, 4) Update retrieval (embedding fine-tuning, weight adjustment), " +
                "5) Improve prompts based on failures, 6) Retrain models periodically, 7) A/B test improvements.",
            pros: [
                "Continuous improvement",
                "Adapts to user needs",
                "Identifies failure patterns",
                "Data-driven optimization",
                "Better user experience over time",
                "Reduces manual tuning"
            ],
            cons: [
                "Requires feedback infrastructure",
                "Privacy considerations",
                "Feedback quality varies",
                "Implementation complexity",
                "Need for feedback analysis",
                "Potential bias in feedback"
            ],
            diagram: `flowchart TD
    A["User Query"] --> B["RAG System"]
    B --> C["Answer"]
    C --> D["User Feedback"]
    D --> E["Feedback Collection"]
    E --> F["Feedback Analysis"]
    F --> G["Identify Patterns"]
    G --> H["Update Retrieval"]
    G --> I["Update Prompts"]
    G --> J["Fine-tune Models"]
    H --> B
    I --> B
    J --> B`,
            implementation: `# Active Learning and Feedback Loops
from collections import defaultdict
import pandas as pd
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader

# 1. Feedback collection
class FeedbackCollector:
    def __init__(self):
        self.feedback_db = []
    
    def collect_feedback(self, query, answer, sources, feedback_type, value):
        feedback = {
            "query": query,
            "answer": answer,
            "sources": [s.page_content for s in sources],
            "feedback_type": feedback_type,  # "explicit", "implicit"
            "value": value,  # 1 (good) or 0 (bad) for explicit
            "timestamp": datetime.now().isoformat()
        }
        self.feedback_db.append(feedback)
    
    def get_negative_examples(self):
        """Get queries with negative feedback"""
        return [f for f in self.feedback_db if f["feedback_type"] == "explicit" and f["value"] == 0]

# 2. Analyze feedback patterns
def analyze_feedback(feedback_collector):
    df = pd.DataFrame(feedback_collector.feedback_db)
    
    # Negative feedback analysis
    negative = df[df["value"] == 0]
    
    # Common failure patterns
    failure_patterns = {
        "irrelevant_retrieval": negative[negative["feedback_type"] == "irrelevant_sources"],
        "wrong_answer": negative[negative["feedback_type"] == "wrong_answer"],
        "incomplete_answer": negative[negative["feedback_type"] == "incomplete"]
    }
    
    return failure_patterns

# 3. Update retrieval with feedback
def fine_tune_embeddings_with_feedback(feedback_collector, base_model):
    # Get positive and negative examples
    positive = [f for f in feedback_collector.feedback_db if f["value"] == 1]
    negative = [f for f in feedback_collector.feedback_db if f["value"] == 0]
    
    # Create training examples
    train_examples = []
    
    for pos in positive:
        # Positive: query should be similar to retrieved sources
        for source in pos["sources"]:
            train_examples.append(InputExample(
                texts=[pos["query"], source],
                label=1.0
            ))
    
    for neg in negative:
        # Negative: query should NOT be similar to retrieved sources
        for source in neg["sources"]:
            train_examples.append(InputExample(
                texts=[neg["query"], source],
                label=0.0
            ))
    
    # Fine-tune model
    model = SentenceTransformer(base_model)
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
    train_loss = losses.CosineSimilarityLoss(model)
    
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=1,
        warmup_steps=100
    )
    
    return model

# 4. Update prompts based on failures
def improve_prompts_from_feedback(failure_patterns, current_prompt):
    # Analyze common failure types
    if "wrong_answer" in failure_patterns:
        # Add instruction to be more careful
        improved_prompt = current_prompt + "\\n\\nImportant: Double-check facts before answering."
    
    if "incomplete_answer" in failure_patterns:
        # Add instruction to be comprehensive
        improved_prompt = current_prompt + "\\n\\nProvide complete and detailed answers."
    
    return improved_prompt

# 5. Implicit feedback collection
def collect_implicit_feedback(query, answer, user_actions):
    feedback = {
        "query": query,
        "answer": answer,
        "clicked_sources": user_actions.get("clicked_sources", []),
        "dwell_time": user_actions.get("dwell_time", 0),
        "asked_followup": user_actions.get("asked_followup", False),
        "skipped": user_actions.get("skipped", False)
    }
    
    # Infer quality from actions
    if feedback["clicked_sources"] and feedback["dwell_time"] > 5:
        feedback["inferred_quality"] = "good"
    elif feedback["skipped"] or feedback["dwell_time"] < 2:
        feedback["inferred_quality"] = "poor"
    else:
        feedback["inferred_quality"] = "neutral"
    
    return feedback

# 6. Continuous improvement loop
class RAGImprovementLoop:
    def __init__(self, rag_system, feedback_collector):
        self.rag_system = rag_system
        self.feedback_collector = feedback_collector
        self.improvement_interval = 100  # Improve every 100 queries
    
    def process_query(self, query):
        # Get answer
        result = self.rag_system.query(query)
        
        # Collect implicit feedback (in real app, this would be async)
        # For demo, we'll simulate
        user_actions = self.simulate_user_actions(result)
        implicit_feedback = collect_implicit_feedback(query, result["answer"], user_actions)
        self.feedback_collector.feedback_db.append(implicit_feedback)
        
        # Periodic improvement
        if len(self.feedback_collector.feedback_db) % self.improvement_interval == 0:
            self.improve_system()
        
        return result
    
    def improve_system(self):
        # Analyze feedback
        patterns = analyze_feedback(self.feedback_collector)
        
        # Update embeddings
        if len(self.feedback_collector.get_negative_examples()) > 10:
            new_model = fine_tune_embeddings_with_feedback(
                self.feedback_collector,
                "sentence-transformers/all-MiniLM-L6-v2"
            )
            self.rag_system.update_embeddings(new_model)
        
        # Update prompts
        improved_prompt = improve_prompts_from_feedback(
            patterns,
            self.rag_system.prompt_template
        )
        self.rag_system.update_prompt(improved_prompt)
        
        print(f"System improved based on {len(self.feedback_collector.feedback_db)} feedback examples")`,
            approaches: [
                "Collect both explicit and implicit feedback.",
                "Store feedback with query context for analysis.",
                "Analyze feedback patterns to identify issues.",
                "Use feedback to fine-tune embeddings.",
                "Update prompts based on failure patterns.",
                "Implement periodic improvement cycles.",
                "A/B test improvements before full deployment.",
                "Respect user privacy in feedback collection.",
                "Handle feedback bias and ensure diversity.",
                "Monitor improvement metrics over time."
            ]
        });

        // Q20 - RAG system architecture patterns
        questions.push({
            number: 20,
            title: "What are the common architecture patterns for production RAG systems?",
            description:
                "Production RAG systems require careful architecture to handle scale, reliability, and performance. " +
                "Common patterns include microservices architecture, event-driven processing, caching layers, " +
                "async processing, and separation of concerns between retrieval and generation.",
            why:
                "Poor architecture leads to scalability issues, high latency, reliability problems, and difficult maintenance. " +
                "Good architecture patterns enable systems to scale, perform well, and remain maintainable as they grow.",
            what:
                "Architecture Patterns: Microservices (separate retrieval and generation), Event-driven (async document processing), " +
                "Caching layers (multi-level caching), API gateway (routing, rate limiting), Message queues (async processing), " +
                "Read replicas (scale reads), CDN (static content), Service mesh (observability, security).",
            how:
                "1) Separate concerns (retrieval service, generation service), 2) Implement caching at multiple levels, " +
                "3) Use async processing for document ingestion, 4) Implement API gateway for routing, " +
                "5) Use message queues for high-volume scenarios, 6) Add monitoring and observability, " +
                "7) Implement health checks and circuit breakers.",
            pros: [
                "Scalable architecture",
                "Better performance",
                "Improved reliability",
                "Easier maintenance",
                "Independent scaling",
                "Fault isolation"
            ],
            cons: [
                "More complex setup",
                "Higher infrastructure costs",
                "More moving parts",
                "Requires DevOps expertise",
                "Distributed system challenges",
                "Initial development overhead"
            ],
            diagram: `flowchart TD
    A["API Gateway"] --> B["Load Balancer"]
    B --> C["Retrieval Service"]
    B --> D["Generation Service"]
    C --> E["Cache Layer"]
    C --> F["Vector DB"]
    D --> G["LLM Service"]
    D --> H["Cache Layer"]
    I["Document Ingestion"] --> J["Message Queue"]
    J --> K["Processing Service"]
    K --> L["Embedding Service"]
    L --> F
    M["Monitoring"] --> C
    M --> D
    M --> K`,
            implementation: `# Production RAG Architecture
from fastapi import FastAPI
from celery import Celery
import redis
from langchain.vectorstores import Pinecone

# 1. Microservices architecture
# Retrieval Service
app_retrieval = FastAPI(title="RAG Retrieval Service")
redis_cache = redis.Redis(host='redis', port=6379)

@app_retrieval.post("/retrieve")
async def retrieve(query: str, k: int = 5):
    # Check cache
    cache_key = f"retrieve:{hash(query)}:{k}"
    cached = redis_cache.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Retrieve from vector store
    docs = vectorstore.similarity_search(query, k=k)
    
    # Cache result
    redis_cache.setex(cache_key, 3600, json.dumps([d.page_content for d in docs]))
    
    return {"documents": [d.page_content for d in docs]}

# Generation Service
app_generation = FastAPI(title="RAG Generation Service")

@app_generation.post("/generate")
async def generate(query: str, context: list):
    # Check cache
    cache_key = f"generate:{hash(query)}:{hash(str(context))}"
    cached = redis_cache.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Generate answer
    prompt = create_prompt(query, context)
    answer = llm.generate(prompt)
    
    # Cache result
    redis_cache.setex(cache_key, 1800, json.dumps(answer))
    
    return {"answer": answer}

# 2. Event-driven document processing
celery_app = Celery('rag_worker', broker='redis://redis:6379/0')

@celery_app.task
def process_document(document_id, document_content):
    # Chunk document
    chunks = chunk_document(document_content)
    
    # Generate embeddings (batch)
    embeddings = batch_embed(chunks)
    
    # Store in vector DB
    vectorstore.add_texts(
        texts=chunks,
        embeddings=embeddings,
        metadatas=[{"document_id": document_id} for _ in chunks]
    )
    
    return {"status": "processed", "chunks": len(chunks)}

# 3. API Gateway pattern
from fastapi import FastAPI, HTTPException
import httpx

app_gateway = FastAPI(title="RAG API Gateway")

@app_gateway.post("/query")
async def query_endpoint(query: str):
    async with httpx.AsyncClient() as client:
        # Parallel calls to retrieval and any preprocessing
        retrieval_task = client.post(
            "http://retrieval-service/retrieve",
            json={"query": query, "k": 5}
        )
        
        # Wait for retrieval
        retrieval_response = await retrieval_task
        documents = retrieval_response.json()["documents"]
        
        # Generate answer
        generation_response = await client.post(
            "http://generation-service/generate",
            json={"query": query, "context": documents}
        )
        
        answer = generation_response.json()["answer"]
        
        return {"answer": answer, "sources": documents}

# 4. Caching strategy
class MultiLevelCache:
    def __init__(self):
        self.l1_cache = {}  # In-memory
        self.l2_cache = redis.Redis(host='redis', port=6379)  # Redis
        self.l3_cache = None  # Could be CDN
    
    async def get(self, key):
        # L1: In-memory
        if key in self.l1_cache:
            return self.l1_cache[key]
        
        # L2: Redis
        cached = self.l2_cache.get(key)
        if cached:
            value = json.loads(cached)
            self.l1_cache[key] = value  # Promote to L1
            return value
        
        return None
    
    async def set(self, key, value, ttl=3600):
        self.l1_cache[key] = value
        self.l2_cache.setex(key, ttl, json.dumps(value))

# 5. Health checks and monitoring
@app_gateway.get("/health")
async def health_check():
    services = {
        "retrieval": await check_service_health("http://retrieval-service/health"),
        "generation": await check_service_health("http://generation-service/health"),
        "vector_db": check_vector_db_health(),
        "cache": check_cache_health()
    }
    
    all_healthy = all(services.values())
    
    return {
        "status": "healthy" if all_healthy else "degraded",
        "services": services
    }

# 6. Circuit breaker pattern
from circuitbreaker import circuit

@circuit(failure_threshold=5, recovery_timeout=60)
async def call_llm_service(prompt):
    # If service fails 5 times, circuit opens for 60 seconds
    response = await httpx.post("http://llm-service/generate", json={"prompt": prompt})
    return response.json()`,
            approaches: [
                "Separate retrieval and generation into different services.",
                "Implement multi-level caching (memory, Redis, CDN).",
                "Use message queues for async document processing.",
                "Implement API gateway for routing and rate limiting.",
                "Add health checks and circuit breakers.",
                "Use load balancers for horizontal scaling.",
                "Implement comprehensive monitoring and logging.",
                "Design for failure (graceful degradation).",
                "Use read replicas for vector database.",
                "Consider service mesh for complex deployments."
            ]
        });

        // Q21 - RAG with LangChain and LlamaIndex
        questions.push({
            number: 21,
            title: "How do LangChain and LlamaIndex differ for building RAG systems?",
            description:
                "LangChain and LlamaIndex are both popular frameworks for building RAG systems, but they have different philosophies and strengths. " +
                "LangChain focuses on composability and flexibility, while LlamaIndex specializes in data ingestion and indexing for LLM applications. " +
                "Understanding their differences helps choose the right tool for specific use cases.",
            why:
                "Choosing the right framework affects development speed, maintainability, and system capabilities. " +
                "Each framework has different strengths, and the choice depends on project requirements, team expertise, and specific needs.",
            what:
                "LangChain: General-purpose LLM framework, highly composable, extensive integrations, flexible architecture, " +
                "good for complex workflows, active community. " +
                "LlamaIndex: Specialized for data ingestion and indexing, optimized for RAG, built-in data connectors, " +
                "query engines, document management, simpler for RAG-specific tasks.",
            how:
                "LangChain: Build custom chains, compose components, integrate with many tools. " +
                "LlamaIndex: Use data loaders, build indexes, query with query engines. " +
                "Can also combine both: use LlamaIndex for indexing, LangChain for orchestration.",
            pros: [
                "LangChain: Maximum flexibility, extensive ecosystem",
                "LangChain: Good for complex multi-step workflows",
                "LlamaIndex: Optimized for RAG use cases",
                "LlamaIndex: Simpler API for common RAG tasks",
                "LlamaIndex: Better document management",
                "Both: Active development and community"
            ],
            cons: [
                "LangChain: Steeper learning curve",
                "LangChain: More boilerplate for simple RAG",
                "LlamaIndex: Less flexible for non-RAG tasks",
                "LlamaIndex: Smaller ecosystem than LangChain",
                "Both: Rapid changes can break code",
                "Both: Need to stay updated with versions"
            ],
            diagram: `flowchart TD
    A["RAG Requirements"] --> B{"Framework Choice"}
    B -->|Complex Workflows| C["LangChain"]
    B -->|RAG-Focused| D["LlamaIndex"]
    C --> E["Compose Chains"]
    C --> F["Integrate Tools"]
    D --> G["Build Indexes"]
    D --> H["Query Engines"]
    E --> I["Custom RAG System"]
    F --> I
    G --> I
    H --> I`,
            implementation: `# LangChain RAG
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# LangChain approach
loader = PyPDFLoader("document.pdf")
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)

embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(chunks, embeddings)

qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

result = qa_chain.run("What is RAG?")

# LlamaIndex RAG
from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext
from llama_index.llms import OpenAI

# LlamaIndex approach
documents = SimpleDirectoryReader("data").load_data()

service_context = ServiceContext.from_defaults(
    llm=OpenAI(model="gpt-3.5-turbo"),
    chunk_size=1000
)

index = VectorStoreIndex.from_documents(documents, service_context=service_context)

query_engine = index.as_query_engine()
response = query_engine.query("What is RAG?")

# Combining both
from llama_index import VectorStoreIndex
from langchain.agents import initialize_agent
from langchain.tools import Tool

# Use LlamaIndex for indexing
index = VectorStoreIndex.from_documents(documents)

# Use LangChain for orchestration
def rag_query(query):
    query_engine = index.as_query_engine()
    return str(query_engine.query(query))

rag_tool = Tool(
    name="RAG Query",
    func=rag_query,
    description="Query documents using RAG"
)

agent = initialize_agent([rag_tool], llm=OpenAI(), agent="zero-shot-react-description")
result = agent.run("What is RAG and how does it work?")`,
            approaches: [
                "Use LangChain for complex, multi-step workflows.",
                "Use LlamaIndex for straightforward RAG applications.",
                "Consider team expertise and learning curve.",
                "Evaluate based on specific requirements.",
                "Can combine both frameworks for best of both worlds.",
                "LangChain: Better for agent-based systems.",
                "LlamaIndex: Better for document-centric applications.",
                "Consider ecosystem and integrations needed.",
                "Test both for your specific use case.",
                "Stay updated with framework changes."
            ]
        });

        // Q22 - RAG with structured data (SQL, JSON)
        questions.push({
            number: 22,
            title: "How do you implement RAG with structured data like SQL databases and JSON?",
            description:
                "Structured data (SQL databases, JSON, CSV) requires different approaches than unstructured text. " +
                "RAG with structured data involves converting structured formats to text, using specialized embeddings, " +
                "and combining semantic search with structured queries for hybrid retrieval.",
            why:
                "Many organizations have structured data that needs to be queried naturally. " +
                "Traditional RAG works with text, but structured data requires special handling to preserve relationships and enable accurate retrieval.",
            what:
                "Approaches: Text conversion (convert tables/JSON to text), Hybrid search (combine SQL + vector search), " +
                "Structured embeddings (embed table rows, JSON objects), Schema-aware retrieval (understand data structure), " +
                "SQL generation (convert natural language to SQL), Text-to-SQL (LLM generates SQL queries).",
            how:
                "1) Convert structured data to text representations, 2) Generate embeddings for structured content, " +
                "3) Store in vector database with metadata, 4) For queries: use semantic search + structured filters, " +
                "5) Optionally generate SQL from natural language, 6) Combine results from both approaches.",
            pros: [
                "Handles structured data naturally",
                "Combines semantic and structured search",
                "Enables natural language queries",
                "Preserves data relationships",
                "Supports complex queries",
                "Better than pure SQL for non-technical users"
            ],
            cons: [
                "More complex implementation",
                "Requires data conversion",
                "May lose some structure",
                "SQL generation can be error-prone",
                "Higher computational cost",
                "Need to maintain both systems"
            ],
            diagram: `flowchart TD
    A["Structured Data"] --> B["SQL Database"]
    A --> C["JSON Files"]
    A --> D["CSV Files"]
    B --> E["Convert to Text"]
    C --> E
    D --> E
    E --> F["Generate Embeddings"]
    F --> G["Vector Store"]
    H["User Query"] --> I["Semantic Search"]
    H --> J["SQL Generation"]
    I --> G
    J --> B
    I --> K["Combine Results"]
    J --> K
    K --> L["Answer"]`,
            implementation: `# RAG with Structured Data
import pandas as pd
import json
from sqlalchemy import create_engine
from langchain.utilities import SQLDatabase
from langchain.agents import create_sql_agent
from langchain.llms import OpenAI

# 1. Convert SQL tables to text
def sql_table_to_text(table_name, db_connection):
    query = f"SELECT * FROM {table_name} LIMIT 100"
    df = pd.read_sql(query, db_connection)
    
    # Convert to text representation
    text = f"Table: {table_name}\\n"
    text += f"Columns: {', '.join(df.columns)}\\n\\n"
    
    for idx, row in df.iterrows():
        row_text = " | ".join([f"{col}: {row[col]}" for col in df.columns])
        text += f"Row {idx}: {row_text}\\n"
    
    return text

# 2. Convert JSON to text
def json_to_text(json_data):
    if isinstance(json_data, dict):
        text = ""
        for key, value in json_data.items():
            if isinstance(value, (dict, list)):
                text += f"{key}: {json.dumps(value)}\\n"
            else:
                text += f"{key}: {value}\\n"
        return text
    elif isinstance(json_data, list):
        return "\\n".join([json_to_text(item) for item in json_data])
    else:
        return str(json_data)

# 3. Embed structured data
def embed_structured_data(structured_data, data_type):
    if data_type == "sql":
        text = sql_table_to_text(structured_data["table"], structured_data["connection"])
    elif data_type == "json":
        text = json_to_text(structured_data)
    elif data_type == "csv":
        df = pd.read_csv(structured_data["path"])
        text = df.to_string()
    
    # Generate embedding
    embedding = embeddings.embed_query(text)
    
    # Store with metadata
    vectorstore.add_texts(
        texts=[text],
        embeddings=[embedding],
        metadatas=[{
            "type": data_type,
            "source": structured_data.get("source", "unknown"),
            "schema": structured_data.get("schema", {})
        }]
    )

# 4. Hybrid SQL + Vector Search
def hybrid_sql_vector_query(query, db_connection):
    # Vector search for semantic matches
    vector_results = vectorstore.similarity_search(query, k=5)
    
    # Extract relevant tables/columns from vector results
    relevant_tables = extract_tables_from_results(vector_results)
    
    # Generate SQL query
    db = SQLDatabase(db_connection)
    agent = create_sql_agent(
        llm=OpenAI(temperature=0),
        db=db,
        verbose=True
    )
    
    sql_result = agent.run(query)
    
    # Combine results
    combined_context = {
        "semantic_matches": [r.page_content for r in vector_results],
        "sql_results": sql_result,
        "relevant_tables": relevant_tables
    }
    
    return combined_context

# 5. Text-to-SQL with RAG
from langchain.chains import SQLDatabaseChain

def text_to_sql_rag(query, db_connection):
    # First, use RAG to find relevant schema information
    schema_query = f"What tables and columns are relevant for: {query}"
    schema_docs = vectorstore.similarity_search(schema_query, k=3)
    
    # Build context with schema info
    schema_context = "\\n".join([d.page_content for d in schema_docs])
    
    # Generate SQL with schema context
    db = SQLDatabase(db_connection)
    db_chain = SQLDatabaseChain.from_llm(
        llm=OpenAI(temperature=0),
        db=db,
        verbose=True
    )
    
    enhanced_query = f"Schema context: {schema_context}\\n\\nUser question: {query}"
    result = db_chain.run(enhanced_query)
    
    return result

# 6. JSON RAG
def json_rag(json_files, query):
    # Load and convert JSON files
    all_texts = []
    for json_file in json_files:
        with open(json_file) as f:
            data = json.load(f)
            text = json_to_text(data)
            all_texts.append(text)
    
    # Create embeddings
    embeddings_list = embeddings.embed_documents(all_texts)
    
    # Store in vector database
    vectorstore.add_texts(
        texts=all_texts,
        embeddings=embeddings_list,
        metadatas=[{"source": f, "type": "json"} for f in json_files]
    )
    
    # Query
    results = vectorstore.similarity_search(query, k=3)
    
    # Generate answer
    context = "\\n".join([r.page_content for r in results])
    answer = llm(f"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:")
    
    return answer`,
            approaches: [
                "Convert structured data to text for embedding.",
                "Preserve structure in text representation.",
                "Use hybrid search combining semantic and structured queries.",
                "Generate SQL from natural language when appropriate.",
                "Maintain metadata about data structure.",
                "Consider using specialized tools (Text-to-SQL).",
                "Test with various structured data formats.",
                "Handle schema changes gracefully.",
                "Optimize conversion for large datasets.",
                "Combine structured and unstructured data when possible."
            ]
        });

        // Q23 - RAG security and privacy
        questions.push({
            number: 23,
            title: "What are the security and privacy considerations for RAG systems?",
            description:
                "RAG systems handle sensitive data and must address security and privacy concerns including data encryption, " +
                "access control, data leakage prevention, PII handling, and compliance with regulations like GDPR, HIPAA. " +
                "Security is critical for production RAG deployments.",
            why:
                "RAG systems process and store sensitive information. Security breaches can expose confidential data, " +
                "violate privacy regulations, and damage trust. Proper security is essential for enterprise deployments.",
            what:
                "Security Areas: Data encryption (at rest, in transit), Access control (authentication, authorization), " +
                "Data isolation (multi-tenant security), PII detection and masking, Audit logging, " +
                "Input validation, Output filtering, Secure APIs, Compliance (GDPR, HIPAA, SOC 2).",
            how:
                "1) Encrypt data at rest and in transit, 2) Implement authentication and authorization, " +
                "3) Isolate data per tenant/user, 4) Detect and mask PII, 5) Log all access and queries, " +
                "6) Validate and sanitize inputs, 7) Filter sensitive information from outputs, " +
                "8) Use secure APIs (HTTPS, API keys), 9) Regular security audits.",
            pros: [
                "Protects sensitive data",
                "Compliance with regulations",
                "Builds user trust",
                "Prevents data breaches",
                "Audit trail for accountability",
                "Enterprise-ready deployment"
            ],
            cons: [
                "Implementation complexity",
                "Performance overhead",
                "Ongoing maintenance",
                "Compliance costs",
                "User experience trade-offs",
                "Requires security expertise"
            ],
            diagram: `flowchart TD
    A["User Request"] --> B["Authentication"]
    B --> C["Authorization Check"]
    C --> D["Input Validation"]
    D --> E["PII Detection"]
    E --> F["Data Isolation"]
    F --> G["Encrypted Retrieval"]
    G --> H["Encrypted Generation"]
    H --> I["Output Filtering"]
    I --> J["Audit Logging"]
    J --> K["Secure Response"]`,
            implementation: `# RAG Security Implementation
from cryptography.fernet import Fernet
import hashlib
import re
from langchain.callbacks import StdOutCallbackHandler

# 1. Data encryption
class EncryptedVectorStore:
    def __init__(self, encryption_key):
        self.cipher = Fernet(encryption_key)
        self.vectorstore = None
    
    def add_encrypted_documents(self, documents):
        encrypted_docs = []
        for doc in documents:
            encrypted_content = self.cipher.encrypt(doc.page_content.encode())
            encrypted_docs.append({
                "content": encrypted_content,
                "metadata": doc.metadata
            })
        # Store encrypted documents
        # Note: Embeddings should be generated before encryption
        return encrypted_docs
    
    def decrypt_document(self, encrypted_content):
        return self.cipher.decrypt(encrypted_content).decode()

# 2. Access control
from functools import wraps
from flask import request, jsonify

def require_auth(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        token = request.headers.get('Authorization')
        if not token or not validate_token(token):
            return jsonify({'error': 'Unauthorized'}), 401
        return f(*args, **kwargs)
    return decorated_function

@app.route('/query', methods=['POST'])
@require_auth
def query_endpoint():
    user_id = get_user_from_token(request.headers.get('Authorization'))
    query = request.json['query']
    
    # Check user permissions
    if not has_permission(user_id, 'query'):
        return jsonify({'error': 'Forbidden'}), 403
    
    # Process query with user context
    result = process_query(query, user_id)
    return jsonify(result)

# 3. PII detection and masking
def detect_pii(text):
    pii_patterns = {
        'email': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',
        'ssn': r'\\b\\d{3}-\\d{2}-\\d{4}\\b',
        'phone': r'\\b\\d{3}-\\d{3}-\\d{4}\\b',
        'credit_card': r'\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b'
    }
    
    detected = {}
    for pii_type, pattern in pii_patterns.items():
        matches = re.findall(pattern, text)
        if matches:
            detected[pii_type] = matches
    
    return detected

def mask_pii(text):
    pii = detect_pii(text)
    masked_text = text
    
    for pii_type, matches in pii.items():
        for match in matches:
            if pii_type == 'email':
                masked = '***@***.***'
            elif pii_type == 'ssn':
                masked = '***-**-****'
            elif pii_type == 'phone':
                masked = '***-***-****'
            else:
                masked = '****'
            masked_text = masked_text.replace(match, masked)
    
    return masked_text

# 4. Data isolation (multi-tenant)
class TenantAwareVectorStore:
    def __init__(self, vectorstore):
        self.vectorstore = vectorstore
    
    def query(self, query, tenant_id, k=5):
        # Add tenant filter to metadata
        filter_dict = {"tenant_id": tenant_id}
        
        results = self.vectorstore.similarity_search(
            query,
            k=k,
            filter=filter_dict
        )
        
        # Verify all results belong to tenant
        filtered_results = [r for r in results if r.metadata.get("tenant_id") == tenant_id]
        
        return filtered_results
    
    def add_documents(self, documents, tenant_id):
        # Add tenant_id to all metadata
        for doc in documents:
            doc.metadata["tenant_id"] = tenant_id
        
        self.vectorstore.add_documents(documents)

# 5. Audit logging
import logging
from datetime import datetime

audit_logger = logging.getLogger('rag_audit')
audit_logger.setLevel(logging.INFO)
handler = logging.FileHandler('rag_audit.log')
audit_logger.addHandler(handler)

def audit_log(user_id, action, query, result=None, error=None):
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "user_id": user_id,
        "action": action,
        "query": mask_pii(query),  # Mask PII in logs
        "result_length": len(result) if result else None,
        "error": str(error) if error else None
    }
    audit_logger.info(json.dumps(log_entry))

# 6. Secure query processing
def secure_query_processing(query, user_id):
    try:
        # Validate input
        if not validate_input(query):
            raise ValueError("Invalid input")
        
        # Detect PII in query
        query_pii = detect_pii(query)
        if query_pii:
            audit_log(user_id, "pii_detected", query)
            # Mask or reject
            query = mask_pii(query)
        
        # Process with tenant isolation
        results = tenant_vectorstore.query(query, user_id)
        
        # Filter sensitive information from results
        filtered_results = filter_sensitive_info(results, user_id)
        
        # Audit log
        audit_log(user_id, "query", query, filtered_results)
        
        return filtered_results
    
    except Exception as e:
        audit_log(user_id, "error", query, error=e)
        raise`,
            approaches: [
                "Encrypt sensitive data at rest and in transit.",
                "Implement strong authentication and authorization.",
                "Isolate data per tenant/user for multi-tenant systems.",
                "Detect and mask PII in inputs and outputs.",
                "Log all access for audit trails.",
                "Validate and sanitize all inputs.",
                "Filter sensitive information from responses.",
                "Use secure communication (HTTPS, TLS).",
                "Regular security audits and penetration testing.",
                "Comply with relevant regulations (GDPR, HIPAA)."
            ]
        });

        // Q24 - RAG performance monitoring
        questions.push({
            number: 24,
            title: "How do you monitor and optimize RAG system performance in production?",
            description:
                "Production RAG systems require comprehensive monitoring to track performance, identify bottlenecks, " +
                "and ensure reliability. Monitoring includes metrics for latency, throughput, accuracy, costs, " +
                "and system health. Optimization involves identifying and addressing performance issues.",
            why:
                "Without monitoring, you can't identify performance issues, cost problems, or quality degradation. " +
                "Performance monitoring enables data-driven optimization and ensures systems meet SLAs.",
            what:
                "Key Metrics: Latency (query time, retrieval time, generation time), Throughput (queries per second), " +
                "Accuracy (retrieval precision, answer quality), Costs (embedding costs, LLM costs, infrastructure), " +
                "System health (error rates, availability), User metrics (satisfaction, engagement).",
            how:
                "1) Instrument code with metrics, 2) Set up monitoring dashboards, 3) Define SLAs and alerts, " +
                "4) Track costs per query, 5) Monitor accuracy over time, 6) Analyze performance bottlenecks, " +
                "7) Optimize hot paths, 8) A/B test improvements.",
            pros: [
                "Identifies performance issues early",
                "Enables data-driven optimization",
                "Tracks cost efficiency",
                "Ensures SLA compliance",
                "Improves user experience",
                "Supports capacity planning"
            ],
            cons: [
                "Requires monitoring infrastructure",
                "Additional overhead",
                "Need to interpret metrics",
                "Alert fatigue if not tuned",
                "Storage costs for metrics",
                "Requires DevOps expertise"
            ],
            diagram: `flowchart TD
    A["RAG System"] --> B["Metrics Collection"]
    B --> C["Latency Metrics"]
    B --> D["Accuracy Metrics"]
    B --> E["Cost Metrics"]
    B --> F["Health Metrics"]
    C --> G["Monitoring Dashboard"]
    D --> G
    E --> G
    F --> G
    G --> H["Alerts"]
    G --> I["Performance Analysis"]
    I --> J["Optimization"]`,
            implementation: `# RAG Performance Monitoring
from prometheus_client import Counter, Histogram, Gauge
import time
import logging

# Metrics
query_counter = Counter('rag_queries_total', 'Total RAG queries', ['status'])
query_latency = Histogram('rag_query_duration_seconds', 'RAG query latency', ['stage'])
retrieval_accuracy = Gauge('rag_retrieval_precision', 'Retrieval precision score')
cost_per_query = Histogram('rag_cost_per_query', 'Cost per query in USD')

# Decorator for monitoring
def monitor_rag_query(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()
        query = kwargs.get('query', args[0] if args else 'unknown')
        
        try:
            # Track retrieval time
            retrieval_start = time.time()
            result = func(*args, **kwargs)
            retrieval_time = time.time() - retrieval_start
            
            # Track generation time
            generation_time = result.get('generation_time', 0)
            total_time = time.time() - start_time
            
            # Record metrics
            query_counter.labels(status='success').inc()
            query_latency.labels(stage='retrieval').observe(retrieval_time)
            query_latency.labels(stage='generation').observe(generation_time)
            query_latency.labels(stage='total').observe(total_time)
            
            # Estimate cost
            cost = estimate_cost(result)
            cost_per_query.observe(cost)
            
            # Log
            logging.info(f"Query: {query[:50]}, Latency: {total_time:.2f}s, Cost: $\\{cost:.4f}")
            
            return result
        
        except Exception as e:
            query_counter.labels(status='error').inc()
            logging.error(f"Query error: {e}", exc_info=True)
            raise
    
    return wrapper

@monitor_rag_query
def process_rag_query(query):
    # Retrieval
    retrieval_start = time.time()
    docs = vectorstore.similarity_search(query, k=5)
    retrieval_time = time.time() - retrieval_start
    
    # Generation
    generation_start = time.time()
    answer = llm_chain.run(context=docs, question=query)
    generation_time = time.time() - generation_start
    
    return {
        "answer": answer,
        "sources": docs,
        "retrieval_time": retrieval_time,
        "generation_time": generation_time
    }

# Accuracy monitoring
def evaluate_retrieval_accuracy(sample_queries, ground_truth):
    correct = 0
    total = len(sample_queries)
    
    for query, expected_docs in zip(sample_queries, ground_truth):
        retrieved = vectorstore.similarity_search(query, k=5)
        retrieved_ids = {doc.metadata.get('id') for doc in retrieved}
        expected_ids = {doc['id'] for doc in expected_docs}
        
        precision = len(retrieved_ids & expected_ids) / len(retrieved_ids) if retrieved_ids else 0
        if precision > 0.5:  # Threshold
            correct += 1
    
    accuracy = correct / total
    retrieval_accuracy.set(accuracy)
    
    return accuracy

# Cost tracking
def estimate_cost(result):
    # Embedding costs (per 1K tokens)
    embedding_cost_per_1k = 0.0001
    
    # LLM costs (varies by model)
    llm_cost_per_1k_input = 0.002
    llm_cost_per_1k_output = 0.002
    
    # Estimate tokens (rough)
    input_tokens = estimate_tokens(result.get('context', ''))
    output_tokens = estimate_tokens(result.get('answer', ''))
    
    embedding_cost = (input_tokens / 1000) * embedding_cost_per_1k
    llm_cost = (input_tokens / 1000) * llm_cost_per_1k_input + (output_tokens / 1000) * llm_cost_per_1k_output
    
    return embedding_cost + llm_cost

def estimate_tokens(text):
    # Rough estimation: 1 token ≈ 4 characters
    return len(text) / 4

# Performance dashboard data
def get_performance_metrics(time_range='1h'):
    return {
        "total_queries": query_counter._value.get(('success',), 0),
        "error_rate": query_counter._value.get(('error',), 0) / max(query_counter._value.get(('success',), 1), 1),
        "avg_latency": query_latency._sum.get(('total',), 0) / max(query_latency._count.get(('total',), 1), 1),
        "retrieval_accuracy": retrieval_accuracy._value,
        "avg_cost_per_query": cost_per_query._sum / max(cost_per_query._count, 1)
    }

# Alerting
def check_alerts():
    metrics = get_performance_metrics()
    
    alerts = []
    
    if metrics["avg_latency"] > 5.0:  # 5 seconds
        alerts.append("High latency detected")
    
    if metrics["error_rate"] > 0.05:  # 5%
        alerts.append("High error rate detected")
    
    if metrics["retrieval_accuracy"] < 0.7:  # 70%
        alerts.append("Low retrieval accuracy")
    
    if metrics["avg_cost_per_query"] > 0.10:  # $0.10
        alerts.append("High cost per query")
    
    return alerts`,
            approaches: [
                "Instrument all critical paths with metrics.",
                "Set up dashboards for real-time monitoring.",
                "Define SLAs and configure alerts.",
                "Track costs per query and optimize expensive operations.",
                "Monitor accuracy over time to detect degradation.",
                "Analyze performance bottlenecks regularly.",
                "Use distributed tracing for complex workflows.",
                "Implement health checks and readiness probes.",
                "Set up log aggregation and analysis.",
                "Regular performance reviews and optimization cycles."
            ]
        });

        // Q25 - RAG with code and technical documentation
        questions.push({
            number: 25,
            title: "How do you implement RAG for code repositories and technical documentation?",
            description:
                "Code and technical documentation require specialized handling in RAG systems. Code has structure (syntax, dependencies), " +
                "technical docs have cross-references, and both benefit from preserving context and relationships. " +
                "RAG for code enables natural language queries over codebases.",
            why:
                "Developers need to search and understand large codebases. Traditional search is limited, but RAG enables natural language queries " +
                "like 'How does authentication work?' or 'Find all API endpoints'. This improves developer productivity.",
            what:
                "Code-Specific Challenges: Syntax preservation, Dependency tracking, Function/class relationships, Code structure, " +
                "Comments and documentation. " +
                "Approaches: Code-aware chunking (preserve functions/classes), AST parsing (understand structure), " +
                "Metadata enrichment (add function names, file paths), Cross-reference handling, Code embeddings (specialized models).",
            how:
                "1) Parse code with AST to understand structure, 2) Chunk at function/class level, 3) Extract metadata (function names, parameters, docstrings), " +
                "4) Generate embeddings (use code-specific models), 5) Store with rich metadata, 6) Enable queries with code context.",
            pros: [
                "Natural language code search",
                "Better code understanding",
                "Finds related code",
                "Preserves code structure",
                "Handles technical documentation",
                "Improves developer productivity"
            ],
            cons: [
                "Requires code parsing",
                "More complex chunking",
                "Need code-specific embeddings",
                "Higher storage requirements",
                "May miss some relationships",
                "Requires maintenance as code changes"
            ],
            diagram: `flowchart TD
    A["Code Repository"] --> B["AST Parser"]
    B --> C["Extract Functions"]
    B --> D["Extract Classes"]
    B --> E["Extract Docstrings"]
    C --> F["Code Chunking"]
    D --> F
    E --> F
    F --> G["Metadata Enrichment"]
    G --> H["Code Embeddings"]
    H --> I["Vector Store"]
    J["Query"] --> K["Code-Aware Retrieval"]
    I --> K
    K --> L["Context Assembly"]
    L --> M["Answer with Code"]`,
            implementation: `# RAG for Code
import ast
from tree_sitter import Language, Parser
from langchain.text_splitter import Language

# 1. Code-aware chunking
class CodeChunker:
    def __init__(self, language='python'):
        self.language = language
    
    def chunk_code_file(self, code_content, file_path):
        # Parse AST
        tree = ast.parse(code_content)
        
        chunks = []
        
        # Extract functions
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                chunk = {
                    "type": "function",
                    "name": node.name,
                    "code": ast.get_source_segment(code_content, node),
                    "docstring": ast.get_docstring(node),
                    "file": file_path,
                    "line_start": node.lineno,
                    "line_end": node.end_lineno
                }
                chunks.append(chunk)
            
            # Extract classes
            elif isinstance(node, ast.ClassDef):
                chunk = {
                    "type": "class",
                    "name": node.name,
                    "code": ast.get_source_segment(code_content, node),
                    "docstring": ast.get_docstring(node),
                    "file": file_path,
                    "methods": [m.name for m in node.body if isinstance(m, ast.FunctionDef)]
                }
                chunks.append(chunk)
        
        return chunks

# 2. Code-specific embeddings
from sentence_transformers import SentenceTransformer

# Use code-specific model
code_model = SentenceTransformer('microsoft/codebert-base')

def embed_code_chunk(chunk):
    # Combine code and documentation
    text = f"{chunk['type']}: {chunk['name']}\\n"
    if chunk.get('docstring'):
        text += f"Documentation: {chunk['docstring']}\\n"
    text += f"Code: {chunk['code']}"
    
    return code_model.encode(text)

# 3. Store with rich metadata
def index_codebase(repo_path):
    chunks = []
    
    for root, dirs, files in os.walk(repo_path):
        for file in files:
            if file.endswith('.py'):
                file_path = os.path.join(root, file)
                with open(file_path) as f:
                    code = f.read()
                
                chunker = CodeChunker('python')
                file_chunks = chunker.chunk_code_file(code, file_path)
                chunks.extend(file_chunks)
    
    # Generate embeddings
    embeddings = [embed_code_chunk(chunk) for chunk in chunks]
    
    # Store with metadata
    texts = [f"{c['type']} {c['name']}: {c.get('docstring', '')} {c['code']}" for c in chunks]
    metadatas = [{
        "type": c["type"],
        "name": c["name"],
        "file": c["file"],
        "line_start": c.get("line_start"),
        "line_end": c.get("line_end")
    } for c in chunks]
    
    vectorstore.add_texts(texts=texts, embeddings=embeddings, metadatas=metadatas)

# 4. Code-aware retrieval
def query_codebase(query):
    # Retrieve relevant code chunks
    results = vectorstore.similarity_search(query, k=5)
    
    # Build context with code structure
    context_parts = []
    for result in results:
        context_parts.append(f"File: {result.metadata['file']}")
        context_parts.append(f"Type: {result.metadata['type']}")
        context_parts.append(f"Name: {result.metadata['name']}")
        context_parts.append(f"Code:\\n{result.page_content}")
        context_parts.append("")
    
    context = "\\n".join(context_parts)
    
    # Generate answer
    prompt = f"""Answer the question about this codebase:
    
    Context:
    {context}
    
    Question: {query}
    
    Answer:"""
    
    answer = llm(prompt)
    return answer

# 5. Technical documentation RAG
def index_technical_docs(docs_path):
    # Use markdown-aware chunking
    from langchain.text_splitter import MarkdownTextSplitter
    
    splitter = MarkdownTextSplitter(chunk_size=1000, chunk_overlap=200)
    
    for doc_file in os.listdir(docs_path):
        if doc_file.endswith('.md'):
            with open(os.path.join(docs_path, doc_file)) as f:
                content = f.read()
            
            chunks = splitter.split_text(content)
            
            # Extract headers for metadata
            headers = extract_markdown_headers(content)
            
            for i, chunk in enumerate(chunks):
                # Find relevant header
                header = find_header_for_chunk(chunk, headers)
                
                vectorstore.add_texts(
                    texts=[chunk],
                    metadatas=[{
                        "file": doc_file,
                        "section": header,
                        "chunk_index": i
                    }]
                )`,
            approaches: [
                "Parse code with AST to understand structure.",
                "Chunk at function/class level to preserve context.",
                "Use code-specific embedding models.",
                "Enrich metadata with function names, file paths, line numbers.",
                "Preserve code structure in chunking.",
                "Handle cross-references in documentation.",
                "Index both code and documentation together.",
                "Update index as code changes.",
                "Support multiple programming languages.",
                "Test with various code query patterns."
            ]
        });

        // Q26 - RAG with real-time data updates
        questions.push({
            number: 26,
            title: "How do you handle real-time data updates and keep RAG systems current?",
            description:
                "RAG systems need to stay current with changing data. Real-time updates require efficient indexing strategies, " +
                "incremental updates, cache invalidation, and handling of document versions. " +
                "This is critical for applications with frequently changing information.",
            why:
                "Stale information leads to incorrect answers. Real-time updates ensure RAG systems provide current information, " +
                "which is essential for time-sensitive applications like news, financial data, or live documentation.",
            what:
                "Update Strategies: Full reindexing (simple but expensive), Incremental updates (efficient for changes), " +
                "Delta updates (only changed portions), Event-driven updates (triggered by changes), " +
                "Version management (handle document versions), Cache invalidation (update cached results).",
            how:
                "1) Detect document changes (file watchers, webhooks, polling), 2) Extract changed content, " +
                "3) Generate embeddings for new/changed content, 4) Update vector store incrementally, " +
                "5) Invalidate relevant caches, 6) Handle version conflicts, 7) Monitor update performance.",
            pros: [
                "Keeps information current",
                "Better answer accuracy",
                "Handles dynamic content",
                "Efficient incremental updates",
                "Supports real-time applications",
                "Version management"
            ],
            cons: [
                "More complex implementation",
                "Requires change detection",
                "Cache invalidation complexity",
                "Version conflict handling",
                "Performance overhead",
                "Need for update infrastructure"
            ],
            diagram: `flowchart TD
    A["Document Change"] --> B["Change Detection"]
    B --> C["Extract Changes"]
    C --> D["Generate Embeddings"]
    D --> E["Update Vector Store"]
    E --> F["Invalidate Cache"]
    F --> G["Version Update"]
    G --> H["Updated RAG System"]
    I["User Query"] --> J{"Cache Valid?"}
    J -->|Yes| K["Return Cached"]
    J -->|No| H
    H --> L["Fresh Answer"]`,
            implementation: `# Real-time RAG Updates
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import hashlib

# 1. Change detection
class DocumentChangeHandler(FileSystemEventHandler):
    def __init__(self, rag_system):
        self.rag_system = rag_system
        self.file_hashes = {}
    
    def on_modified(self, event):
        if event.is_directory:
            return
        
        if event.src_path.endswith(('.pdf', '.md', '.txt')):
            self.handle_document_change(event.src_path)
    
    def handle_document_change(self, file_path):
        # Calculate file hash
        with open(file_path, 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()
        
        # Check if changed
        if self.file_hashes.get(file_path) != file_hash:
            self.rag_system.update_document(file_path)
            self.file_hashes[file_path] = file_hash

# 2. Incremental updates
class IncrementalRAGUpdater:
    def __init__(self, vectorstore):
        self.vectorstore = vectorstore
        self.document_index = {}  # Track indexed documents
    
    def update_document(self, document_path, document_content):
        # Chunk document
        chunks = chunk_document(document_content)
        
        # Get existing chunks for this document
        existing_ids = self.document_index.get(document_path, [])
        
        # Remove old chunks
        if existing_ids:
            self.vectorstore.delete(ids=existing_ids)
        
        # Generate embeddings for new chunks
        embeddings = embeddings_model.embed_documents([c.page_content for c in chunks])
        
        # Create new IDs
        new_ids = [f"{document_path}_{i}" for i in range(len(chunks))]
        
        # Add new chunks
        self.vectorstore.add_texts(
            texts=[c.page_content for c in chunks],
            embeddings=embeddings,
            ids=new_ids,
            metadatas=[{"source": document_path, "chunk_index": i} for i in range(len(chunks))]
        )
        
        # Update index
        self.document_index[document_path] = new_ids
    
    def delete_document(self, document_path):
        # Remove all chunks for this document
        if document_path in self.document_index:
            ids = self.document_index[document_path]
            self.vectorstore.delete(ids=ids)
            del self.document_index[document_path]

# 3. Event-driven updates
from celery import Celery

celery_app = Celery('rag_updater')

@celery_app.task
def process_document_update(document_id, document_content):
    updater = IncrementalRAGUpdater(vectorstore)
    updater.update_document(document_id, document_content)
    
    # Invalidate cache
    invalidate_cache_for_document(document_id)
    
    return {"status": "updated", "document_id": document_id}

# Webhook handler
@app.post("/webhook/document-updated")
async def document_updated_webhook(payload: dict):
    document_id = payload["document_id"]
    document_content = payload.get("content")
    
    # Queue update task
    process_document_update.delay(document_id, document_content)
    
    return {"status": "queued"}

# 4. Cache invalidation
def invalidate_cache_for_document(document_id):
    # Invalidate query result cache
    # Find all cached queries that used this document
    cached_keys = find_cache_keys_for_document(document_id)
    
    for key in cached_keys:
        redis_cache.delete(key)
    
    # Invalidate embedding cache if document changed significantly
    # (This is simplified - real implementation would track which embeddings to invalidate)

# 5. Version management
class VersionedRAGSystem:
    def __init__(self, vectorstore):
        self.vectorstore = vectorstore
        self.versions = {}  # document_id -> version
    
    def update_with_version(self, document_id, content, version):
        current_version = self.versions.get(document_id, 0)
        
        if version <= current_version:
            # Stale update, ignore
            return False
        
        # Update document
        updater = IncrementalRAGUpdater(self.vectorstore)
        updater.update_document(document_id, content)
        
        # Update version
        self.versions[document_id] = version
        
        return True

# 6. Real-time monitoring
def monitor_update_performance():
    update_times = []
    
    def track_update(func):
        def wrapper(*args, **kwargs):
            start = time.time()
            result = func(*args, **kwargs)
            duration = time.time() - start
            update_times.append(duration)
            return result
        return wrapper
    
    return track_update`,
            approaches: [
                "Use file watchers or webhooks for change detection.",
                "Implement incremental updates to avoid full reindexing.",
                "Track document versions to handle conflicts.",
                "Invalidate caches when documents update.",
                "Use message queues for async updates.",
                "Monitor update performance and optimize.",
                "Handle concurrent updates gracefully.",
                "Support both push and pull update models.",
                "Test with high-frequency updates.",
                "Consider eventual consistency for distributed systems."
            ]
        });

        // Q27 - RAG with multiple data sources
        questions.push({
            number: 27,
            title: "How do you implement RAG with multiple heterogeneous data sources?",
            description:
                "Real-world RAG systems often need to query across multiple data sources: databases, APIs, files, knowledge bases, etc. " +
                "This requires unified retrieval, source attribution, result fusion, and handling different data formats. " +
                "Multi-source RAG enables comprehensive answers from diverse information.",
            why:
                "Information is distributed across multiple sources. Single-source RAG is limited, but multi-source RAG " +
                "can provide comprehensive answers by combining information from databases, documents, APIs, and more.",
            what:
                "Challenges: Different data formats, Different access methods, Source prioritization, Result fusion, " +
                "Source attribution, Consistency across sources. " +
                "Solutions: Unified retrieval interface, Source-specific adapters, Result ranking and fusion, " +
                "Metadata for source tracking, Cross-source deduplication.",
            how:
                "1) Create adapters for each data source, 2) Implement unified retrieval interface, " +
                "3) Query all sources in parallel, 4) Rank and fuse results, 5) Attribute sources, " +
                "6) Handle conflicts and inconsistencies, 7) Present unified answer with citations.",
            pros: [
                "Comprehensive information access",
                "Handles diverse data sources",
                "Better answer quality",
                "Source transparency",
                "Flexible architecture",
                "Scalable to new sources"
            ],
            cons: [
                "More complex implementation",
                "Higher latency (multiple sources)",
                "Result fusion complexity",
                "Source conflict handling",
                "More infrastructure needed",
                "Consistency challenges"
            ],
            diagram: `flowchart TD
    A["User Query"] --> B["Query Router"]
    B --> C["Database Adapter"]
    B --> D["API Adapter"]
    B --> E["File Adapter"]
    B --> F["Vector Store Adapter"]
    C --> G["Database Results"]
    D --> H["API Results"]
    E --> I["File Results"]
    F --> J["Vector Results"]
    G --> K["Result Fusion"]
    H --> K
    I --> K
    J --> K
    K --> L["Rank & Deduplicate"]
    L --> M["Source Attribution"]
    M --> N["Unified Answer"]`,
            implementation: `# Multi-Source RAG
from abc import ABC, abstractmethod
import asyncio

# 1. Source adapter interface
class DataSourceAdapter(ABC):
    @abstractmethod
    async def search(self, query: str, k: int = 5):
        pass
    
    @abstractmethod
    def get_source_name(self):
        pass

# 2. Database adapter
class DatabaseAdapter(DataSourceAdapter):
    def __init__(self, db_connection):
        self.db = db_connection
        self.source_name = "database"
    
    async def search(self, query, k=5):
        # Convert query to SQL or use semantic search
        results = await self.semantic_search_db(query, k)
        return [{"content": r, "source": self.source_name, "score": r.score} for r in results]
    
    def get_source_name(self):
        return self.source_name

# 3. API adapter
class APIAdapter(DataSourceAdapter):
    def __init__(self, api_client, endpoint):
        self.api_client = api_client
        self.endpoint = endpoint
        self.source_name = "api"
    
    async def search(self, query, k=5):
        response = await self.api_client.post(self.endpoint, json={"query": query, "limit": k})
        results = response.json()
        return [{"content": r, "source": self.source_name, "score": r.get("score", 0.5)} for r in results]
    
    def get_source_name(self):
        return self.source_name

# 4. Vector store adapter
class VectorStoreAdapter(DataSourceAdapter):
    def __init__(self, vectorstore):
        self.vectorstore = vectorstore
        self.source_name = "documents"
    
    async def search(self, query, k=5):
        results = self.vectorstore.similarity_search_with_score(query, k=k)
        return [{"content": r[0].page_content, "source": self.source_name, "score": r[1]} for r in results]
    
    def get_source_name(self):
        return self.source_name

# 5. Multi-source retrieval
class MultiSourceRAG:
    def __init__(self):
        self.adapters = []
    
    def add_source(self, adapter: DataSourceAdapter):
        self.adapters.append(adapter)
    
    async def search_all_sources(self, query, k_per_source=5):
        # Query all sources in parallel
        tasks = [adapter.search(query, k_per_source) for adapter in self.adapters]
        results = await asyncio.gather(*tasks)
        
        # Flatten results
        all_results = []
        for source_results in results:
            all_results.extend(source_results)
        
        return all_results
    
    def fuse_results(self, results, top_k=10):
        # Sort by score
        sorted_results = sorted(results, key=lambda x: x["score"], reverse=True)
        
        # Deduplicate similar content
        unique_results = []
        seen_content = set()
        
        for result in sorted_results:
            content_hash = hash(result["content"][:100])  # Hash first 100 chars
            if content_hash not in seen_content:
                unique_results.append(result)
                seen_content.add(content_hash)
            
            if len(unique_results) >= top_k:
                break
        
        return unique_results
    
    async def query(self, query, top_k=10):
        # Search all sources
        all_results = await self.search_all_sources(query)
        
        # Fuse results
        fused_results = self.fuse_results(all_results, top_k)
        
        # Build context
        context = "\\n".join([
            f"[Source: {r['source']}] {r['content']}" 
            for r in fused_results
        ])
        
        # Generate answer
        prompt = f"""Answer the question using information from multiple sources:
        
        Context:
        {context}
        
        Question: {query}
        
        Answer with source citations:"""
        
        answer = llm(prompt)
        
        return {
            "answer": answer,
            "sources": fused_results,
            "source_count": len(set(r["source"] for r in fused_results))
        }

# 6. Usage
rag_system = MultiSourceRAG()
rag_system.add_source(DatabaseAdapter(db_connection))
rag_system.add_source(APIAdapter(api_client, "/search"))
rag_system.add_source(VectorStoreAdapter(vectorstore))

result = await rag_system.query("What is RAG?")`,
            approaches: [
                "Create adapter pattern for different sources.",
                "Query sources in parallel for better performance.",
                "Implement result fusion and ranking.",
                "Attribute sources clearly in answers.",
                "Handle source conflicts and inconsistencies.",
                "Deduplicate similar results across sources.",
                "Prioritize sources based on reliability.",
                "Monitor performance per source.",
                "Support adding/removing sources dynamically.",
                "Test with various source combinations."
            ]
        });

        // Q28 - RAG failure modes and handling
        questions.push({
            number: 28,
            title: "What are common RAG failure modes and how do you handle them?",
            description:
                "RAG systems can fail in various ways: retrieval failures (no relevant docs), generation failures (hallucinations, wrong answers), " +
                "context overflow (too much context), empty results, and system errors. " +
                "Understanding failure modes enables robust error handling and graceful degradation.",
            why:
                "Failures are inevitable in production systems. Proper error handling ensures systems remain usable, " +
                "provides good user experience, and enables debugging. Without handling failures, systems become unreliable.",
            what:
                "Failure Types: Retrieval failures (no relevant docs found), Generation failures (hallucinations, contradictions), " +
                "Context overflow (exceeds token limits), Empty results, Timeout errors, Rate limiting, " +
                "Invalid queries, Data quality issues, System errors.",
            how:
                "1) Detect failure types, 2) Implement fallback strategies, 3) Provide helpful error messages, " +
                "4) Log failures for analysis, 5) Retry with backoff, 6) Graceful degradation, " +
                "7) User feedback collection, 8) Monitoring and alerting.",
            pros: [
                "Better user experience",
                "System reliability",
                "Easier debugging",
                "Graceful degradation",
                "Failure pattern identification",
                "Improved over time"
            ],
            cons: [
                "Implementation complexity",
                "Need to anticipate failures",
                "Fallback logic overhead",
                "Testing complexity",
                "May mask underlying issues",
                "Requires monitoring"
            ],
            diagram: `flowchart TD
    A["User Query"] --> B["RAG System"]
    B --> C{"Retrieval Success?"}
    C -->|No| D["Fallback: Broader Search"]
    C -->|Yes| E{"Context Valid?"}
    E -->|No| F["Fallback: Reduce Context"]
    E -->|Yes| G{"Generation Success?"}
    G -->|No| H["Fallback: Simple Answer"]
    G -->|Yes| I{"Answer Quality Good?"}
    I -->|No| J["Fallback: Warning Message"]
    I -->|Yes| K["Return Answer"]
    D --> L["Error Handling"]
    F --> L
    H --> L
    J --> L
    L --> M["Log & Monitor"]`,
            implementation: `# RAG Failure Handling
from enum import Enum
import logging

class FailureType(Enum):
    NO_RESULTS = "no_results"
    LOW_RELEVANCE = "low_relevance"
    CONTEXT_OVERFLOW = "context_overflow"
    GENERATION_ERROR = "generation_error"
    HALLUCINATION = "hallucination"
    TIMEOUT = "timeout"
    RATE_LIMIT = "rate_limit"

class RAGSystemWithErrorHandling:
    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
        self.logger = logging.getLogger(__name__)
    
    def query_with_fallback(self, query, max_retries=3):
        for attempt in range(max_retries):
            try:
                result = self._process_query(query)
                
                # Validate result
                if self._validate_result(result):
                    return result
                else:
                    # Try fallback
                    return self._fallback_strategy(query, result)
            
            except Exception as e:
                failure_type = self._classify_failure(e)
                self.logger.error(f"Failure type: {failure_type}, Attempt: {attempt+1}", exc_info=True)
                
                if attempt < max_retries - 1:
                    # Retry with backoff
                    time.sleep(2 ** attempt)
                    continue
                else:
                    # Final fallback
                    return self._final_fallback(query, failure_type)
    
    def _process_query(self, query):
        # Retrieval
        docs = self.vectorstore.similarity_search(query, k=5)
        
        if not docs:
            raise ValueError("No documents found")
        
        # Check relevance
        if docs[0].metadata.get("similarity_score", 1.0) < 0.5:
            raise ValueError("Low relevance results")
        
        # Build context
        context = self._build_context(docs)
        
        # Check context size
        if self._estimate_tokens(context) > 4000:
            raise ValueError("Context overflow")
        
        # Generation
        answer = self.llm.generate(context, query)
        
        return {"answer": answer, "sources": docs}
    
    def _validate_result(self, result):
        # Check answer quality
        if not result["answer"] or len(result["answer"]) < 10:
            return False
        
        # Check for hallucinations (simplified)
        if self._detect_hallucination(result["answer"], result["sources"]):
            return False
        
        return True
    
    def _fallback_strategy(self, query, result):
        # Try with broader search
        broader_docs = self.vectorstore.similarity_search(query, k=10)
        
        # Reduce context
        reduced_context = self._reduce_context(broader_docs, max_tokens=2000)
        
        # Generate with warning
        answer = self.llm.generate(reduced_context, query)
        
        return {
            "answer": answer,
            "sources": broader_docs[:5],
            "warning": "Results may be less accurate due to low relevance"
        }
    
    def _final_fallback(self, query, failure_type):
        fallback_responses = {
            FailureType.NO_RESULTS: "I couldn't find relevant information to answer your question. Please try rephrasing.",
            FailureType.LOW_RELEVANCE: "I found some information, but it may not be directly relevant. Please refine your question.",
            FailureType.CONTEXT_OVERFLOW: "The query is too complex. Please break it into smaller questions.",
            FailureType.GENERATION_ERROR: "I encountered an error processing your query. Please try again.",
            FailureType.TIMEOUT: "The request took too long. Please try a simpler query.",
            FailureType.RATE_LIMIT: "Too many requests. Please try again in a moment."
        }
        
        return {
            "answer": fallback_responses.get(failure_type, "I'm unable to process your query right now."),
            "sources": [],
            "error": True,
            "failure_type": failure_type.value
        }
    
    def _detect_hallucination(self, answer, sources):
        # Simplified hallucination detection
        # Check if answer contains information not in sources
        source_text = " ".join([s.page_content for s in sources])
        
        # Extract key claims from answer
        # (This is simplified - real implementation would be more sophisticated)
        answer_keywords = set(answer.lower().split())
        source_keywords = set(source_text.lower().split())
        
        # If answer has many keywords not in sources, might be hallucination
        unique_keywords = answer_keywords - source_keywords
        if len(unique_keywords) > len(answer_keywords) * 0.3:
            return True
        
        return False
    
    def _reduce_context(self, docs, max_tokens):
        # Prioritize most relevant docs
        sorted_docs = sorted(docs, key=lambda d: d.metadata.get("similarity_score", 0), reverse=True)
        
        context_parts = []
        token_count = 0
        
        for doc in sorted_docs:
            doc_tokens = self._estimate_tokens(doc.page_content)
            if token_count + doc_tokens <= max_tokens:
                context_parts.append(doc.page_content)
                token_count += doc_tokens
            else:
                # Truncate last doc if needed
                remaining_tokens = max_tokens - token_count
                if remaining_tokens > 100:  # Only if meaningful
                    truncated = self._truncate_text(doc.page_content, remaining_tokens)
                    context_parts.append(truncated)
                break
        
        return "\\n".join(context_parts)`,
            approaches: [
                "Implement comprehensive error handling.",
                "Classify failure types for appropriate responses.",
                "Provide fallback strategies for each failure type.",
                "Validate results before returning to users.",
                "Implement retry logic with exponential backoff.",
                "Detect and warn about potential hallucinations.",
                "Handle context overflow gracefully.",
                "Provide helpful error messages to users.",
                "Log all failures for analysis.",
                "Monitor failure rates and patterns."
            ]
        });

        // Q29 - RAG with domain-specific fine-tuning
        questions.push({
            number: 29,
            title: "How do you fine-tune embedding models and LLMs for domain-specific RAG?",
            description:
                "General-purpose embeddings and LLMs may not perform well on specialized domains (medical, legal, technical). " +
                "Fine-tuning adapts models to domain-specific language, terminology, and knowledge, improving retrieval accuracy and answer quality. " +
                "This is crucial for specialized RAG applications.",
            why:
                "Domain-specific terminology, concepts, and relationships differ from general language. Fine-tuned models understand domain context better, " +
                "leading to more accurate retrieval and generation. This is essential for professional and technical applications.",
            what:
                "Fine-tuning Approaches: Embedding fine-tuning (adapt embeddings to domain), LLM fine-tuning (adapt generation to domain), " +
                "Domain-specific pre-training, Continued pre-training, Instruction tuning, Parameter-efficient fine-tuning (LoRA, QLoRA). " +
                "Data Requirements: Domain-specific datasets, High-quality examples, Sufficient volume.",
            how:
                "1) Collect domain-specific training data, 2) Prepare data (pairs, triplets, instructions), " +
                "3) Choose fine-tuning method (full, LoRA, etc.), 4) Fine-tune embedding model, " +
                "5) Fine-tune LLM (optional), 6) Evaluate on domain test set, 7) Deploy and monitor.",
            pros: [
                "Better domain understanding",
                "Improved retrieval accuracy",
                "More accurate answers",
                "Handles domain terminology",
                "Better for specialized use cases",
                "Competitive advantage"
            ],
            cons: [
                "Requires training data",
                "Computational costs",
                "Time-consuming process",
                "Need ML expertise",
                "Ongoing maintenance",
                "Risk of overfitting"
            ],
            diagram: `flowchart TD
    A["Domain Data"] --> B["Data Preparation"]
    B --> C["Training Pairs"]
    B --> D["Instruction Data"]
    C --> E["Embedding Fine-tuning"]
    D --> F["LLM Fine-tuning"]
    E --> G["Domain Embeddings"]
    F --> H["Domain LLM"]
    G --> I["RAG System"]
    H --> I
    I --> J["Domain-Specific RAG"]`,
            implementation: `# Domain-Specific Fine-tuning
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer

# 1. Fine-tune embeddings for domain
def fine_tune_embeddings(domain_data, base_model="sentence-transformers/all-MiniLM-L6-v2"):
    # Load base model
    model = SentenceTransformer(base_model)
    
    # Prepare training data (query-document pairs)
    train_examples = []
    
    for item in domain_data:
        # Positive pairs (relevant)
        train_examples.append(InputExample(
            texts=[item["query"], item["relevant_doc"]],
            label=1.0
        ))
        
        # Negative pairs (irrelevant)
        train_examples.append(InputExample(
            texts=[item["query"], item["irrelevant_doc"]],
            label=0.0
        ))
    
    # Create data loader
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
    
    # Define loss
    train_loss = losses.CosineSimilarityLoss(model)
    
    # Fine-tune
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=3,
        warmup_steps=100,
        output_path="./domain-embeddings"
    )
    
    return model

# 2. Fine-tune LLM with LoRA (Parameter-efficient)
from peft import LoraConfig, get_peft_model, TaskType

def fine_tune_llm_lora(domain_data, base_model="meta-llama/Llama-2-7b-hf"):
    # Load base model
    model = AutoModelForCausalLM.from_pretrained(base_model)
    
    # Configure LoRA
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,  # Rank
        lora_alpha=32,
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj"]
    )
    
    # Apply LoRA
    model = get_peft_model(model, lora_config)
    
    # Prepare training data
    def format_instruction(example):
        return f"""### Instruction:
{example['instruction']}

### Response:
{example['response']}"""
    
    train_texts = [format_instruction(ex) for ex in domain_data]
    
    # Tokenize
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(base_model)
    tokenizer.pad_token = tokenizer.eos_token
    
    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir="./domain-llm",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        warmup_steps=100,
        logging_steps=10,
        save_steps=500
    )
    
    # Train
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_encodings
    )
    
    trainer.train()
    model.save_pretrained("./domain-llm")
    
    return model

# 3. Use fine-tuned models in RAG
def create_domain_rag(domain_embeddings_path, domain_llm_path):
    # Load fine-tuned embeddings
    domain_embeddings = SentenceTransformer(domain_embeddings_path)
    
    # Create vector store with domain embeddings
    vectorstore = FAISS.from_documents(
        documents,
        embedding=domain_embeddings
    )
    
    # Load fine-tuned LLM
    from transformers import pipeline
    domain_llm = pipeline(
        "text-generation",
        model=domain_llm_path,
        device=0  # GPU if available
    )
    
    # Create RAG system
    def query_domain_rag(query):
        # Retrieve with domain embeddings
        docs = vectorstore.similarity_search(query, k=5)
        context = "\\n".join([d.page_content for d in docs])
        
        # Generate with domain LLM
        prompt = f"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:"
        answer = domain_llm(prompt, max_length=200, do_sample=False)[0]["generated_text"]
        
        return {"answer": answer, "sources": docs}
    
    return query_domain_rag

# 4. Evaluation
def evaluate_domain_rag(rag_system, test_set):
    correct = 0
    total = len(test_set)
    
    for test_case in test_set:
        result = rag_system(test_case["query"])
        
        # Evaluate answer quality (simplified)
        if evaluate_answer_quality(result["answer"], test_case["expected_answer"]):
            correct += 1
    
    accuracy = correct / total
    return accuracy

def evaluate_answer_quality(predicted, expected):
    # Use semantic similarity or LLM-as-judge
    from sentence_transformers import util
    
    model = SentenceTransformer('all-MiniLM-L6-v2')
    pred_emb = model.encode(predicted)
    exp_emb = model.encode(expected)
    
    similarity = util.cos_sim(pred_emb, exp_emb)
    return similarity.item() > 0.7  # Threshold`,
            approaches: [
                "Collect high-quality domain-specific training data.",
                "Start with parameter-efficient methods (LoRA) to save costs.",
                "Fine-tune embeddings first (often biggest impact).",
                "Use contrastive learning for embedding fine-tuning.",
                "Evaluate on domain-specific test sets.",
                "Monitor for overfitting during training.",
                "Consider continued pre-training for very specialized domains.",
                "Combine fine-tuning with prompt engineering.",
                "Regularly retrain as domain evolves.",
                "Compare fine-tuned vs base models to measure improvement."
            ]
        });

        // Q30 - RAG system testing strategies
        questions.push({
            number: 30,
            title: "What are comprehensive testing strategies for RAG systems?",
            description:
                "RAG systems require multi-level testing: unit tests for components, integration tests for workflows, " +
                "end-to-end tests for user scenarios, and evaluation tests for quality. " +
                "Comprehensive testing ensures reliability, quality, and prevents regressions.",
            why:
                "RAG systems are complex with many components. Without proper testing, bugs go undetected, quality degrades, " +
                "and changes break functionality. Testing enables confident deployment and continuous improvement.",
            what:
                "Test Types: Unit tests (individual components), Integration tests (component interactions), " +
                "End-to-end tests (full workflows), Evaluation tests (quality metrics), " +
                "Performance tests (latency, throughput), Load tests (under stress), " +
                "Regression tests (prevent breaking changes), A/B tests (compare versions).",
            how:
                "1) Write unit tests for each component, 2) Create integration tests for workflows, " +
                "3) Build end-to-end test scenarios, 4) Implement evaluation test suite, " +
                "5) Add performance benchmarks, 6) Set up CI/CD testing, " +
                "7) Monitor test coverage, 8) Regular test execution.",
            pros: [
                "Catches bugs early",
                "Prevents regressions",
                "Ensures quality",
                "Enables confident changes",
                "Documents expected behavior",
                "Supports continuous integration"
            ],
            cons: [
                "Requires time to write",
                "Need to maintain tests",
                "Test data management",
                "Flaky tests can be frustrating",
                "May slow development",
                "Requires testing expertise"
            ],
            diagram: `flowchart TD
    A["Test Strategy"] --> B["Unit Tests"]
    A --> C["Integration Tests"]
    A --> D["E2E Tests"]
    A --> E["Evaluation Tests"]
    B --> F["Component Validation"]
    C --> G["Workflow Validation"]
    D --> H["User Scenario Validation"]
    E --> I["Quality Validation"]
    F --> J["Test Results"]
    G --> J
    H --> J
    I --> J
    J --> K["CI/CD Pipeline"]`,
            implementation: `# RAG Testing Strategies
import pytest
from unittest.mock import Mock, patch

# 1. Unit tests
class TestChunking:
    def test_chunk_size(self):
        splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)
        text = "A" * 500
        chunks = splitter.split_text(text)
        
        assert all(len(chunk) <= 100 for chunk in chunks)
        assert len(chunks) > 1
    
    def test_chunk_overlap(self):
        splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)
        text = "A" * 200
        chunks = splitter.split_text(text)
        
        if len(chunks) > 1:
            # Check overlap between consecutive chunks
            overlap = set(chunks[0][-20:]) & set(chunks[1][:20])
            assert len(overlap) > 0

class TestRetrieval:
    def test_similarity_search(self):
        vectorstore = create_test_vectorstore()
        results = vectorstore.similarity_search("test query", k=3)
        
        assert len(results) == 3
        assert all(hasattr(r, 'page_content') for r in results)
    
    def test_metadata_filtering(self):
        vectorstore = create_test_vectorstore()
        results = vectorstore.similarity_search(
            "test",
            k=5,
            filter={"category": "test"}
        )
        
        assert all(r.metadata.get("category") == "test" for r in results)

# 2. Integration tests
class TestRAGPipeline:
    @pytest.fixture
    def rag_system(self):
        return create_test_rag_system()
    
    def test_end_to_end_query(self, rag_system):
        result = rag_system.query("What is RAG?")
        
        assert "answer" in result
        assert "sources" in result
        assert len(result["sources"]) > 0
        assert len(result["answer"]) > 10
    
    def test_retrieval_and_generation(self, rag_system):
        query = "Test question"
        
        # Mock LLM to verify it's called correctly
        with patch.object(rag_system.llm, 'generate') as mock_llm:
            mock_llm.return_value = "Test answer"
            
            result = rag_system.query(query)
            
            # Verify LLM was called with context
            assert mock_llm.called
            call_args = mock_llm.call_args
            assert query in str(call_args)
            assert "context" in str(call_args).lower()

# 3. Evaluation tests
class TestRAGEvaluation:
    def test_retrieval_precision(self):
        test_cases = [
            {
                "query": "What is RAG?",
                "expected_docs": ["doc1", "doc2"],
                "retrieved_docs": ["doc1", "doc2", "doc3"]
            }
        ]
        
        for case in test_cases:
            precision = calculate_precision(
                case["retrieved_docs"],
                case["expected_docs"]
            )
            assert precision >= 0.5  # At least 50% precision
    
    def test_answer_quality(self):
        test_cases = [
            {
                "query": "What is RAG?",
                "context": "RAG is Retrieval-Augmented Generation...",
                "expected_answer": "RAG combines retrieval and generation",
                "generated_answer": "RAG is a technique that..."
            }
        ]
        
        for case in test_cases:
            quality_score = evaluate_answer_quality(
                case["generated_answer"],
                case["expected_answer"]
            )
            assert quality_score > 0.7

# 4. Performance tests
class TestRAGPerformance:
    def test_query_latency(self):
        rag_system = create_test_rag_system()
        
        start = time.time()
        result = rag_system.query("Test query")
        latency = time.time() - start
        
        assert latency < 5.0  # Should complete in under 5 seconds
    
    def test_throughput(self):
        rag_system = create_test_rag_system()
        queries = ["query1", "query2", "query3"] * 10
        
        start = time.time()
        results = [rag_system.query(q) for q in queries]
        total_time = time.time() - start
        
        throughput = len(queries) / total_time
        assert throughput > 1.0  # At least 1 query per second

# 5. Load tests
def test_under_load():
    rag_system = create_test_rag_system()
    
    # Simulate concurrent requests
    import concurrent.futures
    
    queries = [f"query{i}" for i in range(100)]
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(rag_system.query, q) for q in queries]
        results = [f.result() for f in concurrent.futures.as_completed(futures)]
    
    # Check all succeeded
    assert all("answer" in r for r in results)
    assert len(results) == 100

# 6. Regression tests
class TestRegression:
    def test_no_breaking_changes(self):
        # Test that existing functionality still works
        rag_system = create_test_rag_system()
        
        # Known good queries and expected results
        regression_cases = [
            ("What is RAG?", "should contain 'retrieval' and 'generation'"),
            ("How does it work?", "should explain the process")
        ]
        
        for query, expectation in regression_cases:
            result = rag_system.query(query)
            assert expectation in result["answer"].lower() or \
                   any(expectation in s.page_content.lower() for s in result["sources"])

# 7. Test fixtures
@pytest.fixture
def create_test_vectorstore():
    # Create minimal vector store for testing
    documents = [
        Document(page_content="RAG is Retrieval-Augmented Generation", metadata={"id": "doc1"}),
        Document(page_content="It combines retrieval and generation", metadata={"id": "doc2"})
    ]
    return FAISS.from_documents(documents, OpenAIEmbeddings())

@pytest.fixture
def create_test_rag_system():
    vectorstore = create_test_vectorstore()
    llm = Mock()  # Mock LLM for testing
    llm.generate.return_value = "Test answer"
    
    return RAGSystem(vectorstore, llm)`,
            approaches: [
                "Write unit tests for each component (chunking, retrieval, generation).",
                "Create integration tests for full workflows.",
                "Build end-to-end tests with realistic scenarios.",
                "Implement evaluation test suite with quality metrics.",
                "Add performance benchmarks and monitor.",
                "Set up CI/CD to run tests automatically.",
                "Maintain test data and fixtures.",
                "Test edge cases and error conditions.",
                "Use mocking for external dependencies.",
                "Regularly review and update tests."
            ]
        });

        // Q31 - RAG with semantic caching
        questions.push({
            number: 31,
            title: "How do you implement semantic caching to reduce costs and latency in RAG systems?",
            description:
                "Semantic caching stores and retrieves results based on semantic similarity of queries, not exact matches. " +
                "When a similar query is received, the cached result can be reused, significantly reducing LLM API calls and latency. " +
                "This is more effective than exact-match caching for natural language queries.",
            why:
                "Many queries are semantically similar even if worded differently. Semantic caching can serve cached results for similar queries, " +
                "reducing costs by 50-80% and improving latency. This is crucial for high-volume RAG applications.",
            what:
                "Semantic Caching: Stores query embeddings and results, Retrieves based on similarity threshold, " +
                "More flexible than exact-match caching. " +
                "Components: Query embedding, Similarity search in cache, Threshold-based retrieval, " +
                "Cache invalidation, Result freshness checks.",
            how:
                "1) Embed incoming query, 2) Search cache for similar queries (cosine similarity), " +
                "3) If similarity > threshold, return cached result, 4) Otherwise, process query and cache result, " +
                "5) Implement cache invalidation strategy, 6) Monitor cache hit rate.",
            pros: [
                "Significant cost reduction (50-80%)",
                "Lower latency for cached queries",
                "Handles query variations",
                "Better than exact-match caching",
                "Scalable solution",
                "Improves user experience"
            ],
            cons: [
                "Requires similarity threshold tuning",
                "May return slightly different results",
                "Cache storage overhead",
                "Need invalidation strategy",
                "Similarity computation cost",
                "May cache incorrect results"
            ],
            diagram: `flowchart TD
    A["New Query"] --> B["Generate Embedding"]
    B --> C["Search Cache"]
    C --> D{"Similarity > Threshold?"}
    D -->|Yes| E["Return Cached Result"]
    D -->|No| F["Process Query"]
    F --> G["Generate Answer"]
    G --> H["Cache Result"]
    H --> I["Return Answer"]
    E --> J["Update Cache Stats"]`,
            implementation: `# Semantic Caching for RAG
from sentence_transformers import SentenceTransformer, util
import numpy as np
from collections import defaultdict

class SemanticCache:
    def __init__(self, similarity_threshold=0.85):
        self.cache = {}  # query_embedding -> result
        self.query_embeddings = []  # List of embeddings for similarity search
        self.query_texts = []  # Original query texts
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.similarity_threshold = similarity_threshold
        self.stats = {"hits": 0, "misses": 0}
    
    def get(self, query):
        # Generate embedding for query
        query_embedding = self.embedding_model.encode(query, convert_to_tensor=True)
        
        # Search for similar cached queries
        if len(self.query_embeddings) > 0:
            # Convert to tensor
            cached_embeddings = np.array(self.query_embeddings)
            
            # Calculate similarities
            similarities = util.cos_sim(
                query_embedding,
                cached_embeddings
            )[0]
            
            # Find best match
            max_similarity_idx = similarities.argmax().item()
            max_similarity = similarities[max_similarity_idx].item()
            
            if max_similarity >= self.similarity_threshold:
                # Cache hit
                self.stats["hits"] += 1
                cached_query = self.query_texts[max_similarity_idx]
                result = self.cache[cached_query]
                
                # Return with similarity score
                return {
                    "cached": True,
                    "result": result,
                    "similarity": max_similarity,
                    "original_query": cached_query
                }
        
        # Cache miss
        self.stats["misses"] += 1
        return {"cached": False}
    
    def put(self, query, result):
        # Store in cache
        self.cache[query] = result
        
        # Store embedding
        query_embedding = self.embedding_model.encode(query)
        self.query_embeddings.append(query_embedding)
        self.query_texts.append(query)
        
        # Limit cache size (optional)
        max_size = 10000
        if len(self.cache) > max_size:
            # Remove oldest entries (FIFO)
            oldest_query = self.query_texts.pop(0)
            self.query_embeddings.pop(0)
            del self.cache[oldest_query]
    
    def get_stats(self):
        total = self.stats["hits"] + self.stats["misses"]
        hit_rate = self.stats["hits"] / total if total > 0 else 0
        return {
            "hit_rate": hit_rate,
            "hits": self.stats["hits"],
            "misses": self.stats["misses"],
            "size": len(self.cache)
        }

# Use in RAG system
semantic_cache = SemanticCache(similarity_threshold=0.85)

def cached_rag_query(query):
    # Check cache
    cached_result = semantic_cache.get(query)
    
    if cached_result["cached"]:
        print(f"Cache hit! Similarity: {cached_result['similarity']:.2f}")
        return cached_result["result"]
    
    # Process query
    docs = vectorstore.similarity_search(query, k=5)
    answer = llm_chain.run(context=docs, question=query)
    
    result = {
        "answer": answer,
        "sources": docs
    }
    
    # Cache result
    semantic_cache.put(query, result)
    
    return result

# Advanced: Using GPTCache
try:
    from gptcache import Cache
    from gptcache.embedding import Onnx
    from gptcache.adapter import openai
    
    # Initialize cache
    embedding_func = Onnx()
    cache = Cache()
    cache.init(
        embedding_func=embedding_func,
        similarity_threshold=0.85
    )
    
    # Use with OpenAI
    openai.ChatCompletion.create = cache.adapt(openai.ChatCompletion.create)
    
    # Now all OpenAI calls are cached semantically
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": query}]
    )
except ImportError:
    print("GPTCache not installed")`,
            approaches: [
                "Use semantic similarity threshold (typically 0.8-0.9).",
                "Embed queries efficiently for fast similarity search.",
                "Monitor cache hit rate and adjust threshold.",
                "Implement cache size limits to manage memory.",
                "Consider cache invalidation for time-sensitive data.",
                "Use specialized libraries like GPTCache.",
                "Track cost savings from caching.",
                "Test with various query variations.",
                "Balance between cache hits and result freshness.",
                "Consider distributed caching for scale."
            ]
        });

        // Q32 - RAG with agentic workflows
        questions.push({
            number: 32,
            title: "How do you build agentic RAG systems that can reason and take actions?",
            description:
                "Agentic RAG extends traditional RAG with reasoning capabilities, tool use, and action-taking. " +
                "Agents can break down complex queries, use tools to gather information, reason about results, " +
                "and take actions based on retrieved information. This enables more sophisticated RAG applications.",
            why:
                "Complex queries require reasoning, multi-step retrieval, and sometimes actions. Agentic RAG can handle " +
                "queries like 'Research X topic and summarize findings' or 'Find and update document Y', " +
                "going beyond simple Q&A.",
            what:
                "Agent Components: Reasoning (plans steps), Tools (search, APIs, databases), Memory (conversation, context), " +
                "Action execution, Iterative refinement. " +
                "Patterns: ReAct (Reasoning + Acting), Plan-and-Execute, Reflexion (self-critique), " +
                "Tool-augmented RAG, Multi-agent collaboration.",
            how:
                "1) Define agent with tools and memory, 2) Implement reasoning loop (think → act → observe), " +
                "3) Use RAG for information retrieval, 4) Execute tools based on retrieved info, " +
                "5) Refine based on results, 6) Return final answer or action result.",
            pros: [
                "Handles complex queries",
                "Can take actions",
                "Reasoning capabilities",
                "Multi-step problem solving",
                "Tool integration",
                "More powerful than basic RAG"
            ],
            cons: [
                "More complex implementation",
                "Higher latency (multiple steps)",
                "More expensive (multiple LLM calls)",
                "Can get stuck in loops",
                "Harder to debug",
                "Requires careful prompt engineering"
            ],
            diagram: `flowchart TD
    A["User Query"] --> B["Agent Reasoning"]
    B --> C["Plan Steps"]
    C --> D["Step 1: RAG Retrieval"]
    D --> E["Step 2: Use Tool"]
    E --> F["Step 3: Analyze Results"]
    F --> G{"Goal Achieved?"}
    G -->|No| B
    G -->|Yes| H["Final Answer/Action"]
    I["Memory"] --> B
    J["Tools"] --> E`,
            implementation: `# Agentic RAG System
from langchain.agents import initialize_agent, Tool, AgentType
from langchain.memory import ConversationBufferMemory
from langchain.llms import OpenAI

# 1. Define RAG tool
def rag_search(query: str) -> str:
    """Search documents using RAG. Input should be a search query."""
    docs = vectorstore.similarity_search(query, k=3)
    context = "\\n".join([d.page_content for d in docs])
    return context

# 2. Define other tools
def web_search(query: str) -> str:
    """Search the web for current information. Input should be a search query."""
    # Implementation would call web search API
    return f"Web search results for: {query}"

def database_query(query: str) -> str:
    """Query database. Input should be a SQL-like query."""
    # Implementation would execute database query
    return "Database results"

# 3. Create agent with tools
tools = [
    Tool(
        name="RAG Search",
        func=rag_search,
        description="Search internal documents using RAG. Use this for questions about company documents, policies, or knowledge base."
    ),
    Tool(
        name="Web Search",
        func=web_search,
        description="Search the web for current information. Use this for recent events or information not in documents."
    ),
    Tool(
        name="Database Query",
        func=database_query,
        description="Query the database. Use this for structured data queries."
    )
]

# 4. Initialize agent
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
llm = OpenAI(temperature=0)

agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
    memory=memory,
    verbose=True
)

# 5. Use agent
result = agent.run("What is our company policy on remote work? Also check if there are any recent updates online.")

# Advanced: Custom agent with ReAct pattern
class ReActRAGAgent:
    def __init__(self, vectorstore, tools, llm):
        self.vectorstore = vectorstore
        self.tools = {tool.name: tool for tool in tools}
        self.llm = llm
        self.memory = []
    
    def think(self, query, context):
        prompt = f"""You are a helpful assistant with access to tools.
        
        Context: {context}
        Query: {query}
        
        Think about what you need to do. Available tools: {list(self.tools.keys())}
        
        Thought:"""
        
        thought = self.llm(prompt)
        return thought
    
    def act(self, thought):
        # Extract action from thought
        if "use RAG" in thought.lower():
            # Extract query from thought
            query = self.extract_query_from_thought(thought)
            return self.rag_search(query)
        elif "use tool" in thought.lower():
            tool_name = self.extract_tool_from_thought(thought)
            if tool_name in self.tools:
                return self.tools[tool_name].run(thought)
        return None
    
    def rag_search(self, query):
        docs = self.vectorstore.similarity_search(query, k=3)
        return "\\n".join([d.page_content for d in docs])
    
    def run(self, query, max_iterations=5):
        context = ""
        
        for i in range(max_iterations):
            # Think
            thought = self.think(query, context)
            self.memory.append({"step": i, "thought": thought})
            
            # Act
            action_result = self.act(thought)
            
            if action_result:
                context += f"\\nStep {i+1} result: {action_result}"
            
            # Check if we have enough information
            if self.has_sufficient_info(context, query):
                break
        
        # Generate final answer
        final_prompt = f"""Based on the following information, answer the query:
        
        Information gathered:
        {context}
        
        Query: {query}
        
        Answer:"""
        
        answer = self.llm(final_prompt)
        return answer`,
            approaches: [
                "Start with simple agent patterns before complex ones.",
                "Use ReAct pattern for reasoning and acting.",
                "Provide clear tool descriptions for agent.",
                "Implement iteration limits to prevent loops.",
                "Use memory to maintain conversation context.",
                "Monitor agent behavior and refine prompts.",
                "Test with various complex queries.",
                "Handle tool failures gracefully.",
                "Consider multi-agent systems for complex tasks.",
                "Balance autonomy with control."
            ]
        });

        // Q33 - RAG with knowledge graphs
        questions.push({
            number: 33,
            title: "How do you combine knowledge graphs with RAG for enhanced retrieval?",
            description:
                "Knowledge graphs represent entities and relationships explicitly. Combining knowledge graphs with RAG " +
                "enables both semantic search (RAG) and relationship traversal (graph), providing more precise and " +
                "contextually rich answers. This is powerful for domain-specific applications.",
            why:
                "Some queries require understanding relationships (e.g., 'Who worked with X?', 'What are dependencies?'). " +
                "Pure vector search may miss these. Knowledge graphs excel at relationship queries, while RAG handles semantic similarity.",
            what:
                "Hybrid Approach: Vector search for semantic similarity, Graph traversal for relationships. " +
                "Components: Knowledge graph (entities, relationships), Vector embeddings (for semantic search), " +
                "Graph-RAG (combines both), Entity extraction, Relationship extraction, Unified retrieval.",
            how:
                "1) Extract entities and relationships from documents, 2) Build knowledge graph, 3) Generate embeddings for entities, " +
                "4) For queries: use vector search for semantic matches, graph traversal for relationships, " +
                "5) Combine results, 6) Generate answer with both semantic and relational context.",
            pros: [
                "Handles relationship queries",
                "More precise answers",
                "Structured knowledge representation",
                "Combines semantic and relational search",
                "Better for complex queries",
                "Enables reasoning over relationships"
            ],
            cons: [
                "More complex architecture",
                "Requires graph database",
                "Entity extraction complexity",
                "Higher infrastructure costs",
                "More challenging to maintain",
                "Integration complexity"
            ],
            diagram: `flowchart TD
    A["Documents"] --> B["Entity Extraction"]
    B --> C["Knowledge Graph"]
    B --> D["Vector Embeddings"]
    C --> E["Graph Database"]
    D --> F["Vector Database"]
    G["User Query"] --> H["Entity Recognition"]
    H --> I["Graph Traversal"]
    H --> J["Vector Search"]
    E --> I
    F --> J
    I --> K["Combine Results"]
    J --> K
    K --> L["Context Assembly"]
    L --> M["LLM Generation"]`,
            implementation: `# Knowledge Graph + RAG
from neo4j import GraphDatabase
from langchain.graphs import Neo4jGraph
from langchain.vectorstores import Neo4jVector

# 1. Setup Neo4j
driver = GraphDatabase.driver("bolt://localhost:7687", auth=("neo4j", "password"))
graph = Neo4jGraph(driver=driver)

# 2. Extract entities and relationships
def extract_knowledge(documents):
    for doc in documents:
        # Use LLM or NER to extract entities
        entities = extract_entities(doc.content)
        relationships = extract_relationships(doc.content, entities)
        
        # Add to graph
        for entity in entities:
            graph.query(
                "MERGE (e:Entity {name: $name, type: $type})",
                {"name": entity["name"], "type": entity["type"]}
            )
        
        # Add relationships
        for rel in relationships:
            graph.query(
                """
                MATCH (a:Entity {name: $source})
                MATCH (b:Entity {name: $target})
                MERGE (a)-[r:RELATES {type: $rel_type}]->(b)
                """,
                {
                    "source": rel["source"],
                    "target": rel["target"],
                    "rel_type": rel["type"]
                }
            )

# 3. Create vector store with graph
vectorstore = Neo4jVector.from_documents(
    documents=documents,
    embedding=embeddings,
    url="bolt://localhost:7687",
    username="neo4j",
    password="password",
    index_name="document_embeddings"
)

# 4. Hybrid retrieval
def hybrid_graph_rag_query(query):
    # Vector search
    vector_results = vectorstore.similarity_search(query, k=5)
    
    # Extract entities from query
    query_entities = extract_entities_from_query(query)
    
    # Graph traversal
    graph_context = []
    for entity in query_entities:
        # Find related entities
        related = graph.query(
            """
            MATCH (e:Entity {name: $name})-[r]->(related:Entity)
            RETURN related.name as name, related.type as type, type(r) as relationship
            LIMIT 10
            """,
            {"name": entity}
        )
        graph_context.extend(related)
    
    # Combine contexts
    combined_context = {
        "semantic_matches": [r.page_content for r in vector_results],
        "graph_relationships": graph_context
    }
    
    # Generate answer
    prompt = f"""Answer using both document context and knowledge graph:
    
    Documents:
    {format_documents(combined_context['semantic_matches'])}
    
    Knowledge Graph Relationships:
    {format_graph(combined_context['graph_relationships'])}
    
    Question: {query}
    
    Answer:"""
    
    answer = llm(prompt)
    return answer

# 5. Advanced: Graph-augmented retrieval
def graph_augmented_retrieval(query):
    # Step 1: Initial vector search
    initial_docs = vectorstore.similarity_search(query, k=10)
    
    # Step 2: Extract entities from retrieved docs
    entities_in_docs = set()
    for doc in initial_docs:
        entities = extract_entities(doc.page_content)
        entities_in_docs.update([e["name"] for e in entities])
    
    # Step 3: Find related entities in graph
    expanded_entities = set(entities_in_docs)
    for entity in entities_in_docs:
        related = graph.query(
            """
            MATCH (e:Entity {name: $name})-[r]-(related:Entity)
            RETURN related.name as name
            LIMIT 5
            """,
            {"name": entity}
        )
        expanded_entities.update([r["name"] for r in related])
    
    # Step 4: Retrieve documents mentioning expanded entities
    entity_filter = {"entities": list(expanded_entities)}
    expanded_docs = vectorstore.similarity_search(
        query,
        k=5,
        filter=entity_filter
    )
    
    # Step 5: Combine and rank
    all_docs = list(set(initial_docs + expanded_docs))
    return all_docs[:5]`,
            approaches: [
                "Extract entities and relationships during document ingestion.",
                "Use Neo4j or similar graph database.",
                "Combine vector search with graph traversal.",
                "Enrich retrieval with graph context.",
                "Use graph queries for relationship-based questions.",
                "Maintain both vector and graph representations.",
                "Consider using LangChain's graph integration.",
                "Monitor graph size and performance.",
                "Update graph as documents change.",
                "Test with relationship-heavy queries."
            ]
        });

        // Q34 - RAG optimization techniques
        questions.push({
            number: 34,
            title: "What are advanced optimization techniques for improving RAG system performance?",
            description:
                "RAG optimization involves improving retrieval accuracy, generation quality, latency, and costs. " +
                "Advanced techniques include query optimization, result reranking, context compression, " +
                "embedding optimization, and hybrid approaches. These techniques can significantly improve system performance.",
            why:
                "Optimization is crucial for production RAG systems. Poor performance leads to high costs, slow responses, " +
                "and poor user experience. Advanced optimization techniques can improve accuracy by 20-40% and reduce costs by 50%+.",
            what:
                "Optimization Areas: Query optimization (query expansion, rewriting), Retrieval optimization (reranking, filtering), " +
                "Context optimization (compression, selection), Embedding optimization (fine-tuning, model selection), " +
                "Generation optimization (prompt engineering, model selection), Caching strategies, Batch processing.",
            how:
                "1) Analyze performance bottlenecks, 2) Implement query optimization (expansion, rewriting), " +
                "3) Add reranking for better precision, 4) Compress context to reduce tokens, " +
                "5) Fine-tune embeddings for domain, 6) Optimize prompts, 7) Implement caching, " +
                "8) Monitor and iterate.",
            pros: [
                "Improved accuracy (20-40%)",
                "Reduced costs (50%+)",
                "Lower latency",
                "Better user experience",
                "More efficient resource use",
                "Competitive advantage"
            ],
            cons: [
                "Requires optimization effort",
                "May need experimentation",
                "Complexity increases",
                "Need to measure impact",
                "Requires expertise",
                "Ongoing maintenance"
            ],
            diagram: `flowchart TD
    A["Query"] --> B["Query Optimization"]
    B --> C["Expanded Query"]
    C --> D["Vector Search"]
    D --> E["Initial Results"]
    E --> F["Reranking"]
    F --> G["Top Results"]
    G --> H["Context Compression"]
    H --> I["Optimized Context"]
    I --> J["Prompt Optimization"]
    J --> K["LLM Generation"]
    K --> L["Optimized Answer"]`,
            implementation: `# Advanced RAG Optimization
from sentence_transformers import CrossEncoder
from langchain.retrievers.document_compressors import LLMChainExtractor

# 1. Query optimization
def optimize_query(query):
    # Query expansion
    expanded_queries = expand_query(query)
    
    # Query rewriting for better retrieval
    rewritten = rewrite_query(query)
    
    return {
        "original": query,
        "expanded": expanded_queries,
        "rewritten": rewritten
    }

def expand_query(query):
    prompt = f"""Generate 3 alternative phrasings of this query for better document retrieval:
    
    Query: {query}
    
    Alternatives:"""
    
    alternatives = llm(prompt)
    return [q.strip() for q in alternatives.split("\\n") if q.strip()]

# 2. Reranking
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def rerank_results(query, documents, top_k=3):
    # Score each document
    pairs = [[query, doc.page_content] for doc in documents]
    scores = reranker.predict(pairs)
    
    # Sort by score
    ranked = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
    
    return [doc for doc, score in ranked[:top_k]]

# 3. Context compression
compressor = LLMChainExtractor.from_llm(llm)

def compress_context(query, documents):
    # Extract only relevant parts
    compressed_docs = []
    for doc in documents:
        compressed = compressor.compress_documents([doc], query)
        compressed_docs.extend(compressed)
    
    return compressed_docs

# 4. Optimized retrieval pipeline
def optimized_retrieval(query, k=5):
    # Step 1: Query optimization
    optimized = optimize_query(query)
    
    # Step 2: Multi-query retrieval
    all_results = []
    for q in [optimized["original"], optimized["rewritten"]]:
        results = vectorstore.similarity_search(q, k=k*2)
        all_results.extend(results)
    
    # Step 3: Deduplicate
    unique_results = deduplicate_results(all_results)
    
    # Step 4: Rerank
    reranked = rerank_results(query, unique_results, top_k=k)
    
    # Step 5: Compress context
    compressed = compress_context(query, reranked)
    
    return compressed

# 5. Embedding optimization
def optimize_embeddings(domain_data):
    # Fine-tune embeddings on domain data
    from sentence_transformers import SentenceTransformer, InputExample, losses
    from torch.utils.data import DataLoader
    
    model = SentenceTransformer('all-MiniLM-L-6-v2')
    
    # Prepare training data
    train_examples = []
    for item in domain_data:
        train_examples.append(InputExample(
            texts=[item["query"], item["relevant_doc"]],
            label=1.0
        ))
    
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
    train_loss = losses.CosineSimilarityLoss(model)
    
    # Fine-tune
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=3
    )
    
    return model

# 6. Prompt optimization
def create_optimized_prompt(query, context):
    # Use structured prompt with clear instructions
    prompt = f"""You are a helpful assistant. Answer the question based ONLY on the provided context.

Context:
{context}

Question: {query}

Instructions:
- Use only information from the context
- If the context doesn't contain the answer, say "I don't have enough information"
- Cite sources when possible
- Be concise but complete

Answer:"""
    
    return prompt

# 7. Complete optimized pipeline
def optimized_rag_query(query):
    # Optimized retrieval
    docs = optimized_retrieval(query, k=5)
    
    # Build optimized context
    context = "\\n".join([d.page_content for d in docs])
    
    # Optimized prompt
    prompt = create_optimized_prompt(query, context)
    
    # Generate with optimized model (if available)
    answer = optimized_llm.generate(prompt)
    
    return {
        "answer": answer,
        "sources": docs,
        "optimization_applied": True
    }`,
            approaches: [
                "Start with query optimization (often biggest impact).",
                "Add reranking for better precision.",
                "Implement context compression to reduce tokens.",
                "Fine-tune embeddings for your domain.",
                "Optimize prompts for better generation.",
                "Use appropriate models (smaller when possible).",
                "Implement semantic caching.",
                "Monitor and measure optimization impact.",
                "A/B test optimizations.",
                "Iterate based on results."
            ]
        });

        // Q35 - RAG deployment patterns
        questions.push({
            number: 35,
            title: "What are the common deployment patterns for RAG systems in production?",
            description:
                "Production RAG deployment requires choosing appropriate patterns: serverless, containerized, microservices, " +
                "or monolithic. Each pattern has trade-offs in scalability, cost, complexity, and operational overhead. " +
                "The choice depends on requirements, team size, and infrastructure.",
            why:
                "Deployment pattern affects scalability, costs, operational complexity, and development velocity. " +
                "Choosing the wrong pattern can lead to poor performance, high costs, or maintenance challenges.",
            what:
                "Deployment Patterns: Serverless (AWS Lambda, Azure Functions), Containerized (Docker, Kubernetes), " +
                "Microservices (separate services), Monolithic (single service), Edge deployment (on-device), " +
                "Hybrid (combination of patterns).",
            how:
                "1) Assess requirements (scale, latency, cost), 2) Choose deployment pattern, " +
                "3) Set up infrastructure, 4) Deploy services, 5) Configure monitoring, " +
                "6) Set up CI/CD, 7) Implement scaling policies, 8) Monitor and optimize.",
            pros: [
                "Serverless: Low ops, auto-scaling, pay-per-use",
                "Containerized: Portable, scalable, consistent",
                "Microservices: Independent scaling, fault isolation",
                "Monolithic: Simpler, lower latency, easier debugging",
                "Edge: Low latency, offline capability",
                "Hybrid: Best of multiple approaches"
            ],
            cons: [
                "Serverless: Cold starts, vendor lock-in",
                "Containerized: Orchestration complexity",
                "Microservices: Network overhead, complexity",
                "Monolithic: Scaling limitations",
                "Edge: Limited resources, sync challenges",
                "Hybrid: More complex to manage"
            ],
            diagram: `flowchart TD
    A["RAG Application"] --> B{"Deployment Pattern"}
    B -->|Small Scale| C["Serverless"]
    B -->|Medium Scale| D["Containerized"]
    B -->|Large Scale| E["Microservices"]
    B -->|Simple| F["Monolithic"]
    C --> G["AWS Lambda"]
    C --> H["Azure Functions"]
    D --> I["Docker"]
    D --> J["Kubernetes"]
    E --> K["API Gateway"]
    E --> L["Service Mesh"]
    F --> M["Single Service"]`,
            implementation: `# RAG Deployment Patterns

# Pattern 1: Serverless (AWS Lambda)
import json
from langchain.vectorstores import Pinecone

def lambda_handler(event, context):
    query = event['queryStringParameters']['query']
    
    # Initialize vector store (connection pooling)
    vectorstore = Pinecone.from_existing_index(
        index_name="rag-index",
        embedding=embeddings
    )
    
    # Process query
    docs = vectorstore.similarity_search(query, k=5)
    answer = llm_chain.run(context=docs, question=query)
    
    return {
        'statusCode': 200,
        'body': json.dumps({
            'answer': answer,
            'sources': [d.page_content[:100] for d in docs]
        })
    }

# Pattern 2: Containerized (Docker + Kubernetes)
# Dockerfile
"""
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
"""

# Kubernetes deployment
"""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: rag-service
  template:
    metadata:
      labels:
        app: rag-service
    spec:
      containers:
      - name: rag-service
        image: rag-service:latest
        ports:
        - containerPort: 8000
        env:
        - name: PINECONE_API_KEY
          valueFrom:
            secretKeyRef:
              name: rag-secrets
              key: pinecone-key
"""

# Pattern 3: Microservices
# retrieval-service.py
from fastapi import FastAPI

app_retrieval = FastAPI()

@app_retrieval.post("/retrieve")
async def retrieve(query: str, k: int = 5):
    docs = vectorstore.similarity_search(query, k=k)
    return {"documents": [d.page_content for d in docs]}

# generation-service.py
app_generation = FastAPI()

@app_generation.post("/generate")
async def generate(query: str, context: list):
    answer = llm_chain.run(context=context, question=query)
    return {"answer": answer}

# Pattern 4: Monolithic
from fastapi import FastAPI

app = FastAPI()

@app.post("/query")
async def query(query: str):
    # All in one service
    docs = vectorstore.similarity_search(query, k=5)
    answer = llm_chain.run(context=docs, question=query)
    return {"answer": answer, "sources": docs}

# Pattern 5: Hybrid (Serverless + Containers)
# Use serverless for document ingestion
def ingest_document_lambda(event, context):
    document = event['document']
    # Process and index
    chunks = chunk_document(document)
    vectorstore.add_documents(chunks)
    return {"status": "indexed"}

# Use containers for query serving
# (Same as Pattern 2)

# Deployment considerations
class RAGDeployment:
    def __init__(self, pattern="containerized"):
        self.pattern = pattern
    
    def deploy(self):
        if self.pattern == "serverless":
            self.deploy_serverless()
        elif self.pattern == "containerized":
            self.deploy_containers()
        elif self.pattern == "microservices":
            self.deploy_microservices()
        else:
            self.deploy_monolithic()
    
    def setup_monitoring(self):
        # Add monitoring regardless of pattern
        from prometheus_client import start_http_server
        start_http_server(8000)
    
    def setup_scaling(self):
        if self.pattern == "serverless":
            # Auto-scaling built-in
            pass
        elif self.pattern == "containerized":
            # Kubernetes HPA
            pass
        elif self.pattern == "microservices":
            # Service-specific scaling
            pass`,
            approaches: [
                "Choose serverless for low/irregular traffic.",
                "Use containers for consistent, scalable deployments.",
                "Consider microservices for large, complex systems.",
                "Start with monolithic for simplicity, refactor as needed.",
                "Use edge deployment for low-latency requirements.",
                "Implement hybrid patterns for best of both worlds.",
                "Set up proper monitoring for any pattern.",
                "Plan for scaling from the start.",
                "Consider costs and operational overhead.",
                "Test deployment patterns before committing."
            ]
        });

        // Q36 - RAG with LLM function calling
        questions.push({
            number: 36,
            title: "How do you use LLM function calling (tool use) with RAG systems?",
            description:
                "LLM function calling allows LLMs to decide when to call external functions or tools. " +
                "In RAG systems, this enables dynamic retrieval - the LLM can decide what to search for, " +
                "when to retrieve more information, and how to combine multiple retrievals. This creates more intelligent RAG systems.",
            why:
                "Traditional RAG always retrieves before generating. Function calling enables conditional retrieval, " +
                "multi-step retrieval, and dynamic query generation. The LLM can reason about what information it needs and request it.",
            what:
                "Function Calling: LLM decides to call functions, Function definitions (tools available to LLM), " +
                "Tool selection (LLM chooses which tool), Parameter extraction (LLM provides function parameters), " +
                "Execution and response. " +
                "RAG Integration: RAG as a function/tool, Dynamic query generation, Conditional retrieval, Multi-step retrieval.",
            how:
                "1) Define RAG as a callable function/tool, 2) Provide function schema to LLM, " +
                "3) LLM decides when to call RAG function, 4) Execute RAG with LLM-generated query, " +
                "5) Return results to LLM, 6) LLM uses results to generate final answer.",
            pros: [
                "Dynamic and intelligent retrieval",
                "LLM decides what to search",
                "Multi-step reasoning",
                "Conditional retrieval",
                "Better query generation",
                "More flexible than fixed RAG"
            ],
            cons: [
                "More complex implementation",
                "Multiple LLM calls (higher cost)",
                "Higher latency",
                "Requires function calling support",
                "Need to handle tool errors",
                "More challenging to debug"
            ],
            diagram: `flowchart TD
    A["User Query"] --> B["LLM Reasoning"]
    B --> C{"Need Information?"}
    C -->|Yes| D["Call RAG Function"]
    C -->|No| E["Generate Answer"]
    D --> F["RAG Retrieval"]
    F --> G["Return Results"]
    G --> B
    E --> H["Final Answer"]`,
            implementation: `# RAG with Function Calling
from langchain.tools import Tool
from langchain.agents import initialize_agent, AgentType
from langchain.llms import OpenAI

# 1. Define RAG as a tool
def rag_search(query: str) -> str:
    """Search internal documents using RAG. 
    Use this when you need information from company documents, policies, or knowledge base.
    Input should be a search query string."""
    docs = vectorstore.similarity_search(query, k=5)
    context = "\\n".join([f"Document {i+1}: {d.page_content}" for i, d in enumerate(docs)])
    return context

# 2. Create tool
rag_tool = Tool(
    name="DocumentSearch",
    func=rag_search,
    description="Search company documents and knowledge base. Use this to find information about policies, procedures, or documentation."
)

# 3. Create agent with RAG tool
llm = OpenAI(temperature=0)
agent = initialize_agent(
    tools=[rag_tool],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# 4. Use agent (LLM decides when to use RAG)
result = agent.run("What is our remote work policy? Also check if there are any recent updates.")

# Advanced: OpenAI Function Calling
from openai import OpenAI

client = OpenAI()

def rag_function_call(query):
    # Define function schema
    functions = [{
        "name": "search_documents",
        "description": "Search internal documents using RAG",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "The search query"
                },
                "top_k": {
                    "type": "integer",
                    "description": "Number of documents to retrieve",
                    "default": 5
                }
            },
            "required": ["query"]
        }
    }]
    
    # First call - LLM decides if it needs to search
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": query}],
        functions=functions,
        function_call="auto"
    )
    
    message = response.choices[0].message
    
    # If LLM wants to call function
    if message.function_call:
        function_name = message.function_call.name
        function_args = json.loads(message.function_call.arguments)
        
        if function_name == "search_documents":
            # Execute RAG
            search_query = function_args["query"]
            top_k = function_args.get("top_k", 5)
            docs = vectorstore.similarity_search(search_query, k=top_k)
            function_result = "\\n".join([d.page_content for d in docs])
            
            # Second call - LLM uses RAG results
            second_response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "user", "content": query},
                    message,
                    {
                        "role": "function",
                        "name": function_name,
                        "content": function_result
                    }
                ]
            )
            
            return second_response.choices[0].message.content
    
    return message.content

# Multi-step RAG with function calling
def multi_step_rag(query, max_steps=3):
    messages = [{"role": "user", "content": query}]
    
    for step in range(max_steps):
        response = client.chat.completions.create(
            model="gpt-4",
            messages=messages,
            functions=[{
                "name": "search_documents",
                "description": "Search documents",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string"}
                    }
                }
            }],
            function_call="auto"
        )
        
        message = response.choices[0].message
        messages.append(message)
        
        if not message.function_call:
            # LLM has final answer
            return message.content
        
        # Execute function
        function_args = json.loads(message.function_call.arguments)
        docs = vectorstore.similarity_search(function_args["query"], k=5)
        function_result = "\\n".join([d.page_content for d in docs])
        
        messages.append({
            "role": "function",
            "name": message.function_call.name,
            "content": function_result
        })
    
    return "Maximum steps reached"`,
            approaches: [
                "Define RAG as a callable function/tool.",
                "Provide clear function descriptions for LLM.",
                "Use OpenAI function calling or LangChain agents.",
                "Handle function call errors gracefully.",
                "Implement multi-step retrieval when needed.",
                "Monitor function call usage and costs.",
                "Test with various query types.",
                "Consider caching function results.",
                "Set maximum steps to prevent loops.",
                "Log function calls for debugging."
            ]
        });

        // Q37 - RAG with summarization
        questions.push({
            number: 37,
            title: "How do you use summarization to improve RAG context management?",
            description:
                "Summarization helps manage context windows by condensing retrieved documents before passing to the LLM. " +
                "This enables RAG systems to use more source documents while staying within token limits. " +
                "Summarization can be applied to individual chunks, groups of chunks, or entire documents.",
            why:
                "Context windows are limited. Summarization allows including information from more documents " +
                "by condensing them, improving answer quality without exceeding token limits.",
            what:
                "Summarization Types: Chunk summarization (summarize each retrieved chunk), " +
                "Document summarization (summarize entire documents), Hierarchical summarization (summarize sections then combine), " +
                "Query-focused summarization (summarize with query in mind), Extractive vs abstractive summarization.",
            how:
                "1) Retrieve documents, 2) Summarize each document or group of documents, " +
                "3) Combine summaries into context, 4) Pass summarized context to LLM, " +
                "5) Generate answer from summaries, 6) Optionally include original sources for citation.",
            pros: [
                "Fits more information in context",
                "Reduces token usage",
                "Focuses on relevant information",
                "Handles long documents better",
                "Can improve answer quality",
                "More efficient than full documents"
            ],
            cons: [
                "May lose important details",
                "Additional LLM calls (cost)",
                "Summarization quality matters",
                "Higher latency",
                "Requires good summarization prompts",
                "May introduce errors"
            ],
            diagram: `flowchart TD
    A["Retrieved Documents"] --> B["Document 1"]
    A --> C["Document 2"]
    A --> D["Document 3"]
    B --> E["Summarize"]
    C --> E
    D --> E
    E --> F["Combined Summaries"]
    F --> G["Context Window"]
    G --> H["LLM Generation"]`,
            implementation: `# RAG with Summarization
from langchain.chains.summarize import load_summarize_chain
from langchain.llms import OpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. Summarize retrieved documents
def summarize_documents(documents, query):
    llm = OpenAI(temperature=0)
    
    # Create summarization chain
    chain = load_summarize_chain(llm, chain_type="stuff")
    
    # Summarize each document
    summaries = []
    for doc in documents:
        summary = chain.run([doc])
        summaries.append({
            "original": doc.page_content,
            "summary": summary,
            "source": doc.metadata.get("source", "unknown")
        })
    
    return summaries

# 2. Query-focused summarization
def query_focused_summarize(documents, query):
    llm = OpenAI(temperature=0)
    
    prompt = f"""Summarize the following document, focusing on information relevant to the query.
    
    Query: {query}
    
    Document:
    {{text}}
    
    Summary (focus on query relevance):"""
    
    summaries = []
    for doc in documents:
        summary = llm(prompt.format(text=doc.page_content))
        summaries.append(summary)
    
    return summaries

# 3. Hierarchical summarization
def hierarchical_summarize(long_document, query):
    # Split into sections
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,
        chunk_overlap=200
    )
    sections = text_splitter.split_text(long_document)
    
    # Summarize each section
    section_summaries = []
    for section in sections:
        summary = summarize_section(section, query)
        section_summaries.append(summary)
    
    # Combine section summaries
    combined_summary = "\\n".join(section_summaries)
    
    # Final summary if still too long
    if len(combined_summary.split()) > 1000:
        final_summary = summarize_text(combined_summary, query)
        return final_summary
    
    return combined_summary

# 4. RAG with summarization
def rag_with_summarization(query, k=10):
    # Retrieve more documents than needed
    docs = vectorstore.similarity_search(query, k=k)
    
    # Summarize documents
    summaries = summarize_documents(docs, query)
    
    # Build context from summaries
    context = "\\n".join([
        f"Source {i+1}: {s['summary']}" 
        for i, s in enumerate(summaries)
    ])
    
    # Generate answer
    prompt = f"""Answer the question using the following summarized information:
    
    Summaries:
    {context}
    
    Question: {query}
    
    Answer:"""
    
    answer = llm(prompt)
    
    return {
        "answer": answer,
        "sources": [s["source"] for s in summaries],
        "summaries": summaries
    }

# 5. Extractive summarization (keep original sentences)
def extractive_summarize(text, query, max_sentences=5):
    # Split into sentences
    sentences = text.split('. ')
    
    # Score sentences by relevance to query
    query_words = set(query.lower().split())
    scored_sentences = []
    
    for sentence in sentences:
        sentence_words = set(sentence.lower().split())
        overlap = len(query_words & sentence_words)
        scored_sentences.append((sentence, overlap))
    
    # Sort by score and take top N
    scored_sentences.sort(key=lambda x: x[1], reverse=True)
    top_sentences = [s[0] for s in scored_sentences[:max_sentences]]
    
    return '. '.join(top_sentences) + '.'

# 6. Use in RAG pipeline
def improved_rag_with_summarization(query):
    # Retrieve
    docs = vectorstore.similarity_search(query, k=8)
    
    # Summarize (query-focused)
    summaries = query_focused_summarize(docs, query)
    
    # Combine summaries
    context = "\\n\\n".join([
        f"[Source {i+1}]\\n{summary}" 
        for i, summary in enumerate(summaries)
    ])
    
    # Generate
    answer = llm_chain.run(context=context, question=query)
    
    return {
        "answer": answer,
        "summarized_sources": summaries,
        "original_documents": docs
    }`,
            approaches: [
                "Use summarization when context exceeds token limits.",
                "Apply query-focused summarization for better relevance.",
                "Consider hierarchical summarization for long documents.",
                "Balance summary length with information retention.",
                "Test summarization quality on your documents.",
                "Monitor token usage before and after summarization.",
                "Consider extractive summarization to preserve original text.",
                "Cache summaries to avoid recomputation.",
                "Use appropriate summarization models.",
                "Include source references even with summaries."
            ]
        });

        // Q38 - RAG with citation and source tracking
        questions.push({
            number: 38,
            title: "How do you implement proper citation and source tracking in RAG systems?",
            description:
                "Citations are crucial for RAG systems to provide transparency and allow users to verify information. " +
                "Source tracking involves identifying which retrieved documents contributed to the answer, " +
                "extracting relevant passages, and presenting citations in a user-friendly format.",
            why:
                "Users need to verify information and understand where answers come from. Citations build trust, " +
                "enable fact-checking, and are required for many professional and academic applications.",
            what:
                "Citation Components: Source identification (which documents), Passage extraction (relevant quotes), " +
                "Metadata (page numbers, sections, dates), Citation format (APA, MLA, custom), " +
                "In-text citations, Reference list. " +
                "Implementation: Track sources during retrieval, Extract relevant passages, " +
                "Generate citations, Format for display.",
            how:
                "1) Store metadata with retrieved documents, 2) Track which documents are used, " +
                "3) Extract relevant passages from sources, 4) Generate citations with metadata, " +
                "5) Include citations in answer, 6) Format citations appropriately, " +
                "7) Provide reference list.",
            pros: [
                "Transparency and trust",
                "Enables fact-checking",
                "Professional presentation",
                "Required for many use cases",
                "Better user experience",
                "Legal and compliance support"
            ],
            cons: [
                "Implementation complexity",
                "Requires good metadata",
                "Citation formatting overhead",
                "May clutter answers",
                "Need to maintain citation standards",
                "Additional processing"
            ],
            diagram: `flowchart TD
    A["Retrieved Documents"] --> B["Extract Metadata"]
    B --> C["Source 1: Page 5"]
    B --> D["Source 2: Section 3.2"]
    B --> E["Source 3: Document v2.1"]
    C --> F["Generate Citations"]
    D --> F
    E --> F
    F --> G["Format Citations"]
    G --> H["Include in Answer"]
    H --> I["Reference List"]`,
            implementation: `# RAG with Citations
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class Citation:
    source: str
    page: Optional[int] = None
    section: Optional[str] = None
    passage: Optional[str] = None
    metadata: dict = None

class RAGWithCitations:
    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
    
    def query_with_citations(self, query, k=5):
        # Retrieve documents with metadata
        docs = vectorstore.similarity_search_with_score(query, k=k)
        
        # Extract citations
        citations = []
        context_parts = []
        
        for i, (doc, score) in enumerate(docs):
            # Extract relevant passage
            passage = self.extract_relevant_passage(doc.page_content, query)
            
            # Create citation
            citation = Citation(
                source=doc.metadata.get("source", f"Document {i+1}"),
                page=doc.metadata.get("page"),
                section=doc.metadata.get("section"),
                passage=passage,
                metadata=doc.metadata
            )
            citations.append(citation)
            
            # Build context with citation markers
            context_parts.append(f"[{i+1}] {doc.page_content}")
        
        context = "\\n\\n".join(context_parts)
        
        # Generate answer with citation instructions
        prompt = f"""Answer the question using the following sources. Include citations like [1], [2] when referencing information.

Sources:
{context}

Question: {query}

Instructions:
- Use [1], [2], etc. to cite sources
- Be accurate and cite appropriately
- Include all relevant information

Answer:"""
        
        answer = self.llm(prompt)
        
        return {
            "answer": answer,
            "citations": citations,
            "sources": docs
        }
    
    def extract_relevant_passage(self, text, query, max_length=200):
        # Find most relevant sentence or paragraph
        sentences = text.split('. ')
        query_words = set(query.lower().split())
        
        best_sentence = ""
        best_score = 0
        
        for sentence in sentences:
            sentence_words = set(sentence.lower().split())
            overlap = len(query_words & sentence_words)
            if overlap > best_score:
                best_score = overlap
                best_sentence = sentence
        
        # Return passage around best sentence
        if best_sentence:
            idx = text.find(best_sentence)
            start = max(0, idx - 50)
            end = min(len(text), idx + len(best_sentence) + 50)
            return text[start:end]
        
        return text[:max_length]
    
    def format_citations_apa(self, citations):
        """Format citations in APA style"""
        formatted = []
        for i, cit in enumerate(citations, 1):
            if cit.page:
                formatted.append(f"[{i}] {cit.source}, p. {cit.page}")
            else:
                formatted.append(f"[{i}] {cit.source}")
        return formatted
    
    def format_citations_mla(self, citations):
        """Format citations in MLA style"""
        formatted = []
        for i, cit in enumerate(citations, 1):
            if cit.page:
                formatted.append(f"[{i}] {cit.source}. Page {cit.page}.")
            else:
                formatted.append(f"[{i}] {cit.source}.")
        return formatted

# Advanced: Citation extraction from answer
def extract_citations_from_answer(answer):
    """Extract citation numbers from answer"""
    import re
    citations = re.findall(r'\\[(\\d+)\\]', answer)
    return [int(c) for c in citations]

# Usage
rag = RAGWithCitations(vectorstore, llm)
result = rag.query_with_citations("What is RAG?")

# Format citations
apa_citations = rag.format_citations_apa(result["citations"])

# Display
print("Answer:", result["answer"])
print("\\nReferences:")
for cit in apa_citations:
    print(cit)

# Enhanced: Track which parts of answer come from which source
class EnhancedRAGWithCitations:
    def query_with_detailed_citations(self, query):
        result = self.query_with_citations(query)
        
        # Map answer sentences to sources
        answer_sentences = result["answer"].split('. ')
        source_mapping = []
        
        for sentence in answer_sentences:
            # Find which citation numbers are in this sentence
            citations_in_sentence = extract_citations_from_answer(sentence)
            
            if citations_in_sentence:
                source_mapping.append({
                    "sentence": sentence,
                    "sources": [result["citations"][c-1] for c in citations_in_sentence]
                })
        
        result["source_mapping"] = source_mapping
        return result`,
            approaches: [
                "Store comprehensive metadata with documents.",
                "Track which documents contribute to answers.",
                "Extract relevant passages for citations.",
                "Use consistent citation format (APA, MLA, etc.).",
                "Include in-text citations in answers.",
                "Provide reference list at the end.",
                "Make citations clickable when possible.",
                "Show source passages on demand.",
                "Validate citations are accurate.",
                "Support multiple citation styles."
            ]
        });

        // Q39 - RAG with query understanding and intent
        questions.push({
            number: 39,
            title: "How do you improve RAG retrieval through query understanding and intent classification?",
            description:
                "Query understanding analyzes user queries to extract intent, entities, and context before retrieval. " +
                "This enables better query rewriting, intent-based routing, and specialized retrieval strategies. " +
                "Understanding queries improves retrieval accuracy and answer quality.",
            why:
                "Different queries require different retrieval strategies. Understanding query intent (factual, analytical, procedural) " +
                "and extracting entities enables optimized retrieval, better query rewriting, and specialized handling.",
            what:
                "Query Understanding Components: Intent classification (what type of question), " +
                "Entity extraction (key entities mentioned), Query type (factual, how-to, comparison, etc.), " +
                "Complexity analysis (simple vs complex), Domain detection, Query rewriting based on understanding.",
            how:
                "1) Classify query intent, 2) Extract entities and key terms, " +
                "3) Determine query type and complexity, 4) Rewrite query based on understanding, " +
                "5) Choose retrieval strategy, 6) Execute optimized retrieval, " +
                "7) Generate answer with context awareness.",
            pros: [
                "Better retrieval accuracy",
                "Optimized query rewriting",
                "Intent-based routing",
                "Specialized handling",
                "Improved answer quality",
                "Better user experience"
            ],
            cons: [
                "Additional processing",
                "Requires intent classification",
                "More complex implementation",
                "Need training data for intents",
                "Higher latency",
                "May misclassify queries"
            ],
            diagram: `flowchart TD
    A["User Query"] --> B["Query Understanding"]
    B --> C["Intent Classification"]
    B --> D["Entity Extraction"]
    B --> E["Query Type Detection"]
    C --> F["Query Rewriting"]
    D --> F
    E --> F
    F --> G["Optimized Retrieval"]
    G --> H["Context-Aware Answer"]`,
            implementation: `# Query Understanding for RAG
from enum import Enum
import re

class QueryIntent(Enum):
    FACTUAL = "factual"
    HOW_TO = "how_to"
    COMPARISON = "comparison"
    ANALYSIS = "analysis"
    DEFINITION = "definition"
    PROCEDURAL = "procedural"

class QueryUnderstanding:
    def __init__(self, llm):
        self.llm = llm
    
    def understand_query(self, query):
        # 1. Classify intent
        intent = self.classify_intent(query)
        
        # 2. Extract entities
        entities = self.extract_entities(query)
        
        # 3. Determine query type
        query_type = self.determine_query_type(query)
        
        # 4. Analyze complexity
        complexity = self.analyze_complexity(query)
        
        return {
            "intent": intent,
            "entities": entities,
            "type": query_type,
            "complexity": complexity,
            "original_query": query
        }
    
    def classify_intent(self, query):
        prompt = f"""Classify the intent of this query into one of: factual, how_to, comparison, analysis, definition, procedural.
        
        Query: {query}
        
        Intent:"""
        
        response = self.llm(prompt).strip().lower()
        
        # Map to enum
        intent_map = {
            "factual": QueryIntent.FACTUAL,
            "how_to": QueryIntent.HOW_TO,
            "how to": QueryIntent.HOW_TO,
            "comparison": QueryIntent.COMPARISON,
            "analysis": QueryIntent.ANALYSIS,
            "definition": QueryIntent.DEFINITION,
            "procedural": QueryIntent.PROCEDURAL
        }
        
        return intent_map.get(response, QueryIntent.FACTUAL)
    
    def extract_entities(self, query):
        # Use NER or LLM
        prompt = f"""Extract key entities (people, places, concepts, products) from this query:
        
        Query: {query}
        
        Entities (comma-separated):"""
        
        response = self.llm(prompt)
        entities = [e.strip() for e in response.split(',')]
        return entities
    
    def determine_query_type(self, query):
        # Simple heuristics + LLM
        if any(word in query.lower() for word in ['what is', 'what are', 'define']):
            return "definition"
        elif any(word in query.lower() for word in ['how', 'steps', 'process']):
            return "how_to"
        elif any(word in query.lower() for word in ['compare', 'difference', 'vs', 'versus']):
            return "comparison"
        elif any(word in query.lower() for word in ['why', 'analyze', 'explain']):
            return "analysis"
        else:
            return "factual"
    
    def analyze_complexity(self, query):
        # Simple complexity analysis
        word_count = len(query.split())
        has_multiple_questions = query.count('?') > 1
        has_conjunctions = any(word in query.lower() for word in ['and', 'or', 'but', 'also'])
        
        if word_count > 20 or has_multiple_questions or has_conjunctions:
            return "complex"
        return "simple"

# Query rewriting based on understanding
class IntentBasedRAG:
    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
        self.query_understanding = QueryUnderstanding(llm)
    
    def rewrite_query(self, query, understanding):
        intent = understanding["intent"]
        entities = understanding["entities"]
        
        if intent == QueryIntent.DEFINITION:
            # For definitions, add context
            rewritten = f"definition explanation of {query}"
        elif intent == QueryIntent.HOW_TO:
            # For how-to, emphasize steps
            rewritten = f"step by step guide {query} procedure instructions"
        elif intent == QueryIntent.COMPARISON:
            # For comparison, extract comparison terms
            rewritten = f"comparison difference between {query}"
        elif intent == QueryIntent.ANALYSIS:
            # For analysis, add analytical terms
            rewritten = f"analysis explanation reasons {query}"
        else:
            rewritten = query
        
        # Add entities
        if entities:
            rewritten += " " + " ".join(entities)
        
        return rewritten
    
    def retrieve_with_understanding(self, query):
        # Understand query
        understanding = self.query_understanding.understand_query(query)
        
        # Rewrite based on understanding
        rewritten_query = self.rewrite_query(query, understanding)
        
        # Adjust retrieval based on intent
        if understanding["intent"] == QueryIntent.COMPARISON:
            # For comparisons, retrieve more documents
            k = 10
        elif understanding["intent"] == QueryIntent.ANALYSIS:
            # For analysis, prioritize detailed documents
            k = 8
        else:
            k = 5
        
        # Retrieve
        docs = self.vectorstore.similarity_search(rewritten_query, k=k)
        
        return {
            "documents": docs,
            "understanding": understanding,
            "rewritten_query": rewritten_query
        }
    
    def query(self, user_query):
        # Retrieve with understanding
        retrieval_result = self.retrieve_with_understanding(user_query)
        
        # Build context-aware prompt
        understanding = retrieval_result["understanding"]
        context = "\\n".join([d.page_content for d in retrieval_result["documents"]])
        
        # Customize prompt based on intent
        if understanding["intent"] == QueryIntent.DEFINITION:
            prompt = f"""Provide a clear definition based on the context:
            
            Context: {context}
            
            Query: {user_query}
            
            Definition:"""
        elif understanding["intent"] == QueryIntent.HOW_TO:
            prompt = f"""Provide step-by-step instructions based on the context:
            
            Context: {context}
            
            Query: {user_query}
            
            Instructions:"""
        else:
            prompt = f"""Answer the question based on the context:
            
            Context: {context}
            
            Question: {user_query}
            
            Answer:"""
        
        answer = self.llm(prompt)
        
        return {
            "answer": answer,
            "sources": retrieval_result["documents"],
            "intent": understanding["intent"].value,
            "entities": understanding["entities"]
        }`,
            approaches: [
                "Classify query intent before retrieval.",
                "Extract entities to improve search.",
                "Rewrite queries based on intent.",
                "Adjust retrieval strategy by intent type.",
                "Use specialized prompts for different intents.",
                "Handle complex queries differently.",
                "Test intent classification accuracy.",
                "Monitor and improve understanding over time.",
                "Combine rule-based and LLM-based understanding.",
                "Consider user context in understanding."
            ]
        });

        // Q40 - RAG system scalability
        questions.push({
            number: 40,
            title: "How do you design RAG systems for scale (millions of documents, high QPS)?",
            description:
                "Scaling RAG systems to handle millions of documents and high query-per-second (QPS) requires careful architecture. " +
                "This involves distributed vector stores, caching strategies, load balancing, " +
                "sharding, and efficient indexing. Scalability is critical for production deployments.",
            why:
                "Production RAG systems must handle large document collections and high traffic. " +
                "Poor scalability leads to slow responses, high costs, and system failures. Proper design enables growth.",
            what:
                "Scalability Challenges: Large document collections, High query volume, Vector search performance, " +
                "Embedding generation costs, Storage requirements, Network bandwidth. " +
                "Solutions: Distributed vector stores, Sharding strategies, Caching layers, " +
                "Load balancing, Efficient indexing, Batch processing, CDN for static content.",
            how:
                "1) Design distributed architecture, 2) Implement sharding for vector store, " +
                "3) Add multi-level caching, 4) Use load balancers, " +
                "5) Optimize indexing and search, 6) Implement batch processing, " +
                "7) Monitor and auto-scale, 8) Optimize costs at scale.",
            pros: [
                "Handles large document collections",
                "Supports high query volumes",
                "Better performance",
                "Cost-effective at scale",
                "Reliable under load",
                "Future-proof architecture"
            ],
            cons: [
                "More complex architecture",
                "Higher infrastructure costs",
                "Requires distributed systems expertise",
                "More moving parts",
                "Challenging to debug",
                "Initial setup overhead"
            ],
            diagram: `flowchart TD
    A["User Queries"] --> B["Load Balancer"]
    B --> C["RAG Service Cluster"]
    C --> D["Cache Layer"]
    D --> E["Sharded Vector Store"]
    E --> F["Shard 1"]
    E --> G["Shard 2"]
    E --> H["Shard N"]
    F --> I["Distributed Search"]
    G --> I
    H --> I
    I --> J["Result Aggregation"]
    J --> K["Response"]`,
            implementation: `# Scalable RAG Architecture
from typing import List
import asyncio
from concurrent.futures import ThreadPoolExecutor

# 1. Sharded vector store
class ShardedVectorStore:
    def __init__(self, shards: List):
        self.shards = shards  # List of vector store instances
    
    def add_documents(self, documents):
        # Shard documents (e.g., by hash of document ID)
        for doc in documents:
            shard_index = hash(doc.metadata.get("id", "")) % len(self.shards)
            self.shards[shard_index].add_documents([doc])
    
    async def search_all_shards(self, query, k_per_shard=5):
        # Search all shards in parallel
        tasks = [
            asyncio.to_thread(shard.similarity_search, query, k=k_per_shard)
            for shard in self.shards
        ]
        results = await asyncio.gather(*tasks)
        
        # Combine and rerank
        all_results = []
        for shard_results in results:
            all_results.extend(shard_results)
        
        # Rerank globally
        reranked = rerank_results(query, all_results, top_k=k_per_shard)
        return reranked

# 2. Distributed RAG service
class DistributedRAGService:
    def __init__(self, sharded_store, cache, llm_pool):
        self.sharded_store = sharded_store
        self.cache = cache
        self.llm_pool = llm_pool  # Pool of LLM instances
    
    async def query(self, query, user_id=None):
        # Check cache
        cache_key = f"rag:{hash(query)}"
        cached = await self.cache.get(cache_key)
        if cached:
            return cached
        
        # Search sharded store
        docs = await self.sharded_store.search_all_shards(query, k_per_shard=3)
        
        # Get LLM from pool
        llm = self.llm_pool.get_llm()
        
        # Generate answer
        answer = await llm.agenerate(context=docs, question=query)
        
        result = {
            "answer": answer,
            "sources": docs
        }
        
        # Cache result
        await self.cache.set(cache_key, result, ttl=3600)
        
        return result

# 3. Load balancing
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

app = FastAPI()

# Multiple RAG service instances
rag_services = [
    DistributedRAGService(sharded_store, cache, llm_pool),
    DistributedRAGService(sharded_store, cache, llm_pool),
    DistributedRAGService(sharded_store, cache, llm_pool)
]

service_index = 0

def get_next_service():
    global service_index
    service = rag_services[service_index]
    service_index = (service_index + 1) % len(rag_services)
    return service

@app.post("/query")
async def query_endpoint(query: str):
    service = get_next_service()  # Round-robin
    result = await service.query(query)
    return result

# 4. Batch processing for embeddings
class BatchEmbeddingProcessor:
    def __init__(self, embedding_model, batch_size=100):
        self.embedding_model = embedding_model
        self.batch_size = batch_size
    
    async def process_documents(self, documents):
        # Process in batches
        all_embeddings = []
        
        for i in range(0, len(documents), self.batch_size):
            batch = documents[i:i+self.batch_size]
            embeddings = await self.embedding_model.aembed_documents(
                [d.page_content for d in batch]
            )
            all_embeddings.extend(embeddings)
        
        return all_embeddings

# 5. Efficient indexing
class EfficientIndexer:
    def __init__(self, vectorstore):
        self.vectorstore = vectorstore
        self.indexed_count = 0
    
    async def index_documents(self, documents, batch_size=1000):
        # Chunk documents
        all_chunks = []
        for doc in documents:
            chunks = chunk_document(doc)
            all_chunks.extend(chunks)
        
        # Process in batches
        processor = BatchEmbeddingProcessor(embedding_model, batch_size)
        embeddings = await processor.process_documents(all_chunks)
        
        # Add to vector store in batches
        for i in range(0, len(all_chunks), batch_size):
            batch_chunks = all_chunks[i:i+batch_size]
            batch_embeddings = embeddings[i:i+batch_size]
            
            self.vectorstore.add_texts(
                texts=[c.page_content for c in batch_chunks],
                embeddings=batch_embeddings,
                metadatas=[c.metadata for c in batch_chunks]
            )
            
            self.indexed_count += len(batch_chunks)

# 6. Auto-scaling configuration
# Kubernetes HPA example
"""
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: rag-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: rag-service
  minReplicas: 3
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
"""

# 7. Monitoring at scale
class ScalabilityMonitor:
    def __init__(self):
        self.metrics = {
            "queries_per_second": 0,
            "average_latency": 0,
            "cache_hit_rate": 0,
            "shard_utilization": {}
        }
    
    def track_query(self, latency, cached):
        self.metrics["queries_per_second"] += 1
        # Update average latency
        # Update cache hit rate
        pass
    
    def get_scaling_recommendations(self):
        if self.metrics["average_latency"] > 3.0:
            return "Consider adding more RAG service instances"
        if self.metrics["cache_hit_rate"] < 0.5:
            return "Consider increasing cache size or TTL"
        return "System performing well"`,
            approaches: [
                "Design for horizontal scaling from the start.",
                "Implement sharding for large document collections.",
                "Use distributed vector stores (Pinecone, Weaviate cluster).",
                "Add multi-level caching (memory, Redis, CDN).",
                "Implement load balancing for query distribution.",
                "Use batch processing for embeddings.",
                "Monitor and auto-scale based on metrics.",
                "Optimize indexing for large datasets.",
                "Consider read replicas for vector stores.",
                "Plan for cost optimization at scale."
            ]
        });

        // Q41 - RAG with structured output
        questions.push({
            number: 41,
            title: "How do you ensure RAG systems return structured, parseable outputs?",
            description:
                "Structured outputs from RAG systems enable integration with downstream systems, data extraction, " +
                "and programmatic processing. This involves using JSON schemas, Pydantic models, or function calling " +
                "to ensure consistent, validated output formats.",
            why:
                "Unstructured text outputs are hard to integrate with other systems. Structured outputs enable " +
                "automation, data extraction, API integration, and consistent formatting across use cases.",
            what:
                "Structured Output Types: JSON responses, Pydantic models, XML, CSV, Tables, Function calling responses. " +
                "Implementation: Schema definition, Output validation, Format enforcement, Error handling, " +
                "Type conversion, Schema evolution.",
            how:
                "1) Define output schema, 2) Use structured output libraries, " +
                "3) Configure LLM for structured output, 4) Validate outputs, " +
                "5) Handle parsing errors, 6) Transform to desired format, " +
                "7) Return structured response.",
            pros: [
                "Easy integration",
                "Consistent format",
                "Type safety",
                "Validation built-in",
                "Better error handling",
                "Programmatic processing"
            ],
            cons: [
                "Less flexible",
                "Requires schema definition",
                "May need retries on invalid output",
                "Schema changes require updates",
                "More complex setup",
                "LLM may struggle with strict formats"
            ],
            diagram: `flowchart TD
    A["RAG Query"] --> B["Retrieve Context"]
    B --> C["LLM Generation"]
    C --> D["Structured Output"]
    D --> E["Schema Validation"]
    E -->|Valid| F["Return JSON"]
    E -->|Invalid| G["Retry/Reparse"]
    G --> C
    F --> H["Downstream System"]`,
            implementation: `# RAG with Structured Output
from pydantic import BaseModel, Field
from typing import List, Optional
import json

# 1. Define output schema
class RAGResponse(BaseModel):
    answer: str = Field(description="The answer to the question")
    sources: List[str] = Field(description="List of source document IDs")
    confidence: float = Field(description="Confidence score 0-1", ge=0, le=1)
    entities: Optional[List[str]] = Field(description="Key entities mentioned", default=None)
    citations: Optional[List[dict]] = Field(description="Citation details", default=None)

# 2. Structured RAG with Pydantic
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate

class StructuredRAG:
    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
        self.parser = PydanticOutputParser(pydantic_object=RAGResponse)
    
    def query_structured(self, query):
        # Retrieve context
        docs = self.vectorstore.similarity_search(query, k=5)
        context = "\\n".join([d.page_content for d in docs])
        
        # Create prompt with format instructions
        prompt = PromptTemplate(
            template="""Answer the question based on the context. Format your response as JSON matching this schema:
            
            {format_instructions}
            
            Context:
            {context}
            
            Question: {query}
            
            Response:""",
            input_variables=["context", "query"],
            partial_variables={"format_instructions": self.parser.get_format_instructions()}
        )
        
        # Generate
        response = self.llm(prompt.format(context=context, query=query))
        
        # Parse and validate
        try:
            parsed = self.parser.parse(response)
            return parsed
        except Exception as e:
            # Retry with error correction
            return self._retry_with_correction(query, context, response, str(e))
    
    def _retry_with_correction(self, query, context, previous_response, error):
        correction_prompt = f"""Previous response had an error: {error}
        
        Previous response: {previous_response}
        
        Please correct it to match the schema:
        {self.parser.get_format_instructions()}
        
        Context: {context}
        Question: {query}
        
        Corrected response:"""
        
        corrected = self.llm(correction_prompt)
        return self.parser.parse(corrected)

# 3. OpenAI structured output
from openai import OpenAI

client = OpenAI()

def rag_with_openai_structured(query, context):
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant that returns structured JSON."},
            {"role": "user", "content": f"Context: {context}\\n\\nQuestion: {query}"}
        ],
        response_format={
            "type": "json_schema",
            "json_schema": {
                "name": "rag_response",
                "schema": {
                    "type": "object",
                    "properties": {
                        "answer": {"type": "string"},
                        "sources": {"type": "array", "items": {"type": "string"}},
                        "confidence": {"type": "number", "minimum": 0, "maximum": 1}
                    },
                    "required": ["answer", "sources", "confidence"]
                }
            }
        }
    )
    
    return json.loads(response.choices[0].message.content)

# 4. LangChain with structured output
from langchain.chains import LLMChain
from langchain.output_parsers import StructuredOutputParser, ResponseSchema

response_schemas = [
    ResponseSchema(name="answer", description="The answer"),
    ResponseSchema(name="sources", description="Source IDs", type="List[str]"),
    ResponseSchema(name="confidence", description="Confidence 0-1", type="float")
]

output_parser = StructuredOutputParser.from_response_schemas(response_schemas)

def structured_rag_langchain(query, context):
    prompt = PromptTemplate(
        template="""Answer based on context. Format: {format_instructions}
        
        Context: {context}
        Question: {query}
        Response:""",
        input_variables=["context", "query"],
        partial_variables={"format_instructions": output_parser.get_format_instructions()}
    )
    
    chain = LLMChain(llm=llm, prompt=prompt, output_parser=output_parser)
    result = chain.run(context=context, query=query)
    return result

# 5. Table output
class TableRAGResponse(BaseModel):
    answer: str
    data_table: List[dict] = Field(description="Table data as list of dicts")
    columns: List[str] = Field(description="Column names")

def rag_with_table_output(query, context):
    # Generate table-structured response
    prompt = f"""Extract information as a table from the context.
    
    Context: {context}
    Question: {query}
    
    Return JSON with:
    - answer: summary text
    - columns: list of column names
    - data_table: list of row objects
    
    Response:"""
    
    response = llm(prompt)
    return json.loads(response)

# 6. Multi-format support
class MultiFormatRAG:
    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
    
    def query(self, query, output_format="json"):
        docs = self.vectorstore.similarity_search(query, k=5)
        context = "\\n".join([d.page_content for d in docs])
        
        if output_format == "json":
            return self._json_output(query, context)
        elif output_format == "xml":
            return self._xml_output(query, context)
        elif output_format == "pydantic":
            return self._pydantic_output(query, context)
        else:
            return self._text_output(query, context)
    
    def _json_output(self, query, context):
        prompt = f"""Return JSON: {{"answer": "...", "sources": [...]}}
        
        Context: {context}
        Question: {query}"""
        response = self.llm(prompt)
        return json.loads(response)
    
    def _pydantic_output(self, query, context):
        parser = PydanticOutputParser(pydantic_object=RAGResponse)
        # ... implementation
        pass`,
            approaches: [
                "Define clear output schemas upfront.",
                "Use Pydantic for validation and type safety.",
                "Leverage LLM structured output features.",
                "Implement retry logic for invalid outputs.",
                "Validate outputs before returning.",
                "Support multiple output formats.",
                "Handle schema evolution gracefully.",
                "Provide clear error messages.",
                "Test with various query types.",
                "Monitor output validation success rate."
            ]
        });

        // Q42 - RAG with multi-turn conversations
        questions.push({
            number: 42,
            title: "How do you handle multi-turn conversations in RAG systems?",
            description:
                "Multi-turn conversations require maintaining context across multiple queries, handling follow-up questions, " +
                "and understanding references to previous messages. This involves conversation memory, context management, " +
                "and query rewriting to handle implicit references.",
            why:
                "Users ask follow-up questions and refer to previous context. Without conversation memory, " +
                "each query is treated independently, leading to poor user experience and incorrect answers.",
            what:
                "Conversation Components: Message history, Context window management, Query rewriting, " +
                "Reference resolution (pronouns, 'that', 'it'), Follow-up detection, Context summarization. " +
                "Memory Types: Full history, Summarized history, Sliding window, Token-based truncation.",
            how:
                "1) Store conversation history, 2) Detect follow-up questions, " +
                "3) Rewrite queries with context, 4) Manage context window size, " +
                "5) Retrieve relevant documents, 6) Generate answer with conversation context, " +
                "7) Update conversation memory.",
            pros: [
                "Better user experience",
                "Handles follow-ups naturally",
                "Maintains context",
                "More conversational",
                "Handles references",
                "Feels more intelligent"
            ],
            cons: [
                "More complex implementation",
                "Context window management",
                "Higher token usage",
                "May accumulate errors",
                "Requires memory storage",
                "Slower responses"
            ],
            diagram: `flowchart TD
    A["User Query 1"] --> B["RAG Response 1"]
    B --> C["Store in Memory"]
    C --> D["User Query 2"]
    D --> E["Load History"]
    E --> F["Rewrite Query"]
    F --> G["Retrieve + Generate"]
    G --> H["Response 2"]
    H --> C`,
            implementation: `# Multi-turn RAG Conversations
from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory
from langchain.chains import ConversationalRetrievalChain
from typing import List, Dict

# 1. Basic conversation memory
class ConversationalRAG:
    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
    
    def chat(self, query, session_id=None):
        # Get conversation history
        history = self.memory.chat_memory.messages
        
        # Rewrite query with context
        rewritten_query = self._rewrite_query_with_context(query, history)
        
        # Retrieve documents
        docs = self.vectorstore.similarity_search(rewritten_query, k=5)
        context = "\\n".join([d.page_content for d in docs])
        
        # Build prompt with history
        messages = []
        for msg in history[-6:]:  # Last 6 messages
            messages.append(msg)
        
        messages.append({
            "role": "user",
            "content": f"Context: {context}\\n\\nQuestion: {query}"
        })
        
        # Generate
        response = self.llm(messages)
        
        # Update memory
        self.memory.chat_memory.add_user_message(query)
        self.memory.chat_memory.add_ai_message(response)
        
        return response
    
    def _rewrite_query_with_context(self, query, history):
        if not history:
            return query
        
        # Detect if follow-up
        follow_up_indicators = ["it", "that", "this", "they", "them", "what about", "how about"]
        is_followup = any(indicator in query.lower() for indicator in follow_up_indicators)
        
        if is_followup:
            # Rewrite with context
            recent_context = "\\n".join([msg.content for msg in history[-4:]])
            rewrite_prompt = f"""Rewrite this follow-up question to be standalone, using context from previous conversation:
            
            Previous conversation:
            {recent_context}
            
            Follow-up question: {query}
            
            Standalone question:"""
            
            rewritten = self.llm(rewrite_prompt)
            return rewritten
        
        return query

# 2. LangChain ConversationalRetrievalChain
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain

def create_conversational_rag(vectorstore, llm):
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )
    
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        memory=memory,
        verbose=True
    )
    
    return chain

# Usage
chain = create_conversational_rag(vectorstore, llm)
result = chain({"question": "What is RAG?"})
result = chain({"question": "How does it work?"})  # Follow-up

# 3. Conversation summarization for long contexts
class SummarizedConversationalRAG:
    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
        self.memory = ConversationSummaryMemory(
            llm=llm,
            memory_key="chat_history",
            return_messages=True
        )
        self.full_history = []  # Store full messages
    
    def chat(self, query):
        # Summarize old messages if needed
        if len(self.full_history) > 10:
            summary = self.memory.predict_new_summary(
                self.full_history[-10:],
                ""
            )
            self.memory.buffer = summary
        
        # Get current context
        history_summary = self.memory.buffer
        
        # Retrieve
        docs = self.vectorstore.similarity_search(query, k=5)
        context = "\\n".join([d.page_content for d in docs])
        
        # Generate with summarized history
        prompt = f"""Previous conversation summary: {history_summary}
        
        Retrieved context: {context}
        
        Current question: {query}
        
        Answer:"""
        
        response = self.llm(prompt)
        
        # Update memory
        self.full_history.append({"role": "user", "content": query})
        self.full_history.append({"role": "assistant", "content": response})
        
        return response

# 4. Session-based conversations
class SessionBasedRAG:
    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
        self.sessions = {}  # session_id -> memory
    
    def get_or_create_session(self, session_id):
        if session_id not in self.sessions:
            self.sessions[session_id] = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True
            )
        return self.sessions[session_id]
    
    def chat(self, query, session_id):
        memory = self.get_or_create_session(session_id)
        
        # Get history
        history = memory.chat_memory.messages
        
        # Rewrite query
        rewritten = self._rewrite_with_history(query, history)
        
        # Retrieve and generate
        docs = self.vectorstore.similarity_search(rewritten, k=5)
        context = "\\n".join([d.page_content for d in docs])
        
        messages = list(history[-6:]) + [
            {"role": "user", "content": f"Context: {context}\\n\\nQuestion: {query}"}
        ]
        
        response = self.llm(messages)
        
        # Update memory
        memory.chat_memory.add_user_message(query)
        memory.chat_memory.add_ai_message(response)
        
        return response
    
    def _rewrite_with_history(self, query, history):
        if not history:
            return query
        
        # Check for references
        context = "\\n".join([msg.content for msg in history[-4:]])
        
        rewrite_prompt = f"""Rewrite this question to be clear, using context from conversation:
        
        Conversation:
        {context}
        
        Question: {query}
        
        Clear, standalone question:"""
        
        return self.llm(rewrite_prompt)

# 5. Advanced: Context-aware retrieval
class ContextAwareConversationalRAG:
    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
        self.memory = ConversationBufferMemory()
    
    def chat(self, query):
        history = self.memory.chat_memory.messages
        
        # Build query with conversation context
        conversation_context = self._extract_conversation_context(history)
        
        # Enhanced query for retrieval
        retrieval_query = f"{query} {conversation_context}"
        
        # Retrieve
        docs = self.vectorstore.similarity_search(retrieval_query, k=5)
        
        # Generate with full context
        response = self._generate_with_context(query, docs, history)
        
        # Update memory
        self.memory.chat_memory.add_user_message(query)
        self.memory.chat_memory.add_ai_message(response)
        
        return response
    
    def _extract_conversation_context(self, history):
        # Extract key entities and topics from conversation
        if not history:
            return ""
        
        recent_messages = "\\n".join([msg.content for msg in history[-6:]])
        extract_prompt = f"""Extract key topics and entities from this conversation:
        
        {recent_messages}
        
        Key topics (comma-separated):"""
        
        topics = self.llm(extract_prompt)
        return topics`,
            approaches: [
                "Maintain conversation history per session.",
                "Detect and handle follow-up questions.",
                "Rewrite queries with conversation context.",
                "Use conversation summarization for long histories.",
                "Manage context window size carefully.",
                "Handle pronoun and reference resolution.",
                "Implement session management.",
                "Clear memory when context becomes irrelevant.",
                "Test with multi-turn scenarios.",
                "Monitor conversation quality and context usage."
            ]
        });

        // Q43 - RAG with document versioning
        questions.push({
            number: 43,
            title: "How do you handle document versioning and updates in RAG systems?",
            description:
                "Documents change over time. RAG systems need to handle versioning, updates, deletions, " +
                "and ensure users get information from the correct document version. This involves version tracking, " +
                "incremental updates, and handling stale information.",
            why:
                "Documents are updated, corrected, or replaced. Without versioning, RAG systems may return outdated " +
                "or incorrect information. Versioning ensures accuracy and traceability.",
            what:
                "Versioning Components: Document versions, Timestamps, Update tracking, Incremental indexing, " +
                "Version-aware retrieval, Stale data detection, Rollback capabilities. " +
                "Update Strategies: Full reindexing, Incremental updates, Delta updates, Event-driven updates.",
            how:
                "1) Store document metadata with versions, 2) Track document changes, " +
                "3) Update vector store incrementally, 4) Version-aware retrieval, " +
                "5) Handle document deletions, 6) Maintain version history, " +
                "7) Support querying specific versions.",
            pros: [
                "Accurate information",
                "Traceability",
                "Handles updates efficiently",
                "Supports audit trails",
                "Can query historical versions",
                "Better data governance"
            ],
            cons: [
                "More complex storage",
                "Requires version management",
                "Storage overhead",
                "Update complexity",
                "Need to handle conflicts",
                "More metadata to track"
            ],
            diagram: `flowchart TD
    A["Document Update"] --> B["Version Check"]
    B --> C["Create New Version"]
    C --> D["Update Metadata"]
    D --> E["Incremental Indexing"]
    E --> F["Vector Store Update"]
    F --> G["Version-Aware Retrieval"]
    G --> H["Return Latest/Requested Version"]`,
            implementation: `# RAG with Document Versioning
from datetime import datetime
from typing import Optional
import hashlib

class DocumentVersion:
    def __init__(self, doc_id, content, version, timestamp, metadata=None):
        self.doc_id = doc_id
        self.content = content
        self.version = version
        self.timestamp = timestamp
        self.metadata = metadata or {}
        self.content_hash = hashlib.md5(content.encode()).hexdigest()

class VersionedRAG:
    def __init__(self, vectorstore):
        self.vectorstore = vectorstore
        self.versions = {}  # doc_id -> list of versions
        self.current_versions = {}  # doc_id -> current version
    
    def add_document(self, doc_id, content, metadata=None):
        # Check if document exists
        if doc_id in self.current_versions:
            current_version = self.current_versions[doc_id]
            new_version = current_version + 1
        else:
            new_version = 1
        
        # Create version
        version = DocumentVersion(
            doc_id=doc_id,
            content=content,
            version=new_version,
            timestamp=datetime.now(),
            metadata=metadata
        )
        
        # Store version
        if doc_id not in self.versions:
            self.versions[doc_id] = []
        self.versions[doc_id].append(version)
        self.current_versions[doc_id] = new_version
        
        # Update vector store (remove old, add new)
        self._update_vector_store(doc_id, version)
        
        return version
    
    def _update_vector_store(self, doc_id, new_version):
        # Remove old version embeddings
        # (Implementation depends on vector store - may need to filter by metadata)
        
        # Add new version
        self.vectorstore.add_texts(
            texts=[new_version.content],
            metadatas=[{
                "doc_id": doc_id,
                "version": new_version.version,
                "timestamp": new_version.timestamp.isoformat(),
                **new_version.metadata
            }]
        )
    
    def query(self, query, version=None, use_latest=True):
        # Retrieve with version filter
        if use_latest:
            # Only retrieve latest versions
            docs = self.vectorstore.similarity_search(
                query,
                k=5,
                filter={"version": self._get_latest_versions()}
            )
        elif version:
            # Retrieve specific version
            docs = self.vectorstore.similarity_search(
                query,
                k=5,
                filter={"version": version}
            )
        else:
            # Retrieve all versions (may get duplicates)
            docs = self.vectorstore.similarity_search(query, k=5)
            # Deduplicate by keeping latest
            docs = self._deduplicate_by_version(docs)
        
        return docs
    
    def _get_latest_versions(self):
        return list(self.current_versions.values())
    
    def _deduplicate_by_version(self, docs):
        # Group by doc_id, keep latest version
        doc_map = {}
        for doc in docs:
            doc_id = doc.metadata.get("doc_id")
            version = doc.metadata.get("version", 0)
            
            if doc_id not in doc_map or version > doc_map[doc_id].metadata.get("version", 0):
                doc_map[doc_id] = doc
        
        return list(doc_map.values())
    
    def get_version_history(self, doc_id):
        return self.versions.get(doc_id, [])
    
    def get_document_at_version(self, doc_id, version):
        versions = self.versions.get(doc_id, [])
        for v in versions:
            if v.version == version:
                return v
        return None

# 2. Incremental updates
class IncrementalRAG:
    def __init__(self, vectorstore):
        self.vectorstore = vectorstore
        self.update_queue = []
    
    def queue_update(self, doc_id, new_content):
        self.update_queue.append({
            "doc_id": doc_id,
            "content": new_content,
            "timestamp": datetime.now()
        })
    
    def process_updates(self, batch_size=100):
        # Process updates in batches
        while self.update_queue:
            batch = self.update_queue[:batch_size]
            self.update_queue = self.update_queue[batch_size:]
            
            # Process batch
            for update in batch:
                self._apply_update(update)
    
    def _apply_update(self, update):
        # Remove old version
        self._remove_old_version(update["doc_id"])
        
        # Add new version
        self.vectorstore.add_texts(
            texts=[update["content"]],
            metadatas=[{
                "doc_id": update["doc_id"],
                "timestamp": update["timestamp"].isoformat()
            }]
        )
    
    def _remove_old_version(self, doc_id):
        # Implementation depends on vector store
        # May need to query and delete by metadata
        pass

# 3. Event-driven updates
class EventDrivenRAG:
    def __init__(self, vectorstore, event_bus):
        self.vectorstore = vectorstore
        self.event_bus = event_bus
        self._subscribe_to_events()
    
    def _subscribe_to_events(self):
        self.event_bus.subscribe("document.updated", self._on_document_updated)
        self.event_bus.subscribe("document.deleted", self._on_document_deleted)
        self.event_bus.subscribe("document.created", self._on_document_created)
    
    def _on_document_updated(self, event):
        doc_id = event["doc_id"]
        new_content = event["content"]
        
        # Update vector store
        self._update_document(doc_id, new_content)
    
    def _on_document_deleted(self, event):
        doc_id = event["doc_id"]
        
        # Remove from vector store
        self._delete_document(doc_id)
    
    def _on_document_created(self, event):
        doc_id = event["doc_id"]
        content = event["content"]
        
        # Add to vector store
        self.vectorstore.add_texts(
            texts=[content],
            metadatas=[{"doc_id": doc_id}]
        )
    
    def _update_document(self, doc_id, new_content):
        # Remove old, add new
        self._delete_document(doc_id)
        self.vectorstore.add_texts(
            texts=[new_content],
            metadatas=[{"doc_id": doc_id}]
        )
    
    def _delete_document(self, doc_id):
        # Implementation depends on vector store
        pass

# 4. Timestamp-based versioning
class TimestampedRAG:
    def __init__(self, vectorstore):
        self.vectorstore = vectorstore
    
    def query_with_timestamp(self, query, as_of_date=None):
        # Retrieve documents valid as of specific date
        if as_of_date:
            filter_condition = {"timestamp": {"$lte": as_of_date.isoformat()}}
        else:
            filter_condition = None
        
        docs = self.vectorstore.similarity_search(
            query,
            k=5,
            filter=filter_condition
        )
        
        # Get latest version of each document as of date
        return self._get_latest_as_of(docs, as_of_date)
    
    def _get_latest_as_of(self, docs, as_of_date):
        # Group by doc_id, keep latest version before as_of_date
        doc_map = {}
        for doc in docs:
            doc_id = doc.metadata.get("doc_id")
            doc_timestamp = datetime.fromisoformat(doc.metadata.get("timestamp"))
            
            if as_of_date and doc_timestamp > as_of_date:
                continue
            
            if doc_id not in doc_map or doc_timestamp > datetime.fromisoformat(
                doc_map[doc_id].metadata.get("timestamp")
            ):
                doc_map[doc_id] = doc
        
        return list(doc_map.values())`,
            approaches: [
                "Store version metadata with documents.",
                "Implement incremental updates to avoid full reindexing.",
                "Use event-driven updates for real-time changes.",
                "Support querying specific document versions.",
                "Track document timestamps for temporal queries.",
                "Handle document deletions gracefully.",
                "Maintain version history for audit trails.",
                "Deduplicate results by keeping latest versions.",
                "Implement efficient version comparison.",
                "Support rollback to previous versions."
            ]
        });

        // Q44 - RAG evaluation frameworks
        questions.push({
            number: 44,
            title: "What are comprehensive frameworks for evaluating RAG system performance?",
            description:
                "RAG evaluation requires measuring multiple dimensions: retrieval quality, answer accuracy, " +
                "relevance, faithfulness, and user satisfaction. Comprehensive frameworks combine automated metrics " +
                "with human evaluation to assess overall system performance.",
            why:
                "Without proper evaluation, it's impossible to know if RAG improvements actually help. " +
                "Comprehensive evaluation identifies weaknesses, guides optimization, and ensures quality.",
            what:
                "Evaluation Dimensions: Retrieval metrics (precision, recall, MRR), Answer quality (accuracy, relevance), " +
                "Faithfulness (grounded in sources), Completeness, Latency, Cost. " +
                "Frameworks: RAGAS, ARES, TruLens, Custom evaluation suites, Human evaluation, A/B testing.",
            how:
                "1) Define evaluation metrics, 2) Create test datasets, " +
                "3) Run automated evaluation, 4) Conduct human evaluation, " +
                "5) Analyze results, 6) Identify improvement areas, " +
                "7) Iterate and re-evaluate.",
            pros: [
                "Quantifies performance",
                "Identifies weaknesses",
                "Guides optimization",
                "Enables comparison",
                "Tracks improvements",
                "Builds confidence"
            ],
            cons: [
                "Time-consuming",
                "Requires test datasets",
                "Metrics may not capture everything",
                "Human evaluation is expensive",
                "May need domain expertise",
                "Evaluation itself has costs"
            ],
            diagram: `flowchart TD
    A["RAG System"] --> B["Test Queries"]
    B --> C["Retrieval Evaluation"]
    B --> D["Answer Evaluation"]
    B --> E["Faithfulness Check"]
    C --> F["Precision/Recall"]
    D --> G["Accuracy/Relevance"]
    E --> H["Grounded in Sources"]
    F --> I["Overall Score"]
    G --> I
    H --> I
    I --> J["Improvement Plan"]`,
            implementation: `# Comprehensive RAG Evaluation
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall
)
from datasets import Dataset
import pandas as pd

# 1. RAGAS Evaluation
def evaluate_with_ragas(rag_system, test_dataset):
    """Evaluate RAG system using RAGAS metrics"""
    
    results = []
    for item in test_dataset:
        # Query RAG system
        response = rag_system.query(item["question"])
        
        results.append({
            "question": item["question"],
            "answer": response["answer"],
            "contexts": [d.page_content for d in response["sources"]],
            "ground_truth": item.get("ground_truth", "")
        })
    
    # Create dataset
    eval_dataset = Dataset.from_list(results)
    
    # Evaluate
    score = evaluate(
        dataset=eval_dataset,
        metrics=[
            faithfulness,  # Answer grounded in context
            answer_relevancy,  # Answer relevance to question
            context_precision,  # Precision of retrieved context
            context_recall  # Recall of retrieved context
        ]
    )
    
    return score

# 2. Custom evaluation framework
class RAGEvaluator:
    def __init__(self, rag_system):
        self.rag_system = rag_system
        self.metrics = {}
    
    def evaluate(self, test_cases):
        results = {
            "retrieval_metrics": [],
            "answer_metrics": [],
            "faithfulness_metrics": [],
            "latency_metrics": [],
            "cost_metrics": []
        }
        
        for test_case in test_cases:
            # Run query
            start_time = time.time()
            response = self.rag_system.query(test_case["question"])
            latency = time.time() - start_time
            
            # Evaluate retrieval
            retrieval_score = self._evaluate_retrieval(
                response["sources"],
                test_case.get("expected_sources", [])
            )
            results["retrieval_metrics"].append(retrieval_score)
            
            # Evaluate answer
            answer_score = self._evaluate_answer(
                response["answer"],
                test_case.get("expected_answer", ""),
                test_case["question"]
            )
            results["answer_metrics"].append(answer_score)
            
            # Evaluate faithfulness
            faithfulness_score = self._evaluate_faithfulness(
                response["answer"],
                response["sources"]
            )
            results["faithfulness_metrics"].append(faithfulness_score)
            
            # Track latency
            results["latency_metrics"].append(latency)
            
            # Track cost (if available)
            if "cost" in response:
                results["cost_metrics"].append(response["cost"])
        
        # Aggregate results
        return self._aggregate_results(results)
    
    def _evaluate_retrieval(self, retrieved_docs, expected_docs):
        """Evaluate retrieval quality"""
        retrieved_ids = {d.metadata.get("id") for d in retrieved_docs}
        expected_ids = set(expected_docs)
        
        if not expected_ids:
            return {"precision": None, "recall": None, "f1": None}
        
        intersection = retrieved_ids & expected_ids
        
        precision = len(intersection) / len(retrieved_ids) if retrieved_ids else 0
        recall = len(intersection) / len(expected_ids) if expected_ids else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            "precision": precision,
            "recall": recall,
            "f1": f1
        }
    
    def _evaluate_answer(self, answer, expected_answer, question):
        """Evaluate answer quality"""
        # Semantic similarity
        similarity = self._semantic_similarity(answer, expected_answer)
        
        # Answer relevance to question
        relevance = self._answer_relevance(answer, question)
        
        # Completeness (if expected answer provided)
        completeness = self._completeness(answer, expected_answer) if expected_answer else None
        
        return {
            "similarity": similarity,
            "relevance": relevance,
            "completeness": completeness
        }
    
    def _evaluate_faithfulness(self, answer, sources):
        """Check if answer is grounded in sources"""
        # Use LLM to check if answer can be derived from sources
        source_text = "\\n".join([d.page_content for d in sources])
        
        prompt = f"""Check if the answer can be derived from the sources. Return 1 if yes, 0 if no.
        
        Sources:
        {source_text}
        
        Answer: {answer}
        
        Can answer be derived from sources (1 or 0):"""
        
        result = llm(prompt)
        return float(result.strip())
    
    def _semantic_similarity(self, text1, text2):
        # Use embedding similarity
        emb1 = embedding_model.embed_query(text1)
        emb2 = embedding_model.embed_query(text2)
        return cosine_similarity([emb1], [emb2])[0][0]
    
    def _answer_relevance(self, answer, question):
        # Use LLM to score relevance
        prompt = f"""Rate how relevant this answer is to the question (0-1):
        
        Question: {question}
        Answer: {answer}
        
        Relevance score (0-1):"""
        
        score = float(llm(prompt).strip())
        return score
    
    def _completeness(self, answer, expected):
        # Check if answer covers expected content
        prompt = f"""Rate how complete this answer is compared to expected (0-1):
        
        Expected: {expected}
        Answer: {answer}
        
        Completeness (0-1):"""
        
        score = float(llm(prompt).strip())
        return score
    
    def _aggregate_results(self, results):
        """Aggregate evaluation results"""
        aggregated = {}
        
        for metric_type, values in results.items():
            if not values:
                continue
            
            if metric_type == "retrieval_metrics":
                aggregated[metric_type] = {
                    "avg_precision": np.mean([v["precision"] for v in values if v["precision"] is not None]),
                    "avg_recall": np.mean([v["recall"] for v in values if v["recall"] is not None]),
                    "avg_f1": np.mean([v["f1"] for v in values if v["f1"] is not None])
                }
            elif metric_type == "answer_metrics":
                aggregated[metric_type] = {
                    "avg_similarity": np.mean([v["similarity"] for v in values]),
                    "avg_relevance": np.mean([v["relevance"] for v in values]),
                    "avg_completeness": np.mean([v["completeness"] for v in values if v["completeness"] is not None])
                }
            elif metric_type == "faithfulness_metrics":
                aggregated[metric_type] = {
                    "avg_faithfulness": np.mean(values),
                    "faithfulness_rate": sum(1 for v in values if v > 0.5) / len(values)
                }
            elif metric_type == "latency_metrics":
                aggregated[metric_type] = {
                    "avg_latency": np.mean(values),
                    "p50_latency": np.percentile(values, 50),
                    "p95_latency": np.percentile(values, 95),
                    "p99_latency": np.percentile(values, 99)
                }
            elif metric_type == "cost_metrics":
                aggregated[metric_type] = {
                    "avg_cost": np.mean(values),
                    "total_cost": sum(values)
                }
        
        return aggregated

# 3. A/B Testing framework
class RAGABTest:
    def __init__(self, system_a, system_b):
        self.system_a = system_a
        self.system_b = system_b
        self.results = {"a": [], "b": []}
    
    def run_test(self, test_cases, split=0.5):
        """Run A/B test on test cases"""
        for test_case in test_cases:
            # Randomly assign to A or B
            if random.random() < split:
                result = self._evaluate_system(self.system_a, test_case, "a")
                self.results["a"].append(result)
            else:
                result = self._evaluate_system(self.system_b, test_case, "b")
                self.results["b"].append(result)
        
        return self._compare_results()
    
    def _evaluate_system(self, system, test_case, variant):
        response = system.query(test_case["question"])
        return {
            "variant": variant,
            "question": test_case["question"],
            "answer": response["answer"],
            "latency": response.get("latency", 0),
            "sources_count": len(response.get("sources", []))
        }
    
    def _compare_results(self):
        """Compare A and B results"""
        return {
            "system_a": {
                "avg_latency": np.mean([r["latency"] for r in self.results["a"]]),
                "count": len(self.results["a"])
            },
            "system_b": {
                "avg_latency": np.mean([r["latency"] for r in self.results["b"]]),
                "count": len(self.results["b"])
            }
        }`,
            approaches: [
                "Use RAGAS for standard RAG evaluation metrics.",
                "Create comprehensive test datasets.",
                "Evaluate multiple dimensions (retrieval, answer, faithfulness).",
                "Combine automated and human evaluation.",
                "Track latency and cost metrics.",
                "Implement A/B testing for comparisons.",
                "Regular evaluation cycles.",
                "Monitor production metrics.",
                "Use domain-specific evaluation criteria.",
                "Document evaluation methodology."
            ]
        });

        // Q45 - RAG with external knowledge integration
        questions.push({
            number: 45,
            title: "How do you integrate external knowledge sources (APIs, databases) with RAG?",
            description:
                "RAG systems can be enhanced by integrating external knowledge sources like APIs, databases, " +
                "and real-time data. This creates hybrid systems that combine document retrieval with live data, " +
                "enabling more comprehensive and up-to-date answers.",
            why:
                "Static documents may be outdated or incomplete. External sources provide real-time data, " +
                "complementary information, and dynamic content that enhances RAG answers.",
            what:
                "External Sources: APIs (REST, GraphQL), Databases (SQL, NoSQL), Real-time data feeds, " +
                "Web scraping, Third-party services. " +
                "Integration Patterns: Parallel retrieval, Sequential enrichment, Conditional fetching, " +
                "Result fusion, Source prioritization.",
            how:
                "1) Identify external sources, 2) Design integration architecture, " +
                "3) Implement data fetching, 4) Transform external data, " +
                "5) Combine with RAG results, 6) Prioritize and rank sources, " +
                "7) Generate unified answer.",
            pros: [
                "More comprehensive answers",
                "Real-time data",
                "Broader knowledge",
                "Dynamic content",
                "Better coverage",
                "Up-to-date information"
            ],
            cons: [
                "More complex architecture",
                "External dependencies",
                "Higher latency",
                "API rate limits",
                "Cost of external calls",
                "Error handling complexity"
            ],
            diagram: `flowchart TD
    A["User Query"] --> B["RAG Retrieval"]
    A --> C["External API"]
    A --> D["Database Query"]
    A --> E["Real-time Feed"]
    B --> F["Result Fusion"]
    C --> F
    D --> F
    E --> F
    F --> G["Prioritize Sources"]
    G --> H["Generate Unified Answer"]`,
            implementation: `# RAG with External Knowledge Integration
import requests
import asyncio
from typing import List, Dict
from langchain.tools import Tool

# 1. External API integration
class ExternalAPIRAG:
    def __init__(self, vectorstore, llm, api_configs):
        self.vectorstore = vectorstore
        self.llm = llm
        self.api_configs = api_configs
    
    def query_with_external(self, query):
        # Retrieve from documents
        docs = self.vectorstore.similarity_search(query, k=5)
        doc_context = "\\n".join([d.page_content for d in docs])
        
        # Fetch from external APIs
        external_data = self._fetch_external_data(query)
        
        # Combine contexts
        combined_context = f"""Document Context:
        {doc_context}
        
        External Data:
        {external_data}
        
        Question: {query}"""
        
        # Generate answer
        answer = self.llm(combined_context)
        
        return {
            "answer": answer,
            "document_sources": docs,
            "external_sources": external_data
        }
    
    def _fetch_external_data(self, query):
        results = []
        
        for api_name, config in self.api_configs.items():
            try:
                # Determine if API is relevant
                if self._is_relevant(query, config["keywords"]):
                    data = self._call_api(api_name, query, config)
                    results.append(f"[{api_name}] {data}")
            except Exception as e:
                print(f"Error fetching from {api_name}: {e}")
        
        return "\\n".join(results)
    
    def _is_relevant(self, query, keywords):
        query_lower = query.lower()
        return any(keyword.lower() in query_lower for keyword in keywords)
    
    def _call_api(self, api_name, query, config):
        if config["type"] == "rest":
            response = requests.get(
                config["url"],
                params={"query": query, **config.get("params", {})},
                headers=config.get("headers", {}),
                timeout=config.get("timeout", 5)
            )
            return response.json()
        # Add other API types

# 2. Database integration
class DatabaseRAG:
    def __init__(self, vectorstore, llm, database):
        self.vectorstore = vectorstore
        self.llm = llm
        self.database = database
    
    def query_with_database(self, query):
        # Retrieve from documents
        docs = self.vectorstore.similarity_search(query, k=5)
        
        # Generate SQL query from natural language
        sql_query = self._generate_sql(query)
        
        # Execute SQL
        db_results = self._execute_query(sql_query)
        
        # Combine contexts
        context = f"""Documents: {docs}
        Database Results: {db_results}
        Question: {query}"""
        
        answer = self.llm(context)
        
        return {
            "answer": answer,
            "document_sources": docs,
            "database_results": db_results,
            "sql_query": sql_query
        }
    
    def _generate_sql(self, query):
        schema = self.database.get_schema()
        
        prompt = f"""Generate SQL query for this question. Schema: {schema}
        
        Question: {query}
        
        SQL:"""
        
        sql = self.llm(prompt)
        return sql
    
    def _execute_query(self, sql):
        try:
            return self.database.execute(sql)
        except Exception as e:
            return f"Query error: {e}"

# 3. LangChain tools integration
from langchain.agents import initialize_agent, AgentType

def create_rag_with_tools(vectorstore, llm):
    # Define tools
    def search_documents(query: str) -> str:
        docs = vectorstore.similarity_search(query, k=5)
        return "\\n".join([d.page_content for d in docs])
    
    def call_weather_api(location: str) -> str:
        # Example: weather API
        response = requests.get(f"https://api.weather.com/{location}")
        return response.json()
    
    def query_database(query: str) -> str:
        # Example: database query
        # Implementation
        return "Database results"
    
    # Create tools
    tools = [
        Tool(
            name="DocumentSearch",
            func=search_documents,
            description="Search internal documents"
        ),
        Tool(
            name="WeatherAPI",
            func=call_weather_api,
            description="Get weather information for a location"
        ),
        Tool(
            name="DatabaseQuery",
            func=query_database,
            description="Query internal database"
        )
    ]
    
    # Create agent
    agent = initialize_agent(
        tools=tools,
        llm=llm,
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True
    )
    
    return agent

# 4. Async parallel fetching
class AsyncExternalRAG:
    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
    
    async def query(self, query):
        # Parallel execution
        doc_task = asyncio.to_thread(
            self.vectorstore.similarity_search, query, k=5
        )
        api_task = self._fetch_from_api(query)
        db_task = self._query_database(query)
        
        # Wait for all
        docs, api_data, db_data = await asyncio.gather(
            doc_task, api_task, db_task
        )
        
        # Combine and generate
        context = f"""Documents: {docs}
        API: {api_data}
        Database: {db_data}"""
        
        answer = await self.llm.agenerate(context + "\\n\\nQuestion: " + query)
        
        return {
            "answer": answer,
            "sources": {
                "documents": docs,
                "api": api_data,
                "database": db_data
            }
        }
    
    async def _fetch_from_api(self, query):
        # Async API call
        async with aiohttp.ClientSession() as session:
            async with session.get(f"https://api.example.com?q={query}") as response:
                return await response.json()
    
    async def _query_database(self, query):
        # Async database query
        # Implementation
        return "DB results"

# 5. Result fusion and prioritization
class FusedRAG:
    def __init__(self, vectorstore, llm, external_sources):
        self.vectorstore = vectorstore
        self.llm = llm
        self.external_sources = external_sources
    
    def query(self, query):
        # Retrieve from all sources
        all_results = []
        
        # Documents
        docs = self.vectorstore.similarity_search(query, k=5)
        all_results.append({
            "source": "documents",
            "content": docs,
            "priority": 1.0
        })
        
        # External sources
        for source_name, source_func in self.external_sources.items():
            try:
                data = source_func(query)
                all_results.append({
                    "source": source_name,
                    "content": data,
                    "priority": 0.7  # Lower priority than documents
                })
            except:
                pass
        
        # Prioritize and combine
        sorted_results = sorted(all_results, key=lambda x: x["priority"], reverse=True)
        
        # Build context
        context_parts = []
        for result in sorted_results[:5]:  # Top 5 sources
            context_parts.append(f"[{result['source']}] {result['content']}")
        
        context = "\\n".join(context_parts)
        
        # Generate
        answer = self.llm(f"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:")
        
        return {
            "answer": answer,
            "sources": sorted_results
        }`,
            approaches: [
                "Identify relevant external sources for your domain.",
                "Use async/parallel fetching for performance.",
                "Implement result fusion and prioritization.",
                "Handle API rate limits and errors gracefully.",
                "Cache external API responses when possible.",
                "Use LangChain tools for structured integration.",
                "Validate and sanitize external data.",
                "Monitor external source availability.",
                "Design fallback strategies.",
                "Document external dependencies clearly."
            ]
        });

        // Q46 - RAG with personalization
        questions.push({
            number: 46,
            title: "How do you implement personalization in RAG systems?",
            description:
                "Personalization tailors RAG responses to individual users based on their preferences, history, " +
                "role, and context. This involves user profiling, preference learning, context-aware retrieval, " +
                "and customized answer generation.",
            why:
                "Different users have different needs, expertise levels, and preferences. Personalization improves " +
                "relevance, user satisfaction, and makes RAG systems more effective for diverse user bases.",
            what:
                "Personalization Components: User profiles, Preference learning, Role-based filtering, " +
                "History-based ranking, Context awareness, Customized prompts. " +
                "Personalization Types: Content personalization, Format personalization, " +
                "Depth personalization (expert vs beginner), Domain personalization.",
            how:
                "1) Build user profiles, 2) Track user interactions and preferences, " +
                "3) Personalize retrieval (filter/rank by user context), 4) Customize prompts based on user, " +
                "5) Generate personalized answers, 6) Learn from feedback, " +
                "7) Update user profiles continuously.",
            pros: [
                "Better relevance",
                "Improved user satisfaction",
                "Adapts to user needs",
                "More effective for diverse users",
                "Learns from interactions",
                "Higher engagement"
            ],
            cons: [
                "Requires user data",
                "Privacy concerns",
                "More complex implementation",
                "Cold start problem",
                "Need user tracking",
                "May create filter bubbles"
            ],
            diagram: `flowchart TD
    A["User Query"] --> B["User Profile"]
    B --> C["Personalized Retrieval"]
    C --> D["Filter by Role/Preferences"]
    D --> E["Rank by User History"]
    E --> F["Customized Prompt"]
    F --> G["Personalized Answer"]
    G --> H["Update Profile"]`,
            implementation: `# Personalized RAG System
from typing import Dict, List, Optional
from dataclasses import dataclass
from collections import defaultdict

@dataclass
class UserProfile:
    user_id: str
    role: str
    expertise_level: str  # beginner, intermediate, expert
    preferences: Dict[str, any]
    interaction_history: List[Dict]
    preferred_topics: List[str]
    preferred_format: str  # detailed, concise, examples

class PersonalizedRAG:
    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
        self.user_profiles = {}  # user_id -> UserProfile
    
    def get_or_create_profile(self, user_id):
        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = UserProfile(
                user_id=user_id,
                role="default",
                expertise_level="intermediate",
                preferences={},
                interaction_history=[],
                preferred_topics=[],
                preferred_format="detailed"
            )
        return self.user_profiles[user_id]
    
    def query(self, query, user_id=None):
        profile = self.get_or_create_profile(user_id) if user_id else None
        
        # Personalized retrieval
        docs = self._personalized_retrieval(query, profile)
        
        # Customized prompt
        prompt = self._build_personalized_prompt(query, docs, profile)
        
        # Generate answer
        answer = self.llm(prompt)
        
        # Update profile
        if profile:
            self._update_profile(profile, query, answer, docs)
        
        return {
            "answer": answer,
            "sources": docs,
            "personalized": profile is not None
        }
    
    def _personalized_retrieval(self, query, profile):
        if not profile:
            return self.vectorstore.similarity_search(query, k=5)
        
        # Retrieve more documents
        all_docs = self.vectorstore.similarity_search(query, k=10)
        
        # Personalize ranking
        scored_docs = []
        for doc in all_docs:
            score = self._calculate_personalized_score(doc, profile)
            scored_docs.append((doc, score))
        
        # Sort by personalized score
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        
        # Return top 5
        return [doc for doc, score in scored_docs[:5]]
    
    def _calculate_personalized_score(self, doc, profile):
        score = 1.0
        
        # Boost by preferred topics
        doc_text = doc.page_content.lower()
        for topic in profile.preferred_topics:
            if topic.lower() in doc_text:
                score *= 1.5
        
        # Boost by role relevance
        if profile.role in doc.metadata.get("tags", []):
            score *= 1.3
        
        # Boost by interaction history (documents user has engaged with)
        doc_id = doc.metadata.get("id")
        for interaction in profile.interaction_history[-10:]:
            if interaction.get("doc_id") == doc_id and interaction.get("helpful"):
                score *= 1.2
        
        return score
    
    def _build_personalized_prompt(self, query, docs, profile):
        context = "\\n".join([d.page_content for d in docs])
        
        if not profile:
            return f"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:"
        
        # Customize based on expertise level
        if profile.expertise_level == "beginner":
            instruction = "Explain in simple terms suitable for beginners. Use examples."
        elif profile.expertise_level == "expert":
            instruction = "Provide detailed technical explanation. Assume expert knowledge."
        else:
            instruction = "Provide a balanced explanation."
        
        # Customize based on preferred format
        if profile.preferred_format == "concise":
            instruction += " Be concise."
        elif profile.preferred_format == "examples":
            instruction += " Include practical examples."
        
        prompt = f"""Context: {context}
        
        Question: {query}
        
        Instructions: {instruction}
        
        Answer:"""
        
        return prompt
    
    def _update_profile(self, profile, query, answer, docs):
        # Track interaction
        interaction = {
            "query": query,
            "answer": answer,
            "timestamp": datetime.now(),
            "doc_ids": [d.metadata.get("id") for d in docs],
            "helpful": None  # Will be updated from feedback
        }
        
        profile.interaction_history.append(interaction)
        
        # Extract topics from query (simple keyword extraction)
        query_words = set(query.lower().split())
        for word in query_words:
            if len(word) > 4:  # Filter short words
                if word not in profile.preferred_topics:
                    profile.preferred_topics.append(word)
        
        # Keep only recent topics
        profile.preferred_topics = profile.preferred_topics[-20:]

# 2. Role-based personalization
class RoleBasedRAG:
    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
        self.role_filters = {
            "developer": ["code", "implementation", "technical"],
            "manager": ["strategy", "overview", "business"],
            "designer": ["design", "ui", "ux", "visual"]
        }
    
    def query(self, query, user_role=None):
        # Filter documents by role
        if user_role and user_role in self.role_filters:
            # Boost documents matching role keywords
            docs = self._role_filtered_search(query, user_role)
        else:
            docs = self.vectorstore.similarity_search(query, k=5)
        
        # Role-specific prompt
        prompt = self._build_role_prompt(query, docs, user_role)
        
        answer = self.llm(prompt)
        
        return {"answer": answer, "sources": docs}
    
    def _role_filtered_search(self, query, role):
        role_keywords = self.role_filters[role]
        
        # Retrieve more documents
        all_docs = self.vectorstore.similarity_search(query, k=10)
        
        # Score by role relevance
        scored = []
        for doc in all_docs:
            doc_text = doc.page_content.lower()
            role_score = sum(1 for keyword in role_keywords if keyword in doc_text)
            scored.append((doc, role_score))
        
        scored.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, score in scored[:5]]
    
    def _build_role_prompt(self, query, docs, role):
        context = "\\n".join([d.page_content for d in docs])
        
        role_instructions = {
            "developer": "Focus on technical implementation details and code examples.",
            "manager": "Focus on high-level overview, business impact, and strategy.",
            "designer": "Focus on design principles, user experience, and visual aspects."
        }
        
        instruction = role_instructions.get(role, "")
        
        return f"""Context: {context}
        
        Question: {query}
        
        {instruction}
        
        Answer:"""

# 3. Learning from feedback
class FeedbackLearningRAG:
    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
        self.feedback_history = defaultdict(list)
    
    def query(self, query, user_id):
        docs = self.vectorstore.similarity_search(query, k=5)
        answer = self.llm(f"Context: {docs}\\n\\nQuestion: {query}")
        
        return {"answer": answer, "sources": docs}
    
    def record_feedback(self, user_id, query, answer, helpful, doc_ids):
        """Record user feedback to improve personalization"""
        self.feedback_history[user_id].append({
            "query": query,
            "answer": answer,
            "helpful": helpful,
            "doc_ids": doc_ids,
            "timestamp": datetime.now()
        })
        
        # Update user preferences based on feedback
        if helpful:
            # User liked these documents, boost them in future
            self._boost_documents(user_id, doc_ids)
        else:
            # User didn't find these helpful, avoid similar in future
            self._learn_from_negative_feedback(user_id, query, doc_ids)
    
    def _boost_documents(self, user_id, doc_ids):
        # Store preference for these document types
        # Implementation: update user profile
        pass
    
    def _learn_from_negative_feedback(self, user_id, query, doc_ids):
        # Learn what not to retrieve for similar queries
        # Implementation: update retrieval strategy
        pass

# 4. Context-aware personalization
class ContextAwarePersonalizedRAG:
    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
    
    def query(self, query, user_id, context=None):
        """
        context: {
            "current_task": "...",
            "recent_queries": [...],
            "session_info": {...}
        }
        """
        profile = self.get_user_profile(user_id)
        
        # Enhance query with context
        enhanced_query = self._enhance_query_with_context(query, context, profile)
        
        # Retrieve
        docs = self._context_aware_retrieval(enhanced_query, context, profile)
        
        # Generate with context
        answer = self._generate_with_context(query, docs, context, profile)
        
        return {"answer": answer, "sources": docs}
    
    def _enhance_query_with_context(self, query, context, profile):
        if not context:
            return query
        
        # Add context from current task
        if context.get("current_task"):
            query += f" (Context: {context['current_task']})"
        
        # Add context from recent queries
        if context.get("recent_queries"):
            recent = " ".join(context["recent_queries"][-3:])
            query += f" (Related to: {recent})"
        
        return query
    
    def _context_aware_retrieval(self, query, context, profile):
        # Retrieve with context-aware boosting
        docs = self.vectorstore.similarity_search(query, k=10)
        
        # Boost documents relevant to current context
        if context and context.get("current_task"):
            for doc in docs:
                if context["current_task"].lower() in doc.page_content.lower():
                    # Boost this document
                    pass
        
        return docs[:5]
    
    def _generate_with_context(self, query, docs, context, profile):
        context_str = ""
        if context:
            context_str = f"\\nCurrent context: {context.get('current_task', '')}"
        
        prompt = f"""Context from documents: {docs}
        {context_str}
        
        Question: {query}
        
        Answer:"""
        
        return self.llm(prompt)`,
            approaches: [
                "Build user profiles with preferences and history.",
                "Personalize retrieval based on user context.",
                "Customize prompts for different user types.",
                "Learn from user feedback and interactions.",
                "Implement role-based filtering.",
                "Adapt to expertise levels (beginner/expert).",
                "Track and use interaction history.",
                "Respect user privacy and data preferences.",
                "Handle cold start for new users.",
                "Continuously update profiles from interactions."
            ]
        });

        // Q47 - RAG with query expansion and reformulation
        questions.push({
            number: 47,
            title: "How do you use query expansion and reformulation to improve RAG retrieval?",
            description:
                "Query expansion adds related terms and synonyms to improve retrieval coverage. " +
                "Query reformulation rewrites queries to be more effective for retrieval. " +
                "Both techniques help RAG systems find relevant documents even when user queries don't match document terminology.",
            why:
                "User queries often use different terminology than documents. Query expansion and reformulation " +
                "bridge this gap, improving recall and helping find relevant documents that might otherwise be missed.",
            what:
                "Query Expansion: Synonym addition, Related term inclusion, Concept expansion, " +
                "Acronym expansion, Multi-language support. " +
                "Query Reformulation: Paraphrasing, Question decomposition, Query simplification, " +
                "Query enhancement, Intent-based rewriting.",
            how:
                "1) Analyze original query, 2) Expand with synonyms/related terms, " +
                "3) Reformulate for better retrieval, 4) Generate multiple query variations, " +
                "5) Retrieve with expanded/reformulated queries, 6) Combine and rerank results, " +
                "7) Use best results for generation.",
            pros: [
                "Better recall",
                "Finds more relevant documents",
                "Handles terminology mismatches",
                "Improves retrieval quality",
                "More robust to query variations",
                "Better coverage"
            ],
            cons: [
                "May retrieve irrelevant documents",
                "More complex queries",
                "Higher computational cost",
                "Need good expansion sources",
                "May dilute query intent",
                "Requires tuning"
            ],
            diagram: `flowchart TD
    A["Original Query"] --> B["Query Analysis"]
    B --> C["Expand Synonyms"]
    B --> D["Reformulate"]
    B --> E["Add Related Terms"]
    C --> F["Query Variations"]
    D --> F
    E --> F
    F --> G["Multi-Query Retrieval"]
    G --> H["Result Fusion"]
    H --> I["Rerank"]
    I --> J["Best Results"]`,
            implementation: `# Query Expansion and Reformulation for RAG
from typing import List
import nltk
from nltk.corpus import wordnet
import spacy

# 1. Synonym-based expansion
class SynonymExpansion:
    def __init__(self):
        try:
            nltk.download('wordnet')
            nltk.download('omw-1.4')
        except:
            pass
    
    def expand_query(self, query):
        words = query.split()
        expanded_terms = []
        
        for word in words:
            expanded_terms.append(word)
            
            # Get synonyms
            synonyms = self._get_synonyms(word)
            expanded_terms.extend(synonyms[:3])  # Top 3 synonyms
        
        return " ".join(expanded_terms)
    
    def _get_synonyms(self, word):
        synonyms = set()
        for syn in wordnet.synsets(word):
            for lemma in syn.lemmas():
                synonyms.add(lemma.name().replace('_', ' '))
        return list(synonyms)

# 2. LLM-based query expansion
class LLMQueryExpansion:
    def __init__(self, llm):
        self.llm = llm
    
    def expand_query(self, query):
        prompt = f"""Expand this search query with synonyms and related terms. Return only the expanded query.
        
        Original query: {query}
        
        Expanded query:"""
        
        expanded = self.llm(prompt)
        return expanded.strip()
    
    def expand_with_context(self, query, domain=None):
        context = f" in the context of {domain}" if domain else ""
        
        prompt = f"""Expand this query with domain-specific synonyms and related terms{context}:
        
        Query: {query}
        
        Expanded query:"""
        
        return self.llm(prompt).strip()

# 3. Query reformulation
class QueryReformulation:
    def __init__(self, llm):
        self.llm = llm
    
    def reformulate(self, query):
        """Reformulate query for better retrieval"""
        prompt = f"""Reformulate this query to be more effective for document retrieval. 
        Keep the same intent but use terminology more likely to appear in documents.
        
        Original query: {query}
        
        Reformulated query:"""
        
        return self.llm(prompt).strip()
    
    def reformulate_for_retrieval(self, query, document_examples=None):
        """Reformulate using document terminology"""
        doc_context = ""
        if document_examples:
            doc_context = f"\\nExample document terminology: {document_examples[:500]}"
        
        prompt = f"""Reformulate this query to match document terminology better.{doc_context}
        
        Query: {query}
        
        Reformulated query:"""
        
        return self.llm(prompt).strip()
    
    def decompose_complex_query(self, query):
        """Break down complex queries into simpler sub-queries"""
        prompt = f"""Break down this complex query into 2-3 simpler sub-queries:
        
        Complex query: {query}
        
        Sub-queries (one per line):"""
        
        sub_queries = self.llm(prompt).strip().split('\\n')
        return [q.strip() for q in sub_queries if q.strip()]

# 4. Multi-query retrieval
class MultiQueryRAG:
    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
        self.expander = LLMQueryExpansion(llm)
        self.reformulator = QueryReformulation(llm)
    
    def query(self, original_query):
        # Generate query variations
        variations = self._generate_variations(original_query)
        
        # Retrieve with each variation
        all_docs = []
        for variation in variations:
            docs = self.vectorstore.similarity_search(variation, k=5)
            all_docs.extend(docs)
        
        # Deduplicate and rerank
        unique_docs = self._deduplicate_docs(all_docs)
        reranked = self._rerank(unique_docs, original_query)
        
        # Use top results
        top_docs = reranked[:5]
        
        # Generate answer
        context = "\\n".join([d.page_content for d in top_docs])
        answer = self.llm(f"Context: {context}\\n\\nQuestion: {original_query}\\n\\nAnswer:")
        
        return {
            "answer": answer,
            "sources": top_docs,
            "query_variations": variations
        }
    
    def _generate_variations(self, query):
        variations = [query]  # Original
        
        # Expanded version
        expanded = self.expander.expand_query(query)
        variations.append(expanded)
        
        # Reformulated version
        reformulated = self.reformulator.reformulate(query)
        variations.append(reformulated)
        
        # If complex, add decomposed queries
        if len(query.split()) > 5:
            sub_queries = self.reformulator.decompose_complex_query(query)
            variations.extend(sub_queries)
        
        return list(set(variations))  # Remove duplicates
    
    def _deduplicate_docs(self, docs):
        seen = set()
        unique = []
        for doc in docs:
            doc_id = doc.metadata.get("id") or doc.page_content[:100]
            if doc_id not in seen:
                seen.add(doc_id)
                unique.append(doc)
        return unique
    
    def _rerank(self, docs, original_query):
        # Simple reranking by similarity to original query
        # In practice, use a cross-encoder or LLM-based reranker
        scored = []
        for doc in docs:
            # Calculate similarity (simplified)
            score = self._calculate_similarity(original_query, doc.page_content)
            scored.append((doc, score))
        
        scored.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, score in scored]

# 5. Advanced query expansion with embeddings
class EmbeddingBasedExpansion:
    def __init__(self, embedding_model):
        self.embedding_model = embedding_model
    
    def expand_query(self, query, top_k=5):
        # Get query embedding
        query_embedding = self.embedding_model.embed_query(query)
        
        # Find similar terms in vocabulary (if available)
        # This is a simplified version - in practice, you'd have a vocabulary
        # and find terms with similar embeddings
        
        # Alternative: Use LLM to generate terms with similar embeddings
        return query  # Placeholder
    
    def expand_with_semantic_similarity(self, query, candidate_terms):
        """Expand query with semantically similar terms"""
        query_emb = self.embedding_model.embed_query(query)
        
        scored_terms = []
        for term in candidate_terms:
            term_emb = self.embedding_model.embed_query(term)
            similarity = cosine_similarity([query_emb], [term_emb])[0][0]
            scored_terms.append((term, similarity))
        
        # Return top similar terms
        scored_terms.sort(key=lambda x: x[1], reverse=True)
        return [term for term, score in scored_terms[:top_k]]

# 6. Hybrid expansion and reformulation
class HybridQueryEnhancement:
    def __init__(self, vectorstore, llm, embedding_model):
        self.vectorstore = vectorstore
        self.llm = llm
        self.embedding_model = embedding_model
        self.expander = LLMQueryExpansion(llm)
        self.reformulator = QueryReformulation(llm)
    
    def enhanced_query(self, query):
        # Step 1: Expand
        expanded = self.expander.expand_query(query)
        
        # Step 2: Reformulate
        reformulated = self.reformulator.reformulate(query)
        
        # Step 3: Combine strategies
        queries = [
            query,  # Original
            expanded,  # Expanded
            reformulated,  # Reformulated
            f"{query} {expanded}",  # Combined
        ]
        
        # Step 4: Retrieve with all
        all_results = []
        for q in queries:
            docs = self.vectorstore.similarity_search(q, k=3)
            all_results.extend(docs)
        
        # Step 5: Deduplicate and rerank
        unique = self._deduplicate(all_results)
        reranked = self._rerank_by_relevance(unique, query)
        
        return reranked[:5]
    
    def _deduplicate(self, docs):
        seen = set()
        unique = []
        for doc in docs:
            content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()
            if content_hash not in seen:
                seen.add(content_hash)
                unique.append(doc)
        return unique
    
    def _rerank_by_relevance(self, docs, original_query):
        # Use cross-encoder or LLM for reranking
        # Simplified version
        return docs`,
            approaches: [
                "Use synonym expansion to add related terms.",
                "Leverage LLMs for intelligent query expansion.",
                "Reformulate queries to match document terminology.",
                "Generate multiple query variations.",
                "Retrieve with all variations and combine results.",
                "Use embedding-based expansion for semantic similarity.",
                "Decompose complex queries into simpler sub-queries.",
                "Deduplicate and rerank combined results.",
                "Test expansion strategies on your document corpus.",
                "Monitor retrieval quality with and without expansion."
            ]
        });

        // Q48 - RAG with uncertainty quantification
        questions.push({
            number: 48,
            title: "How do you quantify and communicate uncertainty in RAG system outputs?",
            description:
                "RAG systems should indicate when they're uncertain about answers. Uncertainty quantification helps users " +
                "understand answer reliability, identify gaps in knowledge, and make informed decisions. " +
                "This involves confidence scoring, source quality assessment, and uncertainty communication.",
            why:
                "Users need to know when to trust RAG answers. Uncertainty quantification builds trust, enables risk assessment, " +
                "and helps identify when human verification is needed. It's critical for high-stakes applications.",
            what:
                "Uncertainty Types: Epistemic (knowledge gaps), Aleatoric (inherent randomness), Model uncertainty, " +
                "Retrieval uncertainty, Answer confidence. " +
                "Quantification Methods: Confidence scores, Probability distributions, Entropy measures, " +
                "Source agreement, Retrieval quality metrics.",
            how:
                "1) Calculate retrieval confidence, 2) Assess source quality and relevance, " +
                "3) Measure answer confidence, 4) Quantify knowledge gaps, " +
                "5) Combine uncertainty signals, 6) Generate confidence scores, " +
                "7) Communicate uncertainty to users.",
            pros: [
                "Builds user trust",
                "Enables risk assessment",
                "Identifies knowledge gaps",
                "Improves decision-making",
                "Supports human-in-the-loop",
                "Better transparency"
            ],
            cons: [
                "Adds complexity",
                "Uncertainty estimation may be inaccurate",
                "May reduce user confidence",
                "Requires calibration",
                "Additional computation",
                "Need to communicate effectively"
            ],
            diagram: `flowchart TD
    A["RAG Query"] --> B["Retrieval"]
    B --> C["Retrieval Confidence"]
    B --> D["Source Quality"]
    D --> E["Answer Generation"]
    E --> F["Answer Confidence"]
    C --> G["Uncertainty Quantification"]
    D --> G
    F --> G
    G --> H["Confidence Score"]
    H --> I["Uncertainty Communication"]`,
            implementation: `# RAG with Uncertainty Quantification
from typing import Dict, List
import numpy as np
from scipy.stats import entropy

class UncertaintyQuantifier:
    def __init__(self, embedding_model, llm):
        self.embedding_model = embedding_model
        self.llm = llm
    
    def quantify_uncertainty(self, query, retrieved_docs, answer):
        """Quantify overall uncertainty in RAG response"""
        
        # 1. Retrieval uncertainty
        retrieval_uncertainty = self._retrieval_uncertainty(query, retrieved_docs)
        
        # 2. Source quality uncertainty
        source_uncertainty = self._source_quality_uncertainty(retrieved_docs)
        
        # 3. Answer confidence
        answer_confidence = self._answer_confidence(query, answer, retrieved_docs)
        
        # 4. Source agreement
        source_agreement = self._source_agreement(retrieved_docs)
        
        # Combine uncertainties
        overall_uncertainty = self._combine_uncertainties(
            retrieval_uncertainty,
            source_uncertainty,
            answer_confidence,
            source_agreement
        )
        
        return {
            "overall_confidence": 1 - overall_uncertainty,
            "retrieval_confidence": 1 - retrieval_uncertainty,
            "source_quality": 1 - source_uncertainty,
            "answer_confidence": answer_confidence,
            "source_agreement": source_agreement,
            "uncertainty_breakdown": {
                "retrieval": retrieval_uncertainty,
                "source": source_uncertainty,
                "answer": 1 - answer_confidence,
                "agreement": 1 - source_agreement
            }
        }
    
    def _retrieval_uncertainty(self, query, docs):
        """Measure uncertainty in retrieval quality"""
        if not docs:
            return 1.0  # Maximum uncertainty
        
        # Calculate similarity scores
        query_embedding = self.embedding_model.embed_query(query)
        similarities = []
        
        for doc in docs:
            doc_embedding = self.embedding_model.embed_query(doc.page_content)
            similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]
            similarities.append(similarity)
        
        # Uncertainty based on similarity distribution
        avg_similarity = np.mean(similarities)
        similarity_variance = np.var(similarities)
        
        # Low average similarity = high uncertainty
        # High variance = high uncertainty
        uncertainty = (1 - avg_similarity) * 0.7 + similarity_variance * 0.3
        
        return min(uncertainty, 1.0)
    
    def _source_quality_uncertainty(self, docs):
        """Assess uncertainty from source quality"""
        if not docs:
            return 1.0
        
        quality_scores = []
        for doc in docs:
            # Assess source quality factors
            quality = 1.0
            
            # Length (too short or too long may be less reliable)
            length = len(doc.page_content)
            if length < 50:
                quality *= 0.7
            elif length > 5000:
                quality *= 0.8
            
            # Metadata quality
            if not doc.metadata:
                quality *= 0.9
            
            # Date (older sources may be less reliable)
            if "date" in doc.metadata:
                # Check if date is recent (simplified)
                pass
            
            quality_scores.append(quality)
        
        avg_quality = np.mean(quality_scores)
        return 1 - avg_quality
    
    def _answer_confidence(self, query, answer, docs):
        """Measure confidence in the generated answer"""
        # Check if answer can be derived from sources
        source_text = "\\n".join([d.page_content for d in docs])
        
        prompt = f"""Rate your confidence (0-1) that this answer can be derived from the sources.
        
        Sources:
        {source_text}
        
        Answer: {answer}
        
        Confidence (0-1):"""
        
        confidence = float(self.llm(prompt).strip())
        return confidence
    
    def _source_agreement(self, docs):
        """Measure agreement between sources"""
        if len(docs) < 2:
            return 0.5  # Neutral if only one source
        
        # Calculate semantic similarity between sources
        embeddings = [self.embedding_model.embed_query(d.page_content) for d in docs]
        
        similarities = []
        for i in range(len(embeddings)):
            for j in range(i+1, len(embeddings)):
                sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]
                similarities.append(sim)
        
        # High agreement = high average similarity
        agreement = np.mean(similarities)
        return agreement
    
    def _combine_uncertainties(self, retrieval, source, answer, agreement):
        """Combine different uncertainty measures"""
        # Weighted combination
        combined = (
            retrieval * 0.3 +
            source * 0.2 +
            (1 - answer) * 0.3 +
            (1 - agreement) * 0.2
        )
        return min(combined, 1.0)

# 2. Confidence-aware RAG
class ConfidenceAwareRAG:
    def __init__(self, vectorstore, llm, quantifier):
        self.vectorstore = vectorstore
        self.llm = llm
        self.quantifier = quantifier
    
    def query_with_confidence(self, query):
        # Retrieve
        docs = self.vectorstore.similarity_search(query, k=5)
        
        # Generate answer
        context = "\\n".join([d.page_content for d in docs])
        answer = self.llm(f"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:")
        
        # Quantify uncertainty
        uncertainty = self.quantifier.quantify_uncertainty(query, docs, answer)
        
        # Format response with confidence
        response = {
            "answer": answer,
            "sources": docs,
            "confidence": uncertainty["overall_confidence"],
            "confidence_breakdown": uncertainty["uncertainty_breakdown"],
            "uncertainty_level": self._classify_uncertainty(uncertainty["overall_confidence"])
        }
        
        return response
    
    def _classify_uncertainty(self, confidence):
        """Classify uncertainty level"""
        if confidence >= 0.8:
            return "high"
        elif confidence >= 0.6:
            return "medium"
        elif confidence >= 0.4:
            return "low"
        else:
            return "very_low"

# 3. Uncertainty communication
class UncertaintyCommunicator:
    def format_response_with_uncertainty(self, response):
        """Format response to communicate uncertainty"""
        confidence = response["confidence"]
        uncertainty_level = response["uncertainty_level"]
        
        # Add confidence indicator
        if uncertainty_level == "high":
            indicator = "✓ High confidence"
            color = "green"
        elif uncertainty_level == "medium":
            indicator = "⚠ Medium confidence"
            color = "yellow"
        elif uncertainty_level == "low":
            indicator = "⚠ Low confidence"
            color = "orange"
        else:
            indicator = "✗ Very low confidence"
            color = "red"
        
        formatted_answer = f"""{response['answer']}

[Confidence: {confidence:.0%} - {indicator}]

Breakdown:
- Retrieval confidence: {response['confidence_breakdown']['retrieval']:.0%}
- Source quality: {response['confidence_breakdown']['source']:.0%}
- Answer confidence: {response['confidence_breakdown']['answer']:.0%}
- Source agreement: {response['confidence_breakdown']['agreement']:.0%}"""
        
        return formatted_answer

# 4. Calibrated confidence scores
class CalibratedConfidence:
    def __init__(self):
        self.calibration_data = []  # (predicted_confidence, actual_correct)
    
    def calibrate(self, predictions, ground_truth):
        """Calibrate confidence scores using ground truth"""
        for pred, truth in zip(predictions, ground_truth):
            self.calibration_data.append({
                "predicted": pred["confidence"],
                "actual": 1.0 if self._is_correct(pred, truth) else 0.0
            })
        
        # Fit calibration curve
        # (Simplified - in practice use Platt scaling or isotonic regression)
        return self
    
    def adjust_confidence(self, raw_confidence):
        """Adjust confidence based on calibration"""
        # Simple adjustment based on calibration data
        if not self.calibration_data:
            return raw_confidence
        
        # Calculate expected accuracy for this confidence level
        similar_predictions = [
            d for d in self.calibration_data
            if abs(d["predicted"] - raw_confidence) < 0.1
        ]
        
        if similar_predictions:
            actual_accuracy = np.mean([d["actual"] for d in similar_predictions])
            # Adjust towards actual accuracy
            adjusted = raw_confidence * 0.7 + actual_accuracy * 0.3
            return adjusted
        
        return raw_confidence
    
    def _is_correct(self, prediction, ground_truth):
        # Compare prediction answer with ground truth
        # Implementation depends on evaluation method
        return False  # Placeholder`,
            approaches: [
                "Quantify retrieval uncertainty from similarity scores.",
                "Assess source quality and reliability.",
                "Measure answer confidence using LLM self-assessment.",
                "Calculate source agreement for consistency.",
                "Combine multiple uncertainty signals.",
                "Calibrate confidence scores with ground truth.",
                "Communicate uncertainty clearly to users.",
                "Use confidence thresholds for actions.",
                "Track uncertainty over time for improvement.",
                "Provide uncertainty breakdowns for transparency."
            ]
        });

        // Q49 - RAG with federated learning
        questions.push({
            number: 49,
            title: "How do you implement federated learning approaches for RAG systems?",
            description:
                "Federated learning enables training or improving RAG systems across multiple organizations or data sources " +
                "without sharing raw data. This involves distributed model updates, privacy-preserving aggregation, " +
                "and collaborative learning while maintaining data privacy.",
            why:
                "Organizations want to improve RAG systems with more data but can't share sensitive documents. " +
                "Federated learning enables collaborative improvement while preserving privacy and data sovereignty.",
            what:
                "Federated Learning Components: Local model training, Gradient aggregation, Privacy-preserving techniques, " +
                "Secure aggregation, Differential privacy, Homomorphic encryption. " +
                "RAG Applications: Distributed embedding training, Collaborative retrieval improvement, " +
                "Shared knowledge without data sharing, Privacy-preserving RAG.",
            how:
                "1) Initialize global model, 2) Distribute to participants, " +
                "3) Train locally on private data, 4) Aggregate updates securely, " +
                "5) Update global model, 6) Distribute updated model, " +
                "7) Repeat for multiple rounds.",
            pros: [
                "Privacy-preserving",
                "Enables collaboration",
                "Leverages distributed data",
                "No raw data sharing",
                "Regulatory compliance",
                "Scalable approach"
            ],
            cons: [
                "Complex implementation",
                "Communication overhead",
                "Heterogeneous data challenges",
                "Security requirements",
                "Coordination complexity",
                "May need trusted aggregator"
            ],
            diagram: `flowchart TD
    A["Global RAG Model"] --> B["Participant 1"]
    A --> C["Participant 2"]
    A --> D["Participant N"]
    B --> E["Local Training"]
    C --> E
    D --> E
    E --> F["Secure Aggregation"]
    F --> G["Updated Global Model"]
    G --> A`,
            implementation: `# Federated Learning for RAG
from typing import List, Dict
import numpy as np
from cryptography.fernet import Fernet

# 1. Basic federated learning framework
class FederatedRAG:
    def __init__(self, global_model):
        self.global_model = global_model
        self.participants = []
        self.round_history = []
    
    def add_participant(self, participant_id, local_data):
        """Add a participant with their local data"""
        participant = {
            "id": participant_id,
            "local_data": local_data,
            "local_model": None
        }
        self.participants.append(participant)
    
    def federated_training_round(self, num_epochs=1):
        """Execute one round of federated training"""
        # 1. Distribute global model to participants
        for participant in self.participants:
            participant["local_model"] = self._copy_model(self.global_model)
        
        # 2. Local training
        local_updates = []
        for participant in self.participants:
            update = self._train_locally(
                participant["local_model"],
                participant["local_data"],
                num_epochs
            )
            local_updates.append({
                "participant_id": participant["id"],
                "update": update
            })
        
        # 3. Aggregate updates
        aggregated_update = self._aggregate_updates(local_updates)
        
        # 4. Update global model
        self._apply_update(self.global_model, aggregated_update)
        
        # 5. Record round
        self.round_history.append({
            "round": len(self.round_history) + 1,
            "participants": len(self.participants),
            "update_norm": np.linalg.norm(aggregated_update)
        })
    
    def _train_locally(self, model, data, epochs):
        """Train model on local data"""
        # Simplified: return model update (gradients)
        # In practice, this would train the embedding model or retrieval model
        update = {}
        # ... training logic ...
        return update
    
    def _aggregate_updates(self, updates):
        """Aggregate local updates (Federated Averaging)"""
        # Weighted average based on data size
        total_samples = sum(u.get("num_samples", 1) for u in updates)
        
        aggregated = {}
        for key in updates[0]["update"].keys():
            weighted_sum = sum(
                u["update"][key] * (u.get("num_samples", 1) / total_samples)
                for u in updates
            )
            aggregated[key] = weighted_sum
        
        return aggregated
    
    def _apply_update(self, model, update):
        """Apply aggregated update to global model"""
        # Update model parameters
        # Implementation depends on model type
        pass
    
    def _copy_model(self, model):
        """Create a copy of the model"""
        # Deep copy model for local training
        # Implementation depends on model framework
        return model

# 2. Privacy-preserving aggregation
class SecureFederatedRAG:
    def __init__(self, global_model):
        self.global_model = global_model
        self.encryption_key = Fernet.generate_key()
        self.cipher = Fernet(self.encryption_key)
    
    def secure_aggregation(self, encrypted_updates):
        """Aggregate encrypted updates"""
        # Decrypt updates
        decrypted = []
        for encrypted_update in encrypted_updates:
            decrypted_update = self._decrypt_update(encrypted_update)
            decrypted.append(decrypted_update)
        
        # Aggregate (same as before)
        aggregated = self._aggregate_updates(decrypted)
        
        return aggregated
    
    def _encrypt_update(self, update):
        """Encrypt model update"""
        # Serialize and encrypt
        serialized = self._serialize_update(update)
        encrypted = self.cipher.encrypt(serialized)
        return encrypted
    
    def _decrypt_update(self, encrypted):
        """Decrypt model update"""
        decrypted = self.cipher.decrypt(encrypted)
        update = self._deserialize_update(decrypted)
        return update
    
    def _serialize_update(self, update):
        """Serialize update for encryption"""
        # Convert to bytes
        import pickle
        return pickle.dumps(update)
    
    def _deserialize_update(self, data):
        """Deserialize update"""
        import pickle
        return pickle.loads(data)

# 3. Differential privacy for RAG
class DifferentiallyPrivateRAG:
    def __init__(self, global_model, epsilon=1.0):
        self.global_model = global_model
        self.epsilon = epsilon  # Privacy budget
    
    def add_noise_to_update(self, update, sensitivity):
        """Add calibrated noise for differential privacy"""
        # Calculate noise scale
        noise_scale = sensitivity / self.epsilon
        
        # Add Laplace noise
        noisy_update = {}
        for key, value in update.items():
            noise = np.random.laplace(0, noise_scale, value.shape)
            noisy_update[key] = value + noise
        
        return noisy_update
    
    def federated_round_with_privacy(self, local_updates):
        """Federated round with differential privacy"""
        # Add noise to each update
        private_updates = []
        for update in local_updates:
            noisy_update = self.add_noise_to_update(update, sensitivity=1.0)
            private_updates.append(noisy_update)
        
        # Aggregate
        aggregated = self._aggregate_updates(private_updates)
        
        return aggregated

# 4. RAG-specific federated learning
class FederatedRAGEmbeddings:
    """Federated learning for embedding models in RAG"""
    
    def __init__(self, embedding_model):
        self.embedding_model = embedding_model
    
    def federated_embedding_training(self, participants_data):
        """Train embeddings across participants"""
        # Each participant trains on their documents
        local_embeddings = []
        
        for participant_id, documents in participants_data.items():
            # Train embedding model on local documents
            local_embedding_model = self._train_embeddings(documents)
            local_embeddings.append({
                "participant": participant_id,
                "model": local_embedding_model
            })
        
        # Aggregate embedding models
        global_embedding = self._aggregate_embeddings(local_embeddings)
        
        return global_embedding
    
    def _train_embeddings(self, documents):
        """Train embedding model on documents"""
        # Fine-tune embedding model
        # Implementation depends on embedding framework
        pass
    
    def _aggregate_embeddings(self, local_embeddings):
        """Aggregate embedding models"""
        # Average model parameters
        # Implementation
        pass

# 5. Practical federated RAG setup
class PracticalFederatedRAG:
    def __init__(self):
        self.coordinator = None
        self.participants = {}
    
    def setup_federation(self, coordinator_config, participant_configs):
        """Setup federated learning infrastructure"""
        # Initialize coordinator
        self.coordinator = {
            "config": coordinator_config,
            "global_model": None,
            "round": 0
        }
        
        # Initialize participants
        for participant_id, config in participant_configs.items():
            self.participants[participant_id] = {
                "config": config,
                "local_model": None,
                "data_size": config.get("data_size", 0)
            }
    
    def execute_round(self):
        """Execute one federated learning round"""
        # 1. Coordinator sends global model to participants
        global_model = self.coordinator["global_model"]
        
        # 2. Participants train locally
        local_updates = {}
        for participant_id, participant in self.participants.items():
            local_model = self._train_participant(
                participant_id,
                global_model,
                participant["config"]["local_data"]
            )
            
            # Extract update
            update = self._extract_update(global_model, local_model)
            local_updates[participant_id] = update
        
        # 3. Secure aggregation at coordinator
        aggregated = self._secure_aggregate(local_updates)
        
        # 4. Update global model
        self._update_global_model(aggregated)
        
        # 5. Increment round
        self.coordinator["round"] += 1
    
    def _train_participant(self, participant_id, model, data):
        """Train model for a participant"""
        # Local training logic
        pass
    
    def _extract_update(self, global_model, local_model):
        """Extract parameter differences"""
        # Calculate update = local - global
        pass
    
    def _secure_aggregate(self, updates):
        """Securely aggregate updates"""
        # Implement secure aggregation protocol
        pass
    
    def _update_global_model(self, aggregated_update):
        """Update global model with aggregated update"""
        pass`,
            approaches: [
                "Use federated averaging for model aggregation.",
                "Implement secure aggregation protocols.",
                "Apply differential privacy for additional protection.",
                "Handle heterogeneous data across participants.",
                "Design efficient communication protocols.",
                "Consider trusted aggregator or secure multi-party computation.",
                "Monitor and validate participant contributions.",
                "Handle participant dropouts gracefully.",
                "Balance privacy and model quality.",
                "Test with realistic federated scenarios."
            ]
        });

        // Q50 - RAG system maintenance and monitoring
        questions.push({
            number: 50,
            title: "How do you maintain and monitor RAG systems in production?",
            description:
                "Production RAG systems require ongoing maintenance, monitoring, and optimization. This involves " +
                "performance monitoring, quality tracking, drift detection, model updates, and incident response. " +
                "Effective maintenance ensures RAG systems remain accurate, performant, and reliable over time.",
            why:
                "RAG systems degrade over time due to data drift, model staleness, and changing user needs. " +
                "Proactive maintenance and monitoring prevent issues, ensure quality, and enable continuous improvement.",
            what:
                "Maintenance Areas: Performance monitoring, Quality tracking, Drift detection, Model updates, " +
                "Data pipeline health, User feedback integration, Cost optimization, Security monitoring. " +
                "Monitoring Metrics: Latency, Throughput, Error rates, Retrieval quality, Answer quality, " +
                "User satisfaction, Cost per query, System health.",
            how:
                "1) Set up comprehensive monitoring, 2) Track key metrics continuously, " +
                "3) Detect anomalies and drift, 4) Collect user feedback, " +
                "5) Schedule regular model updates, 6) Monitor data pipeline, " +
                "7) Optimize based on metrics, 8) Respond to incidents quickly.",
            pros: [
                "Prevents degradation",
                "Ensures quality",
                "Enables optimization",
                "Early problem detection",
                "Data-driven improvements",
                "Better user experience"
            ],
            cons: [
                "Requires dedicated effort",
                "Monitoring infrastructure costs",
                "Alert fatigue if not tuned",
                "Need for expertise",
                "Ongoing maintenance",
                "Complexity"
            ],
            diagram: `flowchart TD
    A["RAG System"] --> B["Performance Monitor"]
    A --> C["Quality Tracker"]
    A --> D["Drift Detector"]
    B --> E["Metrics Dashboard"]
    C --> E
    D --> E
    E --> F["Alert System"]
    F --> G["Incident Response"]
    G --> H["Model Update"]
    G --> I["Pipeline Fix"]
    H --> A
    I --> A`,
            implementation: `# RAG System Maintenance and Monitoring
from typing import Dict, List
import time
from datetime import datetime, timedelta
from collections import defaultdict
import logging

# 1. Comprehensive monitoring
class RAGMonitor:
    def __init__(self):
        self.metrics = {
            "latency": [],
            "throughput": 0,
            "errors": [],
            "retrieval_scores": [],
            "answer_quality": [],
            "costs": []
        }
        self.alerts = []
    
    def track_query(self, query_result):
        """Track metrics for a query"""
        # Latency
        if "latency" in query_result:
            self.metrics["latency"].append(query_result["latency"])
        
        # Retrieval quality
        if "retrieval_score" in query_result:
            self.metrics["retrieval_scores"].append(query_result["retrieval_score"])
        
        # Answer quality (if available)
        if "quality_score" in query_result:
            self.metrics["answer_quality"].append(query_result["quality_score"])
        
        # Cost
        if "cost" in query_result:
            self.metrics["costs"].append(query_result["cost"])
        
        # Throughput
        self.metrics["throughput"] += 1
        
        # Check for anomalies
        self._check_anomalies()
    
    def track_error(self, error_type, error_message):
        """Track errors"""
        self.metrics["errors"].append({
            "type": error_type,
            "message": error_message,
            "timestamp": datetime.now()
        })
    
    def get_metrics_summary(self):
        """Get summary of current metrics"""
        latency_metrics = self.metrics["latency"]
        if latency_metrics:
            avg_latency = sum(latency_metrics) / len(latency_metrics)
            p95_latency = sorted(latency_metrics)[int(len(latency_metrics) * 0.95)]
        else:
            avg_latency = p95_latency = 0
        
        return {
            "avg_latency": avg_latency,
            "p95_latency": p95_latency,
            "throughput": self.metrics["throughput"],
            "error_rate": len(self.metrics["errors"]) / max(self.metrics["throughput"], 1),
            "avg_retrieval_score": sum(self.metrics["retrieval_scores"]) / max(len(self.metrics["retrieval_scores"]), 1),
            "avg_answer_quality": sum(self.metrics["answer_quality"]) / max(len(self.metrics["answer_quality"]), 1),
            "total_cost": sum(self.metrics["costs"])
        }
    
    def _check_anomalies(self):
        """Check for anomalies and generate alerts"""
        # Check latency
        if self.metrics["latency"]:
            recent_latency = self.metrics["latency"][-100:]
            avg_recent = sum(recent_latency) / len(recent_latency)
            if avg_recent > 5.0:  # Threshold
                self._create_alert("high_latency", f"Average latency: {avg_recent:.2f}s")
        
        # Check error rate
        recent_errors = [e for e in self.metrics["errors"] 
                        if (datetime.now() - e["timestamp"]).seconds < 3600]
        if len(recent_errors) > 10:
            self._create_alert("high_error_rate", f"{len(recent_errors)} errors in last hour")
    
    def _create_alert(self, alert_type, message):
        """Create an alert"""
        alert = {
            "type": alert_type,
            "message": message,
            "timestamp": datetime.now(),
            "severity": "high" if alert_type in ["high_error_rate"] else "medium"
        }
        self.alerts.append(alert)
        logging.warning(f"Alert: {alert_type} - {message}")

# 2. Quality tracking
class QualityTracker:
    def __init__(self):
        self.quality_history = []
        self.baseline_quality = None
    
    def track_quality(self, query, answer, sources, quality_score=None):
        """Track answer quality"""
        if quality_score is None:
            quality_score = self._calculate_quality(query, answer, sources)
        
        record = {
            "timestamp": datetime.now(),
            "query": query,
            "quality_score": quality_score,
            "answer_length": len(answer),
            "num_sources": len(sources)
        }
        
        self.quality_history.append(record)
        
        # Update baseline if needed
        if self.baseline_quality is None:
            self.baseline_quality = quality_score
        
        # Check for quality degradation
        recent_quality = self._get_recent_quality()
        if recent_quality < self.baseline_quality * 0.9:
            logging.warning(f"Quality degradation detected: {recent_quality:.2f} vs baseline {self.baseline_quality:.2f}")
    
    def _calculate_quality(self, query, answer, sources):
        """Calculate quality score"""
        # Simplified quality calculation
        # In practice, use LLM-based evaluation or user feedback
        score = 0.5
        
        # Factor: answer length (not too short, not too long)
        if 50 < len(answer) < 1000:
            score += 0.2
        
        # Factor: number of sources
        if 2 <= len(sources) <= 5:
            score += 0.2
        
        # Factor: source relevance (simplified)
        score += 0.1
        
        return min(score, 1.0)
    
    def _get_recent_quality(self, hours=24):
        """Get average quality over recent period"""
        cutoff = datetime.now() - timedelta(hours=hours)
        recent = [r for r in self.quality_history if r["timestamp"] > cutoff]
        
        if not recent:
            return 0.0
        
        return sum(r["quality_score"] for r in recent) / len(recent)

# 3. Drift detection
class DriftDetector:
    def __init__(self):
        self.baseline_distribution = None
        self.current_distribution = None
    
    def detect_retrieval_drift(self, recent_queries, recent_results):
        """Detect drift in retrieval patterns"""
        # Compare query distribution
        current_query_dist = self._analyze_query_distribution(recent_queries)
        
        if self.baseline_distribution is None:
            self.baseline_distribution = current_query_dist
            return None
        
        # Calculate drift
        drift_score = self._calculate_distribution_drift(
            self.baseline_distribution,
            current_query_dist
        )
        
        if drift_score > 0.3:  # Threshold
            return {
                "type": "retrieval_drift",
                "score": drift_score,
                "message": "Significant change in query patterns detected"
            }
        
        return None
    
    def _analyze_query_distribution(self, queries):
        """Analyze distribution of queries"""
        # Extract key terms, topics, etc.
        # Simplified version
        return {"avg_length": sum(len(q) for q in queries) / len(queries)}
    
    def _calculate_distribution_drift(self, baseline, current):
        """Calculate drift between distributions"""
        # Simplified: compare key metrics
        drift = abs(baseline["avg_length"] - current["avg_length"]) / baseline["avg_length"]
        return drift

# 4. Production RAG maintenance system
class ProductionRAGMaintenance:
    def __init__(self, rag_system):
        self.rag_system = rag_system
        self.monitor = RAGMonitor()
        self.quality_tracker = QualityTracker()
        self.drift_detector = DriftDetector()
        self.maintenance_schedule = []
    
    def setup_monitoring(self):
        """Setup comprehensive monitoring"""
        # Schedule regular checks
        self.maintenance_schedule = [
            {"task": "quality_check", "interval": timedelta(hours=1)},
            {"task": "drift_detection", "interval": timedelta(days=1)},
            {"task": "model_update_check", "interval": timedelta(days=7)},
            {"task": "cost_analysis", "interval": timedelta(days=1)}
        ]
    
    def execute_maintenance(self):
        """Execute scheduled maintenance tasks"""
        for task in self.maintenance_schedule:
            if self._should_run(task):
                self._execute_task(task)
    
    def _should_run(self, task):
        """Check if task should run"""
        # Check last execution time
        # Implementation
        return True
    
    def _execute_task(self, task):
        """Execute a maintenance task"""
        if task["task"] == "quality_check":
            self._check_quality()
        elif task["task"] == "drift_detection":
            self._detect_drift()
        elif task["task"] == "model_update_check":
            self._check_model_updates()
        elif task["task"] == "cost_analysis":
            self._analyze_costs()
    
    def _check_quality(self):
        """Check system quality"""
        summary = self.monitor.get_metrics_summary()
        if summary["avg_answer_quality"] < 0.7:
            logging.warning("Answer quality below threshold")
    
    def _detect_drift(self):
        """Detect data/model drift"""
        # Implementation
        pass
    
    def _check_model_updates(self):
        """Check if model updates are needed"""
        # Check for new embedding models, LLM versions, etc.
        pass
    
    def _analyze_costs(self):
        """Analyze and optimize costs"""
        summary = self.monitor.get_metrics_summary()
        if summary["total_cost"] > 1000:  # Threshold
            logging.warning(f"High costs detected: $\\{summary['total_cost']:.2f}")
    
    def generate_maintenance_report(self):
        """Generate maintenance report"""
        metrics = self.monitor.get_metrics_summary()
        quality = self.quality_tracker._get_recent_quality()
        
        return {
            "timestamp": datetime.now(),
            "metrics": metrics,
            "quality": quality,
            "alerts": self.monitor.alerts[-10:],  # Last 10 alerts
            "recommendations": self._generate_recommendations(metrics, quality)
        }
    
    def _generate_recommendations(self, metrics, quality):
        """Generate maintenance recommendations"""
        recommendations = []
        
        if metrics["avg_latency"] > 3.0:
            recommendations.append("Consider optimizing retrieval or using caching")
        
        if quality < 0.7:
            recommendations.append("Review answer quality and consider model updates")
        
        if metrics["error_rate"] > 0.05:
            recommendations.append("Investigate error sources and fix issues")
        
        return recommendations`,
            approaches: [
                "Set up comprehensive monitoring for all key metrics.",
                "Track quality continuously and detect degradation.",
                "Implement drift detection for data and model changes.",
                "Schedule regular maintenance tasks.",
                "Collect and analyze user feedback.",
                "Monitor costs and optimize spending.",
                "Set up alerting for critical issues.",
                "Maintain model update schedules.",
                "Document incidents and resolutions.",
                "Generate regular maintenance reports."
            ]
        });

        // Rendering logic
        function escapeHtml(text) {
            if (!text) return '';
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }
        
        function escapeForTemplate(text) {
            if (!text) return '';
            return String(text)
                .replace(/\\/g, '\\\\')
                .replace(/`/g, '\\`')
                .replace(/\${/g, '\\${');
        }
        
        const tocList = document.getElementById('tocList');
        const container = document.getElementById('questionsContainer');
        questions.sort((a, b) => a.number - b.number);
        questions.forEach(q => {
            const link = document.createElement('a');
            link.href = `#question-${q.number}`;
            const shortTitle = q.title.length > 40 ? q.title.substring(0, 40) + "..." : q.title;
            link.textContent = `Q${q.number}: ${shortTitle}`;
            link.onclick = (e) => {
                e.preventDefault();
                document.getElementById(`question-${q.number}`).scrollIntoView({ behavior: 'smooth', block: 'start' });
            };
            tocList.appendChild(link);
            const card = document.createElement('div');
            card.className = 'question-card';
            card.id = `question-${q.number}`;
            
            // Escape all content properly
            const escapedTitle = escapeHtml(q.title);
            const escapedDesc = escapeHtml(q.description);
            const escapedWhy = escapeHtml(q.why);
            const escapedWhat = escapeHtml(q.what);
            const escapedHow = escapeHtml(q.how);
            const escapedImplementation = escapeHtml(q.implementation);
            const escapedDiagram = q.diagram.replace(/"/g, '&quot;').replace(/\n/g, '\\n').replace(/</g, '&lt;').replace(/>/g, '&gt;');
            
            card.innerHTML = 
                `<div class="question-number">Question ${q.number} of 50</div>` +
                `<h2 class="question-title">${escapedTitle}</h2>` +
                `<div class="question-section"><h3>1. Detailed Description</h3><p>${escapedDesc}</p></div>` +
                `<div class="question-section"><h3>2. Why / What / How</h3>` +
                `<p><strong>Why:</strong> ${escapedWhy}</p>` +
                `<p><strong>What:</strong> ${escapedWhat}</p>` +
                `<p><strong>How:</strong> ${escapedHow}</p></div>` +
                `<div class="question-section"><h3>3. Pros and Cons</h3>` +
                `<div class="pros-cons">` +
                `<div class="pros-box"><h4>Pros</h4><ul>${q.pros.map(p => `<li>${escapeHtml(p)}</li>`).join('')}</ul></div>` +
                `<div class="cons-box"><h4>Cons</h4><ul>${q.cons.map(c => `<li>${escapeHtml(c)}</li>`).join('')}</ul></div>` +
                `</div></div>` +
                `<div class="question-section"><h3>4. Design Diagram</h3>` +
                `<div class="diagram-container">` +
                `<div class="diagram-title">Architecture Diagram</div>` +
                `<div class="mermaid" data-diagram="${escapedDiagram}">${escapeHtml(q.diagram)}</div>` +
                `</div></div>` +
                `<div class="question-section"><h3>5. Implementation (Python)</h3>` +
                `<div class="code-block"><pre><code>${escapedImplementation}</code></pre></div></div>` +
                `<div class="question-section"><h3>6. Approaches</h3>` +
                `<ul>${q.approaches.map(a => `<li>${escapeHtml(a)}</li>`).join('')}</ul></div>`;
            
            container.appendChild(card);
        });
        function applyBlackTextStyling(svg) {
            if (!svg) return;
            svg.style.backgroundColor = '#ffffff';
            svg.querySelectorAll('text, tspan').forEach(text => { text.setAttribute('fill', '#000000'); text.style.fill = '#000000'; });
            svg.querySelectorAll('.node rect, .node circle, .node ellipse').forEach(node => { node.setAttribute('fill', '#ffffff'); node.setAttribute('stroke', '#000000'); });
        }
        function renderMermaidDiagrams() {
            if (typeof mermaid === 'undefined') { setTimeout(renderMermaidDiagrams, 100); return; }
            mermaid.initialize({ startOnLoad: false, theme: "default", themeVariables: { primaryColor: "#ffffff", primaryTextColor: "#000000", primaryBorderColor: "#000000", lineColor: "#000000", textColor: "#000000", mainBkg: "#ffffff" }, flowchart: { useMaxWidth: true, htmlLabels: true } });
            document.querySelectorAll('.mermaid').forEach(async (element, index) => {
                try {
                    let graphDefinition = element.getAttribute('data-diagram') || element.textContent || '';
                    graphDefinition = graphDefinition.replace(/\\n/g, '\n').replace(/&quot;/g, '"').replace(/&lt;/g, '<').replace(/&gt;/g, '>');
                    const result = await mermaid.render('mermaid-' + index + '-' + Date.now(), graphDefinition.trim());
                    if (result && result.svg) { element.innerHTML = result.svg; const svg = element.querySelector('svg'); if (svg) applyBlackTextStyling(svg); }
                } catch (error) { console.error(`Error rendering diagram ${index}:`, error); }
            });
        }
        if (document.readyState === 'loading') { document.addEventListener('DOMContentLoaded', () => setTimeout(renderMermaidDiagrams, 300)); } else { setTimeout(renderMermaidDiagrams, 300); }
    </script>
</body>
</html>
